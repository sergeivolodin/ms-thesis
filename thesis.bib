@article{Fortuin2018,
abstract = {High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time. To address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space. This model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty. We evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data.},
annote = {REF clustering time series and fitting a markov model.},
archivePrefix = {arXiv},
arxivId = {1806.02199},
author = {Fortuin, Vincent and Huser, Matthias and Locatello, Francesco and Strathmann, Heiko and R{\"{a}}tsch, Gunnar},
eprint = {1806.02199},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fortuin et al. - 2018 - SOM-VAE Interpretable discrete representation learning on time series.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--18},
title = {{SOM-VAE: Interpretable discrete representation learning on time series}},
year = {2018}
}
@article{Kipf2020,
abstract = {A structured understanding of our world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastively-trained Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. We evaluate C-SWMs on compositional environments involving multiple interacting objects that can be manipulated independently by an agent, simple Atari games, and a multi-object physics simulation. Our experiments demonstrate that C-SWMs can overcome limitations of models based on pixel reconstruction and outperform typical representatives of this model class in highly structured environments, while learning interpretable object-based representations.},
annote = {REF contrastive loss for RL representations

From Duplicate 1 (CONTRASTIVE LEARNING OF STRUCTURED WORLD MODELS - Kipf, Thomas; Van Der Pol, Elise; Welling, Max)

use the environments from the paper and compare to their learned model in terms of accuracy and sparsity},
archivePrefix = {arXiv},
arxivId = {1911.12247},
author = {Kipf, Thomas and Pol, Elise Van Der and Welling, Max and {Van Der Pol}, Elise and Welling, Max},
eprint = {1911.12247},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kipf, Pol, Welling - 2019 - Contrastive learning of structured world models.pdf:pdf;:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kipf, Van Der Pol, Welling - Unknown - CONTRASTIVE LEARNING OF STRUCTURED WORLD MODELS.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--21},
title = {{Contrastive learning of structured world models}},
year = {2019}
}
@article{Hamming1980,
abstract = {It is evident from the title that this is a philosophical discussion. I shall not apologize for the philosophy, though I am well aware that most scientists, engineers, and mathematicians have little regard for it; instead, I shall give this short prologue to justify the approach.},
annote = {REF assuming structure in the world

math to explain the 'how'.why does it work? bias or system?},
author = {Hamming, R. W.},
doi = {10.2307/2321982},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamming - 1980 - The Unreasonable Effectiveness of Mathematics.pdf:pdf},
issn = {00029890},
journal = {The American Mathematical Monthly},
number = {2},
pages = {81},
title = {{The Unreasonable Effectiveness of Mathematics}},
volume = {87},
year = {1980}
}
@article{Francois-Lavet2018,
abstract = {In the quest for efficient and robust reinforcement learning methods, both model-free and model-based approaches offer advantages. In this paper we propose a new way of explicitly bridging both approaches via a shared low-dimensional learned encoding of the environment, meant to capture summarizing abstractions. We show that the modularity brought by this approach leads to good generalization while being computationally efficient, with planning happening in a smaller latent state space. In addition, this approach recovers a sufficient low-dimensional representation of the environment, which opens up new strategies for interpretable AI, exploration and transfer learning.},
annote = {REF handcoded interpretability loss


From Duplicate 1 (Combined Reinforcement Learning via Abstract Representations - Fran{\c{c}}ois-Lavet, Vincent; Bengio, Yoshua; Precup, Doina; Pineau, Joelle)

Latent space between model-based and model-free. Low-dim space resembling the environment structure -- low-dim MDP. Interpretability: vector in embedding space {\textless}-{\textgreater} action

From Duplicate 2 (Combined Reinforcement Learning via Abstract Representations - Francois-Lavet, Vincent; Bengio, Yoshua; Precup, Doina; Pineau, Joelle)

Abstract state between model-based and model-free agents; Heuristics for regul; Emerging sane latent spaces with regularization},
archivePrefix = {arXiv},
arxivId = {1809.04506},
author = {Francois-Lavet, Vincent and Bengio, Yoshua and Precup, Doina and Pineau, Joelle and Fran{\c{c}}ois-Lavet, Vincent and Bengio, Yoshua and Precup, Doina and Pineau, Joelle},
doi = {10.1609/aaai.v33i01.33013582},
eprint = {1809.04506},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fran{\c{c}}ois-Lavet et al. - 2018 - Combined Reinforcement Learning via Abstract Representations.pdf:pdf;:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Francois-Lavet et al. - 2019 - Combined Reinforcement Learning via Abstract Representations.pdf:pdf},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
keywords = {abstract rep,deep RL,deep reinforcement learning},
month = {sep},
pages = {3582--3589},
title = {{Combined Reinforcement Learning via Abstract Representations}},
url = {http://arxiv.org/abs/1809.04506},
volume = {33},
year = {2019}
}
@article{Tank2017,
abstract = {While most classical approaches to Granger causality detection repose upon linear time series assumptions, many interactions in neuroscience and economics applications are nonlinear. We develop an approach to nonlinear Granger causality detection using multilayer perceptrons where the input to the network is the past time lags of all series and the output is the future value of a single series. A sufficient condition for Granger non-causality in this setting is that all of the outgoing weights of the input data, the past lags of a series, to the first hidden layer are zero. For estimation, we utilize a group lasso penalty to shrink groups of input weights to zero. We also propose a hierarchical penalty for simultaneous Granger causality and lag estimation. We validate our approach on simulated data from both a sparse linear autoregressive model and the sparse and nonlinear Lorenz-96 model.},
annote = {REF first-layer weights l1 penalty.

REF nonlinear granger causality (basic, not ours)},
archivePrefix = {arXiv},
arxivId = {1711.08160},
author = {Tank, Alex and Cover, Ian and Foti, Nicholas J. and Shojaie, Ali and Fox, Emily B.},
eprint = {1711.08160},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tank et al. - 2017 - An Interpretable and Sparse Neural Network Model for Nonlinear Granger Causality Discovery.pdf:pdf},
number = {Nips},
title = {{An Interpretable and Sparse Neural Network Model for Nonlinear Granger Causality Discovery}},
url = {http://arxiv.org/abs/1711.08160},
year = {2017}
}
@article{Madumal2019,
annote = {REF causal models for interpretability of RL

CONCRETE examples of causal models of envs and explanations (for humans by ALGOS)},
archivePrefix = {arXiv},
arxivId = {1905.10958},
author = {Madumal, Prashan and Miller, Tim and Sonenberg, Liz and Vetere, Frank},
eprint = {1905.10958},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Madumal et al. - 2019 - Explainable Reinforcement Learning Through a Causal Lens.pdf:pdf},
month = {may},
title = {{Explainable Reinforcement Learning Through a Causal Lens}},
url = {https://arxiv.org/abs/1905.10958},
year = {2019}
}
@article{Thomas2018,
abstract = {It has been postulated that a good representation is one that disentangles the underlying explanatory factors of variation. However, it remains an open question what kind of training framework could potentially achieve that. Whereas most previous work focuses on the static setting (e.g., with images), we postulate that some of the causal factors could be discovered if the learner is allowed to interact with its environment. The agent can experiment with different actions and observe their effects. More specifically, we hypothesize that some of these factors correspond to aspects of the environment which are independently controllable, i.e., that there exists a policy and a learnable feature for each such aspect of the environment, such that this policy can yield changes in that feature with minimal changes to other features that explain the statistical variations in the observed data. We propose a specific objective function to find such factors, and verify experimentally that it can indeed disentangle independently controllable aspects of the environment without any extrinsic reward signal.},
annote = {REF 1 action corresponds to incrementing feature

Objective for independently controllable factors. Very small gridworlds.

causality loss and interaction might help learning disentangled representations},
archivePrefix = {arXiv},
arxivId = {1802.09484},
author = {Thomas, Valentin and Bengio, Emmanuel and Fedus, William and Pondard, Jules and Beaudoin, Philippe and Larochelle, Hugo and Pineau, Joelle and Precup, Doina and Bengio, Yoshua},
eprint = {1802.09484},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomas et al. - 2018 - Disentangling the independently controllable factors of variation by interacting with the world.pdf:pdf},
pages = {1--9},
title = {{Disentangling the independently controllable factors of variation by interacting with the world}},
url = {http://arxiv.org/abs/1802.09484},
year = {2018}
}
@article{Petros2020,
annote = {ref sparse feature vector},
author = {Petros, Giannakopoulos and Aggelos, Pikrakis and Yannis, Cotronis},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Petros, Aggelos, Yannis - 2020 - NEURAL DISCRETE ABSTRACTION OF HIGH-DIMENSIONAL SPACES A CASE STUDY IN REINFORCEMENT LEARNING Petros G.pdf:pdf},
isbn = {9789082797053},
pages = {1517--1521},
title = {{NEURAL DISCRETE ABSTRACTION OF HIGH-DIMENSIONAL SPACES : A CASE STUDY IN REINFORCEMENT LEARNING Petros Giannakopoulos Yannis Cotronis National and Kapodistrian University of Athens University of Piraeus}},
year = {2020}
}
@article{Corneil2018,
abstract = {Modern reinforcement learning algorithms reach super-human performance on many board and video games, but they are sample inefficient, i.e. they typically require significantly more playing experience than humans to reach an equal performance level. To improve sample efficiency, an agent may build a model of the environment and use planning methods to update its policy. In this article we introduce Variational State Tabulation (VaST), which maps an environment with a high-dimensional state space (e.g. the space of visual inputs) to an abstract tabular model. Prioritized sweeping with small backups, a highly efficient planning method, can then be used to update state-action values. We show how VaST can rapidly learn to maximize reward in tasks like 3D navigation and efficiently adapt to sudden changes in rewards or transition probabilities.},
annote = {REF prototype states},
author = {Corneil, Dane and Gerstner, Wulfram and Brea, Johanni},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Corneil, Gerstner, Brea - 2018 - Efficient model-based deep reinforcement learning with variational state tabulation.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Efficient model-based deep reinforcement learning with variational state tabulation}},
year = {2018}
}
@article{Barnett2015,
abstract = {Granger causality has long been a prominent method for inferring causal interactions between stochastic variables for a broad range of complex physical systems. However, it has been recognized that a moving average (MA) component in the data presents a serious confound to Granger causal analysis, as routinely performed via autoregressive (AR) modeling. We solve this problem by demonstrating that Granger causality may be calculated simply and efficiently from the parameters of a state-space (SS) model. Since SS models are equivalent to autoregressive moving average models, Granger causality estimated in this fashion is not degraded by the presence of a MA component. This is of particular significance when the data has been filtered, downsampled, observed with noise, or is a subprocess of a higher dimensional process, since all of these operations - commonplace in application domains as diverse as climate science, econometrics, and the neurosciences - induce a MA component. We show how Granger causality, conditional and unconditional, in both time and frequency domains, may be calculated directly from SS model parameters via solution of a discrete algebraic Riccati equation. Numerical simulations demonstrate that Granger causality estimators thus derived have greater statistical power and smaller bias than AR estimators. We also discuss how the SS approach facilitates relaxation of the assumptions of linearity, stationarity, and homoscedasticity underlying current AR methods, thus opening up potentially significant new areas of research in Granger causal analysis.},
annote = {OBSERVATION encoder
REF linear case

Explicit solution},
archivePrefix = {arXiv},
arxivId = {1501.06502},
author = {Barnett, Lionel and Seth, Anil K.},
doi = {10.1103/PhysRevE.91.040101},
eprint = {1501.06502},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Barnett, Seth - 2015 - Granger causality for state-space models.pdf:pdf},
issn = {15502376},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
number = {4},
pages = {1--5},
title = {{Granger causality for state-space models}},
volume = {91},
year = {2015}
}
@article{Pearl2020,
annote = {REF causal models},
author = {Pearl, Judea},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pearl - 2020 - Causality models, reasoning and inference.pdf:pdf},
issn = {0008350X},
pmid = {11768929},
title = {{Causality: models, reasoning and inference}},
year = {2020}
}
@article{Tank2018,
abstract = {While most classical approaches to Granger causality detection assume linear dynamics, many interactions in applied domains, like neuroscience and genomics, are inherently nonlinear. In these cases, using linear models may lead to inconsistent estimation of Granger causal interactions. We propose a class of nonlinear methods by applying structured multilayer perceptrons (MLPs) or recurrent neural networks (RNNs) combined with sparsity-inducing penalties on the weights. By encouraging specific sets of weights to be zero—in particular through the use of convex group-lasso penalties—we can extract the Granger causal structure. To further contrast with traditional approaches, our framework naturally enables us to efficiently capture long-range dependencies between series either via our RNNs or through an automatic lag selection in the MLP. We show that our neural Granger causality methods outperform state-of-the-art nonlinear Granger causality methods on the DREAM3 challenge data. This data consists of nonlinear gene expression and regulation time courses with only a limited number of time points. The successes we show in this challenging dataset provide a powerful example of how deep learning can be useful in cases that go beyond prediction on large datasets. We likewise demonstrate our methods in detecting nonlinear interactions in a human motion capture dataset.},
annote = {ref multi-step neural granger},
archivePrefix = {arXiv},
arxivId = {1802.05842},
author = {Tank, Alex and Covert, Ian and Foti, Nicholas J. and Shojaie, Ali and Fox, Emily B.},
eprint = {1802.05842},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tank et al. - 2018 - Neural granger causality for nonlinear time series.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--14},
title = {{Neural granger causality for nonlinear time series}},
year = {2018}
}
@article{Fiez2020,
abstract = {This paper concerns the local stability and convergence rate of gradient descent-ascent in two-player non-convex, non-concave zero-sum games. We study the role that a finite timescale separation parameter $\tau$ has on the learning dynamics where the learning rate of player 1 is denoted by $\gamma$1 and the learning rate of player 2 is defined to be $\gamma$2 = $\tau$$\gamma$1. Existing work analyzing the role of timescale separation in gradient descent-ascent has primarily focused on the edge cases of players sharing a learning rate ($\tau$ = 1) and the maximizing player approximately converging between each update of the minimizing player ($\tau$ → ∞). For the parameter choice of $\tau$ = 1, it is known that the learning dynamics are not guaranteed to converge to a game-theoretically meaningful equilibria in general as shown by Mazumdar et al. (2020) and Daskalakis and Panageas (2018). In contrast, Jin et al. (2020) showed that the stable critical points of gradient descent-ascent coincide with the set of strict local minmax equilibria as $\tau$ → ∞. In this work, we bridge the gap between past work by showing there exists a finite timescale separation parameter $\tau$∗ such that x∗ is a stable critical point of gradient descent-ascent for all $\tau$ ∈ ($\tau$∗,∞) if and only if it is a strict local minmax equilibrium. Moreover, we provide an explicit construction for computing $\tau$∗ along with corresponding convergence rates and results under deterministic and stochastic gradient feedback. The convergence results we present are complemented by a non-convergence result: given a critical point x∗ that is not a strict local minmax equilibrium, then there exists a finite timescale separation $\tau$0 such that x∗ is unstable for all $\tau$ ∈ ($\tau$0,∞). Finally, we extend the stability and convergence results regarding gradient descent-ascent to gradient penalty regularization methods for generative adversarial networks (Mescheder et al., 2018) and empirically demonstrate on the CIFAR-10 and CelebA datasets the significant impact timescale separation has on training performance.},
annote = {REF time-scale separation is crucial for minimax (lagrange in my case)},
archivePrefix = {arXiv},
arxivId = {2009.14820},
author = {Fiez, Tanner and Ratliff, Lillian J.},
eprint = {2009.14820},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fiez, Ratliff - 2020 - Gradient descent-ascent provably converges to strict local minmax Equilibria with a finite timescale separation.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--70},
title = {{Gradient descent-ascent provably converges to strict local minmax Equilibria with a finite timescale separation}},
year = {2020}
}
@article{Veness2010,
abstract = {This paper introduces a principled approach for the design of a scalable general reinforcement learning agent. This approach is based on a direct approximation of AIXI, a Bayesian optimality notion for general reinforcement learning agents. Previously, it has been unclear whether the theory of AIXI could motivate the design of practical algorithms. We answer this hitherto open question in the affirmative, by providing the first computationally feasible approximation to the AIXI agent. To develop our approximation, we introduce a Monte Carlo Tree Search algorithm along with an agent-specific extension of the Context Tree Weighting algorithm. Empirically, we present a set of encouraging results on a number of stochastic, unknown, and partially observable domains. Copyright {\textcopyright} 2010, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
annote = {REF for potentially speeding up with a sparser model},
archivePrefix = {arXiv},
arxivId = {1007.2049},
author = {Veness, Joel and {Siong Ng}, Kee and Hutter, Marcus and Silver, David},
eprint = {1007.2049},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Veness et al. - 2010 - Reinforcement learning via AIXI approximation(2).pdf:pdf},
isbn = {9781577354642},
journal = {Proceedings of the National Conference on Artificial Intelligence},
keywords = {Technical Papers -- Machine Learning},
number = {3},
pages = {605--611},
title = {{Reinforcement learning via AIXI approximation}},
volume = {1},
year = {2010}
}
@article{Tononi2016,
abstract = {In this Opinion article, we discuss how integrated information theory accounts for several aspects of the relationship between consciousness and the brain. Integrated information theory starts from the essential properties of phenomenal experience, from which it derives the requirements for the physical substrate of consciousness. It argues that the physical substrate of consciousness must be a maximum of intrinsic cause-effect power and provides a means to determine, in principle, the quality and quantity of experience. The theory leads to some counterintuitive predictions and can be used to develop new tools for assessing consciousness in non-communicative patients.},
annote = {REF IIT},
author = {Tononi, Giulio and Boly, Melanie and Massimini, Marcello and Koch, Christof},
doi = {10.1038/nrn.2016.44},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tononi et al. - 2016 - Integrated information theory From consciousness to its physical substrate(2).pdf:pdf},
issn = {14710048},
journal = {Nature Reviews Neuroscience},
number = {7},
pages = {450--461},
pmid = {27225071},
publisher = {Nature Publishing Group},
title = {{Integrated information theory: From consciousness to its physical substrate}},
volume = {17},
year = {2016}
}
@book{Hoang2020,
annote = {REF bayes simple},
author = {Hoang, Le Nguyen},
edition = {1st},
publisher = {Chapman and Hall/CRC},
title = {{The Equation of Knowledge}},
year = {2020}
}
@article{DesJardins1993,
annote = {goals, selecting goals

using minimal-description length to search for models},
author = {DesJardins, Marie},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Learning - 1993 - P AGODA A Model for.pdf:pdf},
journal = {AI Magazine},
number = {1},
pages = {75--76},
title = {{PAGODA: a model for autonomous learning in probabilistic domains}},
volume = {14},
year = {1993}
}
@article{Mooij2016,
abstract = {The gold standard for discovering causal relations is by means of experimentation. Over the last decades, alternative methods have been proposed that can infer causal relations between variables from certain statistical patterns in purely observational data. We introduce Joint Causal Inference (JCI), a novel approach to causal discovery from multiple data sets that elegantly unifies both approaches. JCI is a causal modeling approach rather than a specific algorithm, and it can be used in combination with any causal discovery algorithm that can take into account certain background knowledge. The main idea is to reduce causal discovery from multiple datasets originating from different contexts (e.g., different experimental conditions) to causal discovery from a single pooled dataset by adding auxiliary context variables and incorporating applicable background knowledge on the causal relationships involving the context variables. We propose different flavours of JCI that differ in the amount of background knowledge that is assumed. JCI can deal with several different types of interventions in a unified fashion, does not require knowledge on intervention targets or types in case of interventional data, and allows one to fully exploit all the information in the joint distribution on system and context variables. We explain how some well-known causal discovery algorithms can be seen as implementations of the JCI framework, but we also propose novel implementations that are simple adaptations of existing causal discovery methods for purely observational data to the JCI setting. We evaluate different implementations of the JCI approach on synthetic data and on flow cytometry protein expression data and conclude that JCI implementations can outperform state-of-the-art causal discovery algorithms.},
annote = {REF adding a context variable (give the mask)

Aggregating data from multiple datasetss},
archivePrefix = {arXiv},
arxivId = {1611.10351},
author = {Mooij, Joris M. and Magliacane, Sara and Claassen, Tom},
eprint = {1611.10351},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mooij, Magliacane, Claassen - 2016 - Joint Causal Inference from Multiple Contexts.pdf:pdf},
title = {{Joint Causal Inference from Multiple Contexts}},
url = {http://arxiv.org/abs/1611.10351},
year = {2016}
}
@article{Pathak2017,
annote = {REF curoisoty

Reward for falsifying the model},
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pathak et al. - 2017 - Curiosity-driven Exploration by Self-supervised Prediction.pdf:pdf},
title = {{Curiosity-driven Exploration by Self-supervised Prediction}},
year = {2017}
}
@article{Chalupka2017,
abstract = {Causal feature learning (CFL) (Chalupka et al. is a causal inference framework rooted in the language of causal graphical models (Pearl J, Reasoning and inference. Cambridge University Press, Cambridge, 2009; Spirtes et al., Causation, Prediction, and Search. Massachusetts Institute of Technology, Massachusetts, 2000), and computational mechanics (Shalizi, PhD thesis, University of Wisconsin at Madison, 2001). CFL is aimed at discovering high-level causal relations from low-level data, and at reducing the experimental effort to understand confounding among the high-level variables. We first review the scientific motivation for CFL, then present a detailed introduction to the framework, laying out the definitions and algorithmic steps. A simple example illustrates the techniques involved in the learning steps and provides visual intuition. Finally, we discuss the limitations of the current framework and list a number of open problems.},
annote = {REF no-decoder causal learning with neural networks (only grouping)


Learning features from low-level data which are causally related

Microvariables ={\textgreater} Macrovariables ={\textgreater} causal relationships

Can do it automatically. Using NNs to learn. Need interventions for every variable?},
author = {Chalupka, Krzysztof and Eberhardt, Frederick and Perona, Pietro},
doi = {10.1007/s41237-016-0008-2},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chalupka, Eberhardt, Perona - 2017 - Causal feature learning an overview.pdf:pdf},
isbn = {4123701600082},
issn = {0385-7417},
journal = {Behaviormetrika},
keywords = {Causal discovery,Causal inference,Graphical models,causal discovery {\'{a}} causal,communicated by shohei shimizu,inference {\'{a}} graphical models,krzysztof chalupka,multiscale modeling,networks {\'{a}} macrovariables {\'{a}},{\'{a}} bayesian},
number = {1},
pages = {137--164},
publisher = {Springer Japan},
title = {{Causal feature learning: an overview}},
volume = {44},
year = {2017}
}
@article{Everitt2019,
abstract = {Agents are systems that optimize an objective function in an environment. Together, the goal and the environment induce secondary objectives, incentives. Modeling the agent-environment interaction in graphical models called influence diagrams, we can answer two fundamental questions about an agent's incentives directly from the graph: (1) which nodes is the agent incentivized to observe, and (2) which nodes is the agent incentivized to influence? The answers tell us which information and influence points need extra protection. For example, we may want a classifier for job applications to not use the ethnicity of the candidate, and a reinforcement learning agent not to take direct control of its reward mechanism. Different algorithms and training paradigms can lead to different influence diagrams, so our method can be used to identify algorithms with problematic incentives and help in designing algorithms with better incentives.},
annote = {REF causality for interpretability

Causal Influence diagrams def and do-properties},
archivePrefix = {arXiv},
arxivId = {1902.09980},
author = {Everitt, Tom and Ortega, Pedro A. and Barnes, Elizabeth and Legg, Shane},
eprint = {1902.09980},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Everitt et al. - 2019 - Understanding Agent Incentives using Causal Influence Diagrams. Part I Single Action Settings.pdf:pdf},
title = {{Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings}},
url = {http://arxiv.org/abs/1902.09980},
year = {2019}
}
@article{Ave,
annote = {Similar to our approach: learning the effect of actions as conditional distributions. using logical formulas.

do-calculus (without calling it so)

Nodes can group into unacceptable features

!!sparsity of the model},
author = {DesJardins, Marie},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ave, Ca - Unknown - Representing and Reasoning With Probabilistic Knowledge A Bayesian Approach P ( Y ) n L P ( GB IC ) P ( Llu ( t.pdf:pdf},
title = {{Representing and Reasoning With Probabilistic Knowledge : A Bayesian Approach}},
year = {1993}
}
@techreport{Dasgupta2019,
abstract = {Discovering and exploiting the causal structure in the environment is a crucial challenge for intelligent agents. Here we explore whether modern deep reinforcement learning can be used to train agents to perform causal reasoning. We adopt a meta-learning approach, where the agent learns a policy for conducting experiments via causal interventions, in order to support a subsequent task which rewards making accurate causal inferences. We also found the agent could make sophisticated counterfactual predictions, as well as learn to draw causal inferences from purely observational data. Though powerful formalisms for causal reasoning have been developed, applying them in real-world domains can be difficult because fitting to large amounts of high dimensional data often requires making idealized assumptions. Our results suggest that causal reasoning in complex settings may benefit from powerful learning-based approaches. More generally, this work may offer new strategies for structured exploration in reinforcement learning, by providing agents with the ability to perform-and interpret-experiments.},
annote = {REF structured exploration via causal graphs

Causal reasoning can arise even in model-free agents

too simple envs?},
author = {Dasgupta, Ishita and Wang, Jane and Chiappa, Silvia and Mitrovic, Jovana and Ortega, Pedro and Raposo, David and Hughes, Edward and Battaglia, Peter and Botvinick, Matthew and Kurth-Nelson, Zeb},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - CAUSAL REASONING FROM META REINFORCEMENT LEARNING.pdf:pdf},
title = {{CAUSAL REASONING FROM META REINFORCEMENT LEARNING}},
year = {2019}
}
@article{Ke2019,
abstract = {Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing parameters and slowly-changing meta-parameters. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions robust to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph.},
annote = {REF bernoulli distibution and separate networks

CAN USE AS AN ELEMENT for learning from RL data in our Artificla Science method because here we learn the causal model in a differentiable way from unknown interventions

Algo:
1. Add a component to reward to value X{\_}i=1, collect trajectories into the dataset for this model
2. Re-train this model

NO CODE?

NN for learning causal model
slow meta/fast normal.

Meta params + fcn=NN

Efficient parameterization of an exponential number of SCM DAGs},
archivePrefix = {arXiv},
arxivId = {1910.01075},
author = {Ke, Nan Rosemary and Bilaniuk, Olexa and Goyal, Anirudh and Bauer, Stefan and Larochelle, Hugo and Pal, Chris and Bengio, Yoshua},
eprint = {1910.01075},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ke et al. - 2019 - Learning Neural Causal Models from Unknown Interventions.pdf:pdf},
month = {oct},
title = {{Learning Neural Causal Models from Unknown Interventions}},
url = {http://arxiv.org/abs/1910.01075},
year = {2019}
}
@article{He2008,
abstract = {The causal discovery from data is important for various scientific investigations. Because we cannot distinguish the different directed acyclic graphs (DAGs) in a Markov equivalence class learned from observational data, we have to collect further information on causal structures from experiments with external interventions. In this paper, we propose an active learning approach for discovering causal structures in which we first find a Markov equivalence class from observational data, and then we orient undirected edges in every chain component via intervention experiments separately. In the experiments, some variables are manipulated through external interventions. We discuss two kinds of intervention experiments, randomized experiment and quasi-experiment. Furthermore, we give two optimal designs of experiments, a batch-intervention design and a sequential-intervention design, to minimize the number of manipulated variables and the set of candidate structures based on the minimax and the maximum entropy criteria. We show theoretically that structural learning can be done locally in subgraphs of chain components without need of checking illegal v-structures and cycles in the whole network and that a Markov equivalence subclass obtained after each intervention can still be depicted as a chain graph.},
annote = {An approach for active causal discovery. Graph considered explicitly. Theory. Maximum entropy criterion for intervention design

REF for intervention design},
author = {He, Yang Bo and Geng, Zhi},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He, Geng - 2008 - Active learning of causal networks with intervention experiments and optimal designs.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Active learning,Causal networks,Directed acyclic graphs,Intervention,Markov equivalence class,Optimal design,Structural learning},
pages = {2523--2547},
title = {{Active learning of causal networks with intervention experiments and optimal designs}},
volume = {9},
year = {2008}
}
@article{Nauta2019,
abstract = {Having insight into the causal associations in a complex system facilitates decision making, e.g., for medical treatments, urban infrastructure improvements or financial investments. The amount of observational data grows, which enables the discovery of causal relationships between variables from observation of their behaviour in time. Existing methods for causal discovery from time series data do not yet exploit the representational power of deep learning. We therefore present the Temporal Causal Discovery Framework (TCDF), a deep learning framework that learns a causal graph structure by discovering causal relationships in observational time series data. TCDF uses attention-based convolutional neural networks combined with a causal validation step. By interpreting the internal parameters of the convolutional networks, TCDF can also discover the time delay between a cause and the occurrence of its effect. Our framework learns temporal causal graphs, which can include confounders and instantaneous effects. Experiments on financial and neuroscientific benchmarks show state-of-the-art performance of TCDF on discovering causal relationships in continuous time series data. Furthermore, we show that TCDF can circumstantially discover the presence of hidden confounders. Our broadly applicable framework can be used to gain novel insights into the causal dependencies in a complex system, which is important for reliable predictions, knowledge discovery and data-driven decision making.},
annote = {REF attention-based multi-step causal (no feature learning)

Better methods for graph learning

PyTorch library},
author = {Nauta, Meike and Bucur, Doina and Seifert, Christin},
doi = {10.3390/make1010019},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nauta, Bucur, Seifert - 2019 - Causal Discovery with Attention-Based Convolutional Neural Networks.pdf:pdf},
journal = {Machine Learning and Knowledge Extraction},
keywords = {attention,causal discovery,convolutional neural network,machine learning,time series},
number = {1},
pages = {312--340},
title = {{Causal Discovery with Attention-Based Convolutional Neural Networks}},
volume = {1},
year = {2019}
}
@article{DeHaan2019,
abstract = {Behavioral cloning reduces policy learning to supervised learning by training a discriminative model to predict expert actions given observations. Such discriminative models are non-causal: the training procedure is unaware of the causal structure of the interaction between the expert and the environment. We point out that ignoring causality is particularly damaging because of the distributional shift in imitation learning. In particular, it leads to a counter-intuitive "causal misidentification" phenomenon: access to more information can yield worse performance. We investigate how this problem arises, and propose a solution to combat it through targeted interventions---either environment interaction or expert queries---to determine the correct causal model. We show that causal misidentification occurs in several benchmark control domains as well as realistic driving settings, and validate our solution against DAgger and other baselines and ablations.},
annote = {REF giving the mask to the input

REF sampling graphs

but here sample from uniform graph distribution

Identifying non-causal variables. Learning variables from data as well

Just subset of variables},
archivePrefix = {arXiv},
arxivId = {1905.11979},
author = {de Haan, Pim and Jayaraman, Dinesh and Levine, Sergey},
eprint = {1905.11979},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/de Haan, Jayaraman, Levine - 2019 - Causal Confusion in Imitation Learning.pdf:pdf},
number = {NeurIPS},
pages = {1--17},
title = {{Causal Confusion in Imitation Learning}},
url = {http://arxiv.org/abs/1905.11979},
year = {2019}
}
@article{Frisch2014,
abstract = {Accounts of causal discovery have traditionally split into approaches based on passive observational data and approaches based on experimental interventions that take control of (the distribution of) one or more variables. The former includes a vast number of techniques for the inference to causal structure on the basis of statistical features of data, while the latter provides in addition a methodology of how an experiment should be performed, in order to be infor-mative about causal structure. In this thesis, the causal Bayes net framework is used to integrate these two approaches and general guidelines are provided not only of how experiments should be performed but also which experiments should be performed to discover the causal structure among a potentially large number of random variables. In that sense this thesis aims to extend consid-erations found in experimental design from single experiments to sequences of experiments. To do so, the thesis provides a precise account of what constitutes an intervention that allows for, but does not necessessitate, a role of agency in interventions. We describe a space of interventions that is broader than standard randomized controlled trials, and explore what implications follow for discov-ery when different types of interventions are used. Results pertaining to the methodology of causal discovery, its limits, the efficiency of its search strategies and the meta-analysis of experimental results are presented. This thesis analy-ses the combinatorics of sequences of experiments for causal discovery, ties the discovery problem into a game-theoretic framework and points to some of the (many) difficulties that remain open research questions.},
annote = {REF intervention design

Big thesis on designing interventions},
author = {Frisch, Mathias and Frisch, Mathias},
doi = {10.1017/cbo9781139381772.004},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frisch, Frisch - 2014 - Causation and intervention.pdf:pdf},
journal = {Causal Reasoning in Physics},
pages = {77--110},
title = {{Causation and intervention}},
year = {2014}
}
@article{Kreutz-delgado2003,
abstract = {Algorithms for data-driven learning of domain-specific overcomplete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave/Schur-concave (CSC) negative log priors. Such priors are appropriate for obtaining sparse representations of environmental signals within an appropriately chosen (environmentally matched) dictionary. The elements of the dictionary can be interpreted as concepts, features, or words capable of succinct expression of events encountered in the environment (the source of the measured signals). This is a generalization of vector quantization in that one is interested in a description involving a few dictionary entries (the proverbial "25 words or less"), but not necessarily as succinct as one entry. To learn an environmentally adapted dictionary capable of concise expression of signals generated by the environment, we develop algorithms that iterate between a representative set of sparse representations found by variants of FOCUSS and an update of the dictionary using these sparse representations. Experiments were performed using synthetic data and natural images. For complete dictionaries, we demonstrate that our algorithms have improved performance over other independent component analysis (ICA) methods, measured in terms of signal-to-noise ratios of separated sources. In the overcomplete case, we show that the true underlying dictionary and sparse sources can be accurately recovered. In tests with natural images, learned overcomplete dictionaries are shown to have higher coding efficiency than complete dictionaries; that is, images encoded with an overcomplete dictionary have both higher compression (fewer bits per pixel) and higher accuracy (lower mean square error).},
annote = {reference for sparse dictionary learning},
archivePrefix = {arXiv},
arxivId = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2944020/pdf/nihms234072.pdf},
author = {Kreutz-delgado, Kenneth and Murray, Joseph F and Sejnowski, Terrence J},
eprint = {/www.ncbi.nlm.nih.gov/pmc/articles/PMC2944020/pdf/nihms234072.pdf},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kreutz-delgado, Murray, Sejnowski - 2003 - Dictionary Learning ALgorithms for Sparse Representation.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
number = {2},
pmid = {12590811},
primaryClass = {http:},
title = {{Dictionary Learning ALgorithms for Sparse Representation}},
volume = {15},
year = {2003}
}
@article{Cha2018,
abstract = {Unsupervised methods have proven effective for discriminative tasks in a single-modality scenario. In this paper, we present a multimodal framework for learning sparse representations that can capture semantic correlation between modalities. The framework can model relationships at a higher level by forcing the shared sparse representation. In particular, we propose the use of joint dictionary learning technique for sparse coding and formulate the joint representation for concision, cross-modal representations (in case of a missing modality), and union of the cross-modal representations. Given the accelerated growth of multimodal data posted on the Web such as YouTube, Wikipedia, and Twitter, learning good multimodal features is becoming increasingly important. We show that the shared representations enabled by our framework substantially improve the classification performance under both unimodal and multimodal settings. We further show how deep architectures built on the proposed framework are effective for the case of highly nonlinear correlations between modalities. The effectiveness of our approach is demonstrated experimentally in image denoising, multimedia event detection and retrieval on the TRECVID dataset (audio-video), category classification on the Wikipedia dataset (image-text), and sentiment classification on PhotoTweet (image-text).},
annote = {REF sparse feature vector},
archivePrefix = {arXiv},
arxivId = {1511.06238},
author = {Cha, Miriam and Gwon, Youngjune and Kung, Hsiang-Tsung},
doi = {10.46397/jaih.2.2},
eprint = {1511.06238},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cha, Gwon, Kung - 2018 - Multimodal Sparse Representation Learning and Applications.pdf:pdf},
issn = {26354691},
journal = {Journal of AI Humanities},
pages = {39--64},
title = {{Multimodal Sparse Representation Learning and Applications}},
volume = {2},
year = {2018}
}
@article{Pei2019,
abstract = {The information bottleneck principle in [24] is an elegant and useful approach to representation learning. In this paper, we investigate the problem of representation learning in the context of reinforcement learning using the information bottleneck framework, aiming at improving the sample efficiency of the learning algorithms. We analytically derive the optimal conditional distribution of the representation, and provide a variational lower bound. Then, we maximize this lower bound with the Stein variational (SV) gradient method (originally developed in [13,14]). We incorporate this framework in the advantageous actor critic algorithm (A2C)[15] and the proximal policy optimization algorithm (PPO) [20]. Our experimental results show that our framework can improve the sample efficiency of vanilla A2C and PPO significantly. Finally, we study the information bottleneck (IB) perspective in deep RL with the algorithm called mutual information neural estimation(MINE) [3]. We experimentally verify that the information extraction-compression process also exists in deep RL and our framework is capable of accelerating this process. We also analyze the relationship between MINE and our method, through this relationship, we theoretically derive an algorithm to optimize our IB framework without constructing the lower bound.},
annote = {REF sparse feature vectors},
archivePrefix = {arXiv},
arxivId = {1911.05695},
author = {Pei, Yingjun and Hou, Xinwen},
eprint = {1911.05695},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pei, Hou - 2019 - Learning Representations in Reinforcement Learning An Information Bottleneck Approach.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Learning Representations in Reinforcement Learning: An Information Bottleneck Approach}},
year = {2019}
}
@article{Rafati2019,
abstract = {Reinforcement learning (RL) algorithms allow artificial agents to improve their selection of actions to increase rewarding experiences in their environments. Temporal Difference (TD) Learning – a model-free RL method – is a leading account of the midbrain dopamine system and the basal ganglia in reinforcement learning. These algorithms typically learn a mapping from the agent's current sensed state to a selected action (known as a policy function) via learning a value function (expected future rewards). TD Learning methods have been very successful on a broad range of control tasks, but learning can become intractably slow as the state space of the environment grows. This has motivated methods that learn internal representations of the agent's state, effectively reducing the size of the state space and restructuring state representations in order to support generalization. However, TD Learning coupled with an artificial neural network, as a function approximator, has been shown to fail to learn some fairly simple control tasks, challenging this explanation of reward-based learning. We hypothesize that such failures do not arise in the brain because of the ubiquitous presence of lateral inhibition in the cortex, producing sparse distributed internal representations that support the learning of expected future reward. The sparse conjunctive representations can avoid catastrophic interference while still supporting generalization. We provide support for this conjecture through computational simulations, demonstrating the benefits of learned sparse representations for three problematic classic control tasks: Puddle-world, Mountain-car, and Acrobot.},
annote = {REF sparse feature vector},
archivePrefix = {arXiv},
arxivId = {1909.01575},
author = {Rafati, Jacob and Noelle, David C.},
eprint = {1909.01575},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rafati, Noelle - 2019 - Learning sparse representations in reinforcement learning.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Catastrophic interference,Dopamine system,Generalization,K-Winners-Take-All (kWTA),Lateral inhibition,Learning representations,Midbrain,Reinforcement learning,Representations,SARSA,Sparse,Temporal Difference Learning},
pages = {1--20},
title = {{Learning sparse representations in reinforcement learning}},
year = {2019}
}
@article{Baze2012,
abstract = {Encephalization, or brain size larger than expected from body size, has long been considered to correlate with improved cognitive abilities across species and even intelligence. However, it is still unknown what characteristics of relatively large brains underlie their improved functions. Here, it is shown that more encephalized rodent species have the number of neurons expected for their brain size, but a larger number of neurons than expected for their body size. The number of neurons in excess relative to body size might be available for improved associative functions and, thus, be responsible for the cognitive advantage observed in more encephalized animals. It is further proposed that, if such neuronal excess does provide for improved cognitive abilities, then the total number of excess neurons in each species - here dubbed the neuronal index - should be a better indicator of cognitive abilities than the encephalization quotient (EQ). Because the neuronal index is a function of both the number of neurons expected from the size of the body and the absolute number of neurons in the brain, differences in this parameter across species that share similar EQs might explain why these often have different cognitive capabilities, particularly when comparing across mammalian orders. {\textcopyright} 2007 Wiley-Liss, Inc.},
annote = {REF sparse dictionary learning},
author = {Baze, Wallace B and Mcarthur, Mark J and Hopkins, William D and Hof, Patrick R and Smaers, Jeroen B. and Dechmann, Dina K.N. and Goswami, Anjali and Soligo, Christophe and Safi, Kamran and Herculano-Houzel, Suzana and Sue, Angela and Hicks, Angela Sue},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baze et al. - 2012 - UC San Diego UC San Diego Electronic Theses and Dissertations by.pdf:pdf},
issn = {00278424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Brain allometry,Brain size,Comparative method,Comparative neuroanatomy,Encephalization,Energetics,Evolution,Number of glia,Number of neurons,Phylogenetic,Rodents,Trait coevolution},
number = {44},
pages = {1280--1287},
pmid = {17847061},
title = {{Sparse Recovery and Representation Learning}},
volume = {290},
year = {2012}
}
@article{Seeger2007,
abstract = {We present a framework for efficient, accurate approximate Bayesian inference in generalized linear models (GLMs), based on the expectation propagation (EP) technique. The parameters can be endowed with a factorizing prior distribution, encoding properties such as sparsity or non-negativity. The central role of posterior log-concavity in Bayesian GLMs is emphasized and related to stability issues in EP. In particular, we use our technique to infer the parameters of a point process model for neuronal spiking data from multiple electrodes, demonstrating significantly superior predictive performance when a sparsity assumption is enforced via a Laplace prior distribution. {\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
annote = {reference for bayesian inference giving a linear combination of losses},
author = {Seeger, Matthias and Gerwinn, Sebastian and Bethge, Matthias},
doi = {10.1007/978-3-540-74958-5_29},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Seeger, Gerwinn, Bethge - 2007 - Bayesian inference for sparse generalized linear models.pdf:pdf},
isbn = {9783540749578},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {298--309},
title = {{Bayesian inference for sparse generalized linear models}},
volume = {4701 LNAI},
year = {2007}
}
@article{Bengio2007,
annote = {reference as learning abstraction},
author = {Bengio, Yoshua and Delalleau, Olivier and Roux, Nicolas Le and Larochelle, Hugo and Lamblin, Pascal and Popovici, Dan and Courville, Aaron and Simard, Clarence and Louradour, Jerome},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio et al. - 2007 - Deep Architectures for Baby AI Summary for Day 1.pdf:pdf},
title = {{Deep Architectures for Baby AI Summary for Day 1}},
year = {2007}
}
@article{Hui2020,
annote = {REF grounded RL-NLP},
archivePrefix = {arXiv},
arxivId = {arXiv:2007.12770v1},
author = {Hui, David Yu-tung and Chevalier-boisvert, Maxime and Bahdanau, Dzmitry and Bengio, Yoshua},
eprint = {arXiv:2007.12770v1},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hui et al. - 2020 - BabyAI 1.1.pdf:pdf},
pages = {1--9},
title = {{BabyAI 1.1}},
volume = {1},
year = {2020}
}
@article{Lahlou2019,
annote = {REF grounded language learning

evaluate?

take environments as BENCHMARK and test grounding},
archivePrefix = {arXiv},
arxivId = {arXiv:1810.08272v4},
author = {Lahlou, Salem},
eprint = {arXiv:1810.08272v4},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lahlou - 2019 - Baby AI a platform to study the sample-efficiency of grounded language learning.pdf:pdf},
pages = {1--19},
title = {{Baby AI: a platform to study the sample-efficiency of grounded language learning}},
year = {2019}
}
@article{Didolkar2021,
annote = {REF reusable networks},
archivePrefix = {arXiv},
arxivId = {arXiv:2103.01937v1},
author = {Didolkar, Aniket and Goyal, Anirudh and Rosemary, Nan and Charles, Ke and Philippe, Blundell and Heess, Nicolas and Mozer, Michael and Bengio, Yoshua},
eprint = {arXiv:2103.01937v1},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Didolkar et al. - Unknown - Neural Production Systems.pdf:pdf},
title = {{Neural Production Systems}},
year = {2021}
}
@article{Fu2013,
annote = {REF vanilla sparsity-regularized causal models (no decoder)},
author = {Fu, Fei and Zhou, Qing},
doi = {10.1080/01621459.2012.754359},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu, Zhou - 2013 - Learning Sparse Causal Gaussian Networks With Experimental Intervention Regularization and Coordinate Descent With Ex.pdf:pdf},
keywords = {adaptive lasso,experimental data,l 1 regularization,penalized likelihood,structure learning},
title = {{Learning Sparse Causal Gaussian Networks With Experimental Intervention : Regularization and Coordinate Descent With Experimental Intervention : Regularization}},
volume = {1459},
year = {2013}
}
@article{Priol2021,
annote = {REF faster adaptation with a correct causal model -{\textgreater} distributional shift robustness},
archivePrefix = {arXiv},
arxivId = {arXiv:2005.09136v2},
author = {Priol, R{\'{e}}mi Le and Lacoste-julien, Simon},
eprint = {arXiv:2005.09136v2},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Priol, Lacoste-julien - 2021 - An Analysis of the Adaptation Speed of Causal Models.pdf:pdf},
title = {{An Analysis of the Adaptation Speed of Causal Models}},
volume = {130},
year = {2021}
}
@article{Lamb2020,
annote = {ref sparse input to models},
archivePrefix = {arXiv},
arxivId = {arXiv:2010.08012v1},
author = {Lamb, Alex and Mozer, Michael and Beaudoin, Philippe},
eprint = {arXiv:2010.08012v1},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lamb, Mozer, Beaudoin - Unknown - Neural Function Modules with Sparse Arguments A Dynamic Approach to Integrating Information across La.pdf:pdf},
title = {{Neural Function Modules with Sparse Arguments : A Dynamic Approach to Integrating Information across Layers}},
year = {2020}
}
@article{Goyal2021,
annote = {REF sparsity of factor graph as a prior for cognition},
archivePrefix = {arXiv},
arxivId = {arXiv:2011.15091v3},
author = {Goyal, Anirudh},
eprint = {arXiv:2011.15091v3},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goyal - Unknown - Inductive Biases for Deep Learning of Higher-Level Cognition.pdf:pdf},
pages = {1--42},
title = {{Inductive Biases for Deep Learning of Higher-Level Cognition}},
year = {2021}
}
@article{Deleu2015,
annote = {NEWMETHOD

proximal gradient descent for sparsity},
archivePrefix = {arXiv},
arxivId = {arXiv:2102.03869v1},
author = {Deleu, Tristan and Bengio, Yoshua},
eprint = {arXiv:2102.03869v1},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deleu, Bengio - 2015 - Structured Sparsity Inducing Adaptive Optimizers for Deep Learning.pdf:pdf},
title = {{Structured Sparsity Inducing Adaptive Optimizers for Deep Learning}},
year = {2015}
}
@article{Veerapaneni2019,
abstract = {This paper tests the hypothesis that modeling a scene in terms of entities and their local interactions, as opposed to modeling the scene globally, provides a significant benefit in generalizing to physical tasks in a combinatorial space the learner has not encountered before. We present object-centric perception, prediction, and planning (OP3), which to the best of our knowledge is the first entity-centric probabilistic dynamic latent variable framework for model-based reinforcement learning that acquires entity representations from raw visual observations without supervision and uses them to predict and plan. OP3 enforces entity-abstraction – symmetric processing of each entity representation with the same locally-scoped function – which enables it to scale to model different numbers and configurations of objects from those in training. Our approach to solving the key technical challenge of grounding these entity representations to actual objects in the environment is to frame this variable binding problem as an inference problem, and we develop an interactive inference algorithm that uses temporal continuity and interactive feedback to bind information about object properties to the entity variables. On block-stacking tasks, OP3 generalizes to novel block configurations and more objects than observed during training, outperforming an oracle model that assumes access to object supervision and achieving two to three times better accuracy than a state-of-the-art video prediction model that does not exhibit entity abstraction.},
annote = {REF reusing model components},
archivePrefix = {arXiv},
arxivId = {1910.12827},
author = {Veerapaneni, Rishi and Co-Reyes, John D. and Chang, Michael and Janner, Michael and Finn, Chelsea and Wu, Jiajun and Tenenbaum, Joshua B. and Levine, Sergey},
eprint = {1910.12827},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Veerapaneni et al. - 2019 - Entity abstraction in visual model-based reinforcement learning.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Compositionality,Model-based reinforcement learning,Objects},
number = {CoRL},
pages = {1--18},
title = {{Entity abstraction in visual model-based reinforcement learning}},
year = {2019}
}
@article{Adegbege2021,
annote = {REF lagrange primal-dual converges},
author = {Adegbege, Ambrose A and Member, Senior and Kim, Mun Y},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Adegbege, Member, Kim - 2021 - Saddle-Point Convergence of Constrained Primal-Dual Dynamics.pdf:pdf},
number = {4},
pages = {1357--1362},
title = {{Saddle-Point Convergence of Constrained Primal-Dual Dynamics}},
volume = {5},
year = {2021}
}
@article{Ayoub2020,
annote = {REF mbrl value},
author = {Ayoub, Alex and Jia, Zeyu and Szepesv, Csaba and Lin, Wang},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ayoub et al. - 2020 - Model-Based Reinforcement Learning with Value-Targeted Regression.pdf:pdf},
title = {{Model-Based Reinforcement Learning with Value-Targeted Regression}},
year = {2020}
}
@article{Mitchell2020,
annote = {REF general abstraction learning},
archivePrefix = {arXiv},
arxivId = {2102.10717},
author = {Mitchell, Melanie},
doi = {10.1162/isal_a_00354},
eprint = {2102.10717},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitchell - 2020 - Conceptual Abstraction and Analogy in Artificial Intelligence.pdf:pdf},
pages = {7--7},
title = {{Conceptual Abstraction and Analogy in Artificial Intelligence}},
year = {2020}
}
@article{Scholkopf2021,
annote = {REF structural causal models
"Problem 1 Learning Disentangled Representations" -- our case},
archivePrefix = {arXiv},
arxivId = {arXiv:2102.11107v1},
author = {Sch{\"{o}}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal},
eprint = {arXiv:2102.11107v1},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sch{\"{o}}lkopf et al. - Unknown - Towards Causal Representation Learning.pdf:pdf},
pages = {1--24},
title = {{Towards Causal Representation Learning}},
year = {2021}
}
@article{Learning2014,
annote = {REF disentanglement

REF repr learning via contrastive loss},
archivePrefix = {arXiv},
arxivId = {arXiv:2102.10543v1},
author = {Learning, Contrastive and Need, You},
eprint = {arXiv:2102.10543v1},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Learning, Need - 2014 - Do Generative Models Know Disentanglement Contrastive Learning is All You Need.pdf:pdf},
title = {{Do Generative Models Know Disentanglement? Contrastive Learning is All You Need}},
year = {2014}
}
@article{Ibrahim2020,
annote = {REF disentanglement and the need for actions

explicit transformation latent variable "action"},
archivePrefix = {arXiv},
arxivId = {arXiv:2102.05623v1},
author = {Ibrahim, Mark},
eprint = {arXiv:2102.05623v1},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ibrahim - 2020 - Addressing the topological defects of disentanglement via distributed operators.pdf:pdf},
pages = {1--46},
title = {{Addressing the topological defects of disentanglement via distributed operators}},
year = {2020}
}
@article{Yarats2016,
annote = {REF RL representation learning [other types] -- prototypes},
archivePrefix = {arXiv},
arxivId = {arXiv:2102.11271v1},
author = {Yarats, Denis and Fergus, Rob and Lazaric, Alessandro and Pinto, Lerrel},
eprint = {arXiv:2102.11271v1},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yarats et al. - 2016 - Reinforcement Learning with Prototypical Representations.pdf:pdf},
title = {{Reinforcement Learning with Prototypical Representations}},
year = {2016}
}
@article{Chari2020,
abstract = {Explainability has been an important goal since the early days of Artificial Intelligence. Several approaches for producing explanations have been developed. However, many of these approaches were tightly coupled with the capabilities of the artificial intelligence systems at the time. With the proliferation of AI-enabled systems in sometimes critical settings, there is a need for them to be explainable to end-users and decision-makers. We present a historical overview of explainable artificial intelligence systems, with a focus on knowledge-enabled systems, spanning the expert systems, cognitive assistants, semantic applications, and machine learning domains. Additionally, borrowing from the strengths of past approaches and identifying gaps needed to make explanations user- and contextfocused, we propose new definitions for explanations and explainable knowledgeenabled systems.},
annote = {REF explainability + causation as one of the methods},
archivePrefix = {arXiv},
arxivId = {2003.07520},
author = {Chari, Shruthi and Gruen, Daniel M. and Seneviratne, Oshani and McGuinness, Deborah L.},
eprint = {2003.07520},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chari et al. - 2020 - Foundations of explainable knowledge-enabled systems.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Explainable Knowledge-Enabled Systems,Historical Evolution,KG4XAI},
number = {March},
title = {{Foundations of explainable knowledge-enabled systems}},
year = {2020}
}
@article{Lillicrap2019,
annote = {REF sparse feature vector},
archivePrefix = {arXiv},
arxivId = {arXiv:2010.02193v2},
author = {Lillicrap, Timothy and Ba, Jimmy},
eprint = {arXiv:2010.02193v2},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lillicrap, Ba - 2019 - Mastering Atari with discrete world models.pdf:pdf},
pages = {1--24},
title = {{Mastering Atari with discrete world models}},
year = {2019}
}
@article{Dieleman2021,
abstract = {Semantically meaningful information content in perceptual signals is usually unevenly distributed. In speech signals for example, there are often many silences, and the speed of pronunciation can vary considerably. In this work, we propose slow autoencoders (SlowAEs) for unsupervised learning of high-level variable-rate discrete representations of sequences, and apply them to speech. We show that the resulting event-based representations automatically grow or shrink depending on the density of salient information in the input signals, while still allowing for faithful signal reconstruction. We develop run-length Transformers (RLTs) for event-based representation modelling and use them to construct language models in the speech domain, which are able to generate grammatical and semantically coherent utterances and continuations.},
annote = {REF manual update ('annealing') multiplicative update},
archivePrefix = {arXiv},
arxivId = {2103.06089},
author = {Dieleman, Sander and Nash, Charlie and Engel, Jesse and Simonyan, Karen},
eprint = {2103.06089},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dieleman et al. - 2021 - Variable-rate discrete representation learning.pdf:pdf},
title = {{Variable-rate discrete representation learning}},
url = {http://arxiv.org/abs/2103.06089},
year = {2021}
}
@article{Claassen2013,
abstract = {This paper shows that causal model discovery is not an NP-hard problem, in the sense that for sparse graphs bounded by node degree k the sound and complete causal model can be obtained in worst case order N2(k+2) independence tests, even when latent variables and selection bias may be present. We present a modification of the well-known FCI algorithm that implements the method for an independence oracle, and suggest improvements for sample/real-world data versions. It does not contradict any known hardness results, and does not solve an NP-hard problem: it just proves that sparse causal discovery is perhaps more complicated, but not as hard as learning minimal Bayesian networks.},
annote = {!REF no decoder -{\textgreater} not NP-hard

Importance of our work (decoder makes things hard)},
archivePrefix = {arXiv},
arxivId = {1309.6824},
author = {Claassen, Tom and Mooij, Joris M. and Heskes, Tom},
eprint = {1309.6824},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Claassen, Mooij, Heskes - 2013 - Learning sparse causal models is not NP-hard.pdf:pdf},
journal = {Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013},
pages = {172--181},
title = {{Learning sparse causal models is not NP-hard}},
year = {2013}
}
@article{Jonsson2006,
abstract = {We present Variable Influence Structure Analysis, or VISA, an algorithm that performs hierarchical decomposition of factored Markov decision processes. VISA uses a dynamic Bayesian network model of actions, and constructs a causal graph that captures relationships between state variables. In tasks with sparse causal graphs VISA exploits structure by introducing activities that cause the values of state variables to change. The result is a hierarchy of activities that together represent a solution to the original task. VISA performs state abstraction for each activity by ignoring irrelevant state variables and lower-level activities. In addition, we describe an algorithm for constructing compact models of the activities introduced. State abstraction and compact activity models enable VISA to apply efficient algorithms to solve the stand-alone subtask associated with each activity. Experimental results show that the decomposition introduced by VISA can significantly accelerate construction of an optimal, or near-optimal, policy.},
annote = {classical algorithm, states are grouped into meta-states. state-variables -- not the same as we mean?

add a reference of "previous causal rl classical algorithms without neural stuff"},
author = {Jonsson, Anders and Barto, Andrew},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jonsson, Barto - 2006 - Causal graph based decomposition of factored MDPs.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Hierarchical decomposition,Markov decision processes,State abstraction},
pages = {2259--2301},
title = {{Causal graph based decomposition of factored MDPs}},
volume = {7},
year = {2006}
}
@article{Liu2018,
abstract = {We investigate sparse representations for control in reinforcement learning. While these representations are widely used in computer vision, their prevalence in reinforcement learning is limited to sparse coding where extracting representations for new data can be computationally intensive. Here, we begin by demonstrating that learning a control policy incrementally with a representation from a standard neural network fails in classic control domains, whereas learning with a representation obtained from a neural network that has sparsity properties enforced is effective. We provide evidence that the reason for this is that the sparse representation provides locality, and so avoids catastrophic interference, and particularly keeps consistent, stable values for bootstrapping. We then discuss how to learn such sparse representations. We explore the idea of Distributional Regularizers, where the activation of hidden nodes is encouraged to match a particular distribution that results in sparse activation across time. We identify a simple but effective way to obtain sparse representations, not afforded by previously proposed strategies, making it more practical for further investigation into sparse representations for reinforcement learning.},
annote = {REF sparse features},
author = {Liu, Vincent and Kumaraswamy, Raksha and Le, Lei and White, Martha},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2018 - The utility of sparse representations for control in reinforcement learning.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Reinforcement Learning, Representation Learning},
title = {{The utility of sparse representations for control in reinforcement learning}},
year = {2018}
}
@article{Kurutach2021,
abstract = {Discrete representations have been key in enabling robots to plan at more abstract levels and solve temporally-extended tasks more efficiently for decades. However, they typically require expert specifications. On the other hand, deep reinforcement learning aims to learn to solve tasks end-to-end, but struggles with long-horizon tasks. In this work, we propose Discrete Object-factorized Representation Planning (DORP), which learns temporally-abstracted discrete representations from exploratory video data in an unsupervised fashion via a mutual information max-imization objective. DORP plans a sequence of abstract states for a low-level model-predictive controller to follow. In our experiments, we show that DORP robustly solves unseen long-horizon tasks. Interestingly, it discovers independent representations per object and binary properties such as a key-and-door.},
annote = {REF causal model on [slow] features, no sparsity},
author = {Kurutach, Thanard and Russell, Stuart and Abbeel, Pieter},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kurutach, Russell, Abbeel - 2021 - Discrete Predictive Representation for Long-Horizon Planning.pdf:pdf},
journal = {ICLR openrevivew},
pages = {1--10},
title = {{Discrete Predictive Representation for Long-Horizon Planning}},
url = {https://openreview.net/forum?id=jcpcUjw7Kzz},
year = {2021}
}
@article{Bakhtin2019,
abstract = {Understanding and reasoning about physics is an important ability of intelligent agents. We develop the PHYRE benchmark for physical reasoning that contains a set of simple classical mechanics puzzles in a 2D physical environment. The benchmark is designed to encourage the development of learning algorithms that are sample-efficient and generalize well across puzzles. We test several modern learning algorithms on PHYRE and find that these algorithms fall short in solving the puzzles efficiently. We expect that PHYRE will encourage the development of novel sample-efficient agents that learn efficient but useful models of physics. For code and to play PHYRE for yourself, please visit https://player.phyre.ai.},
annote = {ref physical environments for reasoning in rl},
archivePrefix = {arXiv},
arxivId = {1908.05656},
author = {Bakhtin, Anton and van der Maaten, Laurens and Johnson, Justin and Gustafson, Laura and Girshick, Ross},
eprint = {1908.05656},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bakhtin et al. - 2019 - PHYRE A new benchmark for physical reasoning.pdf:pdf},
issn = {23318422},
journal = {arXiv},
number = {NeurIPS},
title = {{PHYRE: A new benchmark for physical reasoning}},
year = {2019}
}
@article{Seth2011,
abstract = {An outstanding challenge in neuroscience is to develop theoretically grounded and practically applicable quantitative measures that are sensitive to conscious level. Such measures should be high for vivid alert conscious wakefulness, and low for unconscious states such as dreamless sleep, coma and general anaesthesia. Here, we describe recent progress in the development of measures of dynamical complexity, in particular causal density and integrated information. These and similar measures capture in different ways the extent to which a system's dynamics are simultaneously differentiated and integrated. Because conscious scenes are distinguished by the same dynamical features, these measures are therefore good candidates for reflecting conscious level. After reviewing the theoretical background, we present new simulation results demonstrating similarities and differences between the measures, and we discuss remaining challenges in the practical application of the measures to empirically obtained data. {\textcopyright} 2011 The Royal Society.},
annote = {REF Connection between causal models and IIT},
author = {Seth, Anil K. and Barrett, Adam B. and Barnett, Lionel},
doi = {10.1098/rsta.2011.0079},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Seth, Barrett, Barnett - 2011 - Causal density and integrated information as measures of conscious level.pdf:pdf},
issn = {1364503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {Causal density,Consciousness,Integrated information},
number = {1952},
pages = {3748--3767},
pmid = {21893526},
title = {{Causal density and integrated information as measures of conscious level}},
volume = {369},
year = {2011}
}
@article{Kaiser2019,
abstract = {Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction - substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.},
annote = {REF mbrl},
archivePrefix = {arXiv},
arxivId = {1903.00374},
author = {Kaiser, {\L}ukasz and Babaeizadeh, Mohammad and Mi{\l}os, Piotr and Osi{\'{n}}ski, B{\l}a{\.{z}}ej and Campbell, Roy H. and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and Mohiuddin, Afroz and Sepassi, Ryan and Tucker, George and Michalewski, Henryk},
eprint = {1903.00374},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaiser et al. - 2019 - Model based reinforcement learning for atari.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Model based reinforcement learning for atari}},
year = {2019}
}
@article{Franceschi2019,
abstract = {Graph neural networks (GNNs) are a popular class of machine learning models whose major advantage is their ability to incorporate a sparse and discrete dependency structure between data points. Unfortunately, GNNs can only be used when such a graph-structure is available. In practice, however, real-world graphs are often noisy and incomplete or might not be available at all. With this work, we propose to jointly learn the graph structure and the parameters of graph convolutional networks (GCNs) by approximately solving a bilevel program that learns a discrete probability distribution on the edges of the graph. This allows one to apply GCNs not only in scenarios where the given graph is incomplete or corrupted but also in those where a graph is not available. We conduct a series of experiments that analyze the behavior of the proposed method and demonstrate that it outperforms related methods by a significant margin.},
annote = {REF bilevel optimization with optimizing until the loss stops decreasing},
author = {Franceschi, Luca and Niepert, Mathias and Pontil, Massimiliano and He, Xiao},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Franceschi et al. - 2019 - Learning discrete structures for graph neural networks.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Learning discrete structures for graph neural networks}},
year = {2019}
}
@article{Zhu2017,
abstract = {In this paper, we propose a new unsupervised spectral feature selection model by embedding a graph regularizer into the framework of joint sparse regression for preserving the local structures of data. To do this, we first extract the bases of training data by previous dictionary learning methods and, then, map original data into the basis space to generate their new representations, by proposing a novel joint graph sparse coding (JGSC) model. In JGSC, we first formulate its objective function by simultaneously taking subspace learning and joint sparse regression into account, then, design a new optimization solution to solve the resulting objective function, and further prove the convergence of the proposed solution. Furthermore, we extend JGSC to a robust JGSC (RJGSC) via replacing the least square loss function with a robust loss function, for achieving the same goals and also avoiding the impact of outliers. Finally, experimental results on real data sets showed that both JGSC and RJGSC outperformed the state-of-the-art algorithms in terms of $\kappa$-nearest neighbor classification performance.},
annote = {REF sparse feature vector},
author = {Zhu, Xiaofeng and Li, Xuelong and Zhang, Shichao and Ju, Chunhua and Wu, Xindong},
doi = {10.1109/TNNLS.2016.2521602},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu et al. - 2017 - Robust Joint Graph Sparse Coding for Unsupervised Spectral Feature Selection.pdf:pdf},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Dimensionality reduction,manifold learning,regression,sparse coding},
number = {6},
pages = {1263--1275},
pmid = {26955053},
publisher = {IEEE},
title = {{Robust Joint Graph Sparse Coding for Unsupervised Spectral Feature Selection}},
volume = {28},
year = {2017}
}
@article{DeBruin2018,
abstract = {Most deep reinforcement learning techniques are unsuitable for robotics, as they require too much interaction time to learn useful, general control policies. This problem can be largely attributed to the fact that a state representation needs to be learned as a part of learning control policies, which can only be done through fitting expected returns based on observed rewards. While the reward function provides information on the desirability of the state of the world, it does not necessarily provide information on how to distill a good, general representation of that state from the sensory observations. State representation learning objectives can be used to help learn such a representation. While many of these objectives have been proposed, they are typically not directly combined with reinforcement learning algorithms. We investigate several methods for integrating state representation learning into reinforcement learning. In these methods, the state representation learning objectives help regularize the state representation during the reinforcement learning, and the reinforcement learning itself is viewed as a crucial state representation learning objective and allowed to help shape the representation. Using autonomous racing tests in the TORCS simulator, we show how the integrated methods quickly learn policies that generalize to new environments much better than deep reinforcement learning without state representation learning.},
annote = {REF model-based RL},
author = {{De Bruin}, Tim and Kober, Jens and Tuyls, Karl and Babuska, Robert},
doi = {10.1109/LRA.2018.2800101},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Bruin et al. - 2018 - Integrating State Representation Learning into Deep Reinforcement Learning.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Deep learning in robotics and automation,learning and adaptive systems,sensor fusion},
number = {3},
pages = {1394--1401},
publisher = {IEEE},
title = {{Integrating State Representation Learning into Deep Reinforcement Learning}},
volume = {3},
year = {2018}
}
@techreport{Bengio2013,
abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of conditional computation, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
annote = {!!REF Background Gumbel etc},
archivePrefix = {arXiv},
arxivId = {1308.3432v1},
author = {Bengio, Yoshua and L{\'{e}}onard, Nicholas and Courville, Aaron},
eprint = {1308.3432v1},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, L{\'{e}}onard, Courville - Unknown - Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation.pdf:pdf},
title = {{Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation}},
year = {2013}
}
@techreport{Tang2019,
abstract = {Graph Convolutional Neural Networks (GCNNs) extend classical CNNs to graph data domain, such as brain networks, social networks and 3D point clouds. It is critical to identify an appropriate graph for the subsequent graph convolution. Existing methods manually construct or learn one fixed graph for all the layers of a GCNN. In order to adapt to the underlying structure of node features in different layers, we propose dynamic learning of graphs and node features jointly in GCNNs. In particular, we cast the graph optimization problem as distance metric learning to capture pairwise similarities of features in each layer. We deploy the Mahalanobis distance metric and further decompose the metric matrix into a low-dimensional matrix, which converts graph learning to the optimization of a low-dimensional matrix for efficient implementation. Extensive experiments on point clouds and citation network datasets demonstrate the superiority of the proposed method in terms of both accuracies and robustness.},
annote = {REF decoder + graph learning together},
author = {Tang, Jiaxiang and Hu, Wei and Gao, Xiang and Guo, Zongming},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2019 - JOINT GRAPH AND FEATURE LEARNING IN GRAPH CONVOLUTIONAL NEURAL NETWORKS A PREPRINT.pdf:pdf},
title = {{JOINT GRAPH AND FEATURE LEARNING IN GRAPH CONVOLUTIONAL NEURAL NETWORKS A PREPRINT}},
year = {2019}
}
@article{Yamada2018,
abstract = {Feature selection problems have been extensively studied for linear estimation, for instance, Lasso, but less emphasis has been placed on feature selection for non-linear functions. In this study, we propose a method for feature selection in high-dimensional non-linear function estimation problems. The new procedure is based on minimizing the `0 norm of the vector of indicator variables that represent if a feature is selected or not. Our approach relies on the continuous relaxation of Bernoulli distributions, which allows our model to learn the parameters of the approximate Bernoulli distributions via gradient descent. This general framework simultaneously minimizes a loss function while selecting relevant features. Furthermore, we provide an information-theoretic justification of incorporating Bernoulli distribution into our approach and demonstrate the potential of the approach on synthetic and real-life applications.},
annote = {REF multiplying by gumbel mask},
archivePrefix = {arXiv},
arxivId = {1810.04247},
author = {Yamada, Yutaro and Lindenbaum, Ofir and Negahban, Sahand and Kluger, Yuval},
eprint = {1810.04247},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yamada et al. - 2018 - Feature selection using stochastic gates.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Machine Learning, ICML},
title = {{Feature selection using stochastic gates}},
year = {2018}
}
@article{Abid2019,
abstract = {We introduce the concrete autoencoder, an endto- end differentiable method for global feature selection, which efficiently identifies a subset of the most informative features and simultaneously learns a neural network to reconstruct the input data from the selected features. Our method is unsupervised, and is based on using a concrete selector layer as the encoder and using a standard neural network as the decoder. During the training phase, the temperature of the concrete selector layer is gradually decreased, which encourages a user-specified number of discrete features to be learned. During test time, the selected features can be used with the decoder network to reconstruct the remaining input features. We evaluate concrete autoencoders on a variety of datasets, where they significantly outperform state-of-theart methods for feature selection and data reconstruction. In particular, on a large-scale gene expression dataset, the concrete autoencoder selects a small subset of genes whose expression levels can be use to impute the expression levels of the remaining genes. In doing so, it improves on the current widely-used expert-curated L1000 landmark genes, potentially reducing measurement costs by 20{\%}. The concrete autoencoder can be implemented by adding just a few lines of code to a standard autoencoder.},
annote = {REF Feature selection with Gumbel, annealing temperature},
author = {Abid, Abubakar and Balin, Muhammad Fatih and Zou, James},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abid, Balin, Zou - 2019 - Concrete autoencoders for differentiable feature selection and reconstruction.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Concrete autoencoders for differentiable feature selection and reconstruction}},
year = {2019}
}
@article{Weib2011,
annote = {NEWMETHOD
REF heuristics for learning binary masks},
author = {Wei{\ss}, Gregor and Meine, Christian},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wei{\ss}, Meine - 2011 - END-TO-END INPUT SELECTION FOR DEEP NEURAL NETWORKS.pdf:pdf},
journal = {Dbw},
number = {1},
pages = {83--87},
title = {{END-TO-END INPUT SELECTION FOR DEEP NEURAL NETWORKS}},
volume = {70},
year = {2011}
}
@article{VandenOord2017,
abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: The encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" - where the latents are ignored when they are paired with a powerful autoregressive decoder - typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
annote = {ref sparse feature vector},
author = {{Van den Oord}, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van den Oord, Vinyals, Kavukcuoglu - 2017 - Neural discrete representation learning.pdf:pdf},
issn = {23318422},
journal = {arXiv},
number = {Nips},
title = {{Neural discrete representation learning}},
year = {2017}
}
@article{Kalainathan2018,
abstract = {A new causal discovery method, Structural Agnostic Modeling (SAM), is presented in this paper. Leveraging both conditional independencies and distributional asymmetries in the data, SAM aims at recovering full causal models from continuous observational data along a multivariate nonparametric setting. The approach is based on a game between d players estimating each variable distribution conditionally to the others as a neural net, and an adversary aimed at discriminating the overall joint conditional distribution, and that of the original data. An original learning criterion combining distribution estimation, sparsity and acyclicity constraints is used to enforce the end-to-end optimization of the graph structure and parameters through stochastic gradient descent. Besides the theoretical analysis of the approach in the large sample limit, SAM is extensively experimentally validated on synthetic and real data.},
annote = {REF structure binary gates},
archivePrefix = {arXiv},
arxivId = {1803.04929},
author = {Kalainathan, Diviyan and Goudet, Olivier and Guyon, Isabelle and Lopez-Paz, David and Sebag, Mich{\`{e}}le},
eprint = {1803.04929},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalainathan et al. - 2018 - Structural agnostic modeling Adversarial learning of causal graphs.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--49},
title = {{Structural agnostic modeling: Adversarial learning of causal graphs}},
year = {2018}
}
@article{Ng2019,
abstract = {Learning causal graphical models based on directed acyclic graphs is an important task in causal discovery and causal inference. We consider a general framework towards efficient causal structure learning with potentially large graphs. Within this framework, we propose a masked gradient-based structure learning method based on binary adjacency matrix that exists for any structural equation model. To enable first-order optimization methods, we use Gumbel-Softmax approach to approximate the binary valued entries of the adjacency matrix, which usually results in real values that are close to zero or one. The proposed method can readily include any differentiable score function and model function for learning causal structures. Experiments on both synthetic and real-world datasets are conducted to show the effectiveness of our approach.},
annote = {REF mask to learn a sparse causal model},
archivePrefix = {arXiv},
arxivId = {1910.08527},
author = {Ng, Ignavier and Fang, Zhuangyan and Zhu, Shengyu and Chen, Zhitang},
eprint = {1910.08527},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng et al. - 2019 - Masked gradient-based causal structure learning.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Masked gradient-based causal structure learning}},
year = {2019}
}
@article{VanSteenkiste2020,
annote = {REF better features -{\textgreater} faster learning, sparse feature vector},
author = {van Steenkiste, Sjoerd},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van Steenkiste - 2020 - Learning Structured Neural Representations for Visual Reasoning Tasks Sjoerd van Steenkiste under the supervisio.pdf:pdf},
journal = {2020},
number = {November},
title = {{Learning Structured Neural Representations for Visual Reasoning Tasks Sjoerd van Steenkiste under the supervision of}},
year = {2020}
}
@article{Citation2021,
annote = {REF MDL principle for interpretability},
author = {Ullrich, K},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Citation - 2021 - UvA-DARE (Digital Academic Repository) A coding perspective on deep latent variable models Ullrich, K.pdf:pdf},
title = {{UvA-DARE (Digital Academic Repository) A coding perspective on deep latent variable models Ullrich, K.}},
year = {2021}
}
@article{Lino2021,
abstract = {Figure 1: Example in the WikiBio dataset (Lebret et al., 2016) showing the biography of Frank Lino. The baseline Pointer-Generator (See et al., 2017) exhibits hallucination. network (See et al., 2017), which contains the phrase criminal defense attorney that is false (but loosely related to FBI in the table). Thus, hallucination can often result from the coupling of model shortcomings (e.g. lack of formal reasoning, learning false correlations), and noise/artifacts in the training data. In this work, we propose a confidence oriented approach which assigns a learned confidence score to each decoder position, and then uses the score in two ways to reduce hallucination: (1) In test, it uses confidence to adjust the output probabilities by a calibration technique (Braverman et al., 2019). (2) In training, we employ a variational Bayes objective to jointly learn the confidence score while allowing the model to skip tokens with a low confidence score to avoid training on reference phrases that are difficult to infer from the source. In Figure 1, our approach leads to a faithful generation that omits the occupation. Empirically, when evaluated on the WikiBio dataset (Lebret et al., 2016), we show that our approach is considerably more faithful to the source than existing state-of-the-art solutions, according to both PARENT precision (Dhingra et al., 2019) and human evaluation.},
annote = {REF sparse dynamics model},
author = {Lino, Frank},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lino - 1938 - Practical Real Time Recurrent Learning with a Sparse Approximation to the Jacobian.pdf:pdf},
pages = {11--39},
title = {{Practical Real Time Recurrent Learning with a Sparse Approximation to the Jacobian}},
year = {2021}
}
@article{Cranmer2020,
abstract = {We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example—a detailed dark matter simulation—and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution-data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.},
annote = {REF sparsity of GNN for simplicity

1. l1-sparsity in the edge model in GNN
2. Out-of-the-box symbolic regression applied to the learned edge/node model},
archivePrefix = {arXiv},
arxivId = {2006.11287},
author = {Cranmer, Miles and Sanchez-Gonzalez, Alvaro and Battaglia, Peter and Xu, Rui and Cranmer, Kyle and Spergel, David and Ho, Shirley},
eprint = {2006.11287},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cranmer et al. - 2020 - Discovering symbolic models from deep learning with inductive biases.pdf:pdf},
journal = {arXiv},
number = {NeurIPS},
pages = {1--25},
title = {{Discovering symbolic models from deep learning with inductive biases}},
year = {2020}
}
@article{Johnson2016,
abstract = {We propose a general modeling and inference framework that combines the complementary strengths of probabilistic graphical models and deep learning methods. Our model family composes latent graphical models with neural network observation likelihoods. For inference, we use recognition networks to produce local evidence potentials, then combine them with the model distribution using efficient message-passing algorithms. All components are trained simultaneously with a single stochastic variational inference objective. We illustrate this framework by automatically segmenting and categorizing mouse behavior from raw depth video, and demonstrate several other example models.},
annote = {REF learning a causal model TOGETHER with a decoder, but not sparsity!

switching linear dynamics -- piecewise-linear, switch according to a Markov model},
archivePrefix = {arXiv},
arxivId = {1603.06277},
author = {Johnson, Matthew James and Duvenaud, David and Wiltschko, Alexander B. and Datta, Sandeep R. and Adams, Ryan P.},
eprint = {1603.06277},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson et al. - 2016 - Composing graphical models with neural networks for structured representations and fast inference.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {graphical models, variational autoencoders, variat},
pages = {2954--2962},
title = {{Composing graphical models with neural networks for structured representations and fast inference}},
year = {2016}
}
@article{Luneau2020,
abstract = {We consider generalized linear models in regimes where the number of nonzero components of the signal and accessible data points are sublinear with respect to the size of the signal. We prove a variational formula for the asymptotic mutual information per sample when the system size grows to infinity. This result allows us to heuristically derive an expression for the minimum mean-square error (MMSE) of the Bayesian estimator. We then find that, for discrete signals and suitable vanishing scalings of the sparsity and sampling rate, the MMSE displays an all-or-nothing phenomenon, namely, the MMSE sharply jumps from its maximum value to zero at a critical sampling rate. The all-or-nothing phenomenon has recently been proved to occur in high-dimensional linear regression. Our analysis goes beyond the linear case and applies to learning the weights of a perceptron with general activation function in a teacher-student scenario. In particular we discuss an all-or-nothing phenomenon for the generalization error with a sublinear set of training examples.},
annote = {REF sparsity charts over time, some theory},
archivePrefix = {arXiv},
arxivId = {2006.11313},
author = {Luneau, Cl{\'{e}}ment and Macris, Nicolas and Barbier, Jean},
eprint = {2006.11313},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luneau, Macris, Barbier - 2020 - Information theoretic limits of learning a sparse rule.pdf:pdf},
journal = {arXiv},
number = {NeurIPS},
title = {{Information theoretic limits of learning a sparse rule}},
year = {2020}
}
@article{Velickovic2020,
abstract = {Graph neural networks (GNNs) are typically applied to static graphs that are assumed to be known upfront. This static input structure is often informed purely by insight of the machine learning practitioner, and might not be optimal for the actual task the GNN is solving. In absence of reliable domain expertise, one might resort to inferring the latent graph structure, which is often difficult due to the vast search space of possible graphs. Here we introduce Pointer Graph Networks (PGNs) which augment sets or graphs with additional inferred edges for improved model expressivity. PGNs allow each node to dynamically point to another node, followed by message passing over these pointers. The sparsity of this adaptable graph structure makes learning tractable while still being sufficiently expressive to simulate complex algorithms. Critically, the pointing mechanism is directly supervised to model long-term sequences of operations on classical data structures, incorporating useful structural inductive biases from theoretical computer science. Qualitatively, we demonstrate that PGNs can learn parallelisable variants of pointer-based data structures, namely disjoint set unions and link/cut trees. PGNs generalise out-of-distribution to 5× larger test inputs on dynamic graph connectivity tasks, outperforming unrestricted GNNs and Deep Sets.},
annote = {ref GNN learnble graph [but fixed number of edges]

graph neural network, with additional learned nodes that allow for message-passing},
archivePrefix = {arXiv},
arxivId = {2006.06380},
author = {Veli{\v{c}}kovi{\'{c}}, Petar and Buesing, Lars and Overlan, Matthew C. and Pascanu, Razvan and Vinyals, Oriol and Blundell, Charles},
eprint = {2006.06380},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Veli{\v{c}}kovi{\'{c}} et al. - 2020 - Pointer Graph Networks.pdf:pdf},
journal = {arXiv},
title = {{Pointer Graph Networks}},
year = {2020}
}
@article{VanDenOord2018,
abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
annote = {REF contrastive learning},
archivePrefix = {arXiv},
arxivId = {1807.03748},
author = {{Van Den Oord}, Aaron and Li, Yazhe and Vinyals, Oriol},
eprint = {1807.03748},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Den Oord, Li, Vinyals - 2018 - Representation learning with contrastive predictive coding.pdf:pdf},
journal = {arXiv},
title = {{Representation learning with contrastive predictive coding}},
year = {2018}
}
@article{Hofstadter1994,
abstract = {describe Copycat, a nondeterministic model of analogy making aiming at psychological realism / the authors maintain that the hybrid middle ground in cognitive modeling occupied by Copycat is, at present, the most useful level at which to attempt to understand the fluidity of concepts and perception that is so clearly apparent in human analogy making / argue that nondeterminism is necessary for flexible cognition, and claim that their architecture, being essentially a model of fluid concepts and mental pressures, has validity beyond analogy making (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
annote = {maybe as reference to grounding/symbolic reasoning (gofa)},
author = {Hofstadter, Douglas and Mitchell, Melanie},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hofstadter, Mitchell - 1994 - The copycat project A model of mental fluidity and analogy-making.pdf:pdf},
journal = {Advances in connectionist and neural computation theory},
number = {31-112},
pages = {29--30},
title = {{The copycat project: A model of mental fluidity and analogy-making}},
volume = {2},
year = {1994}
}
@article{Abramson2020,
abstract = {A common vision from science fiction is that robots will one day inhabit our physical spaces, sense the world as we do, assist our physical labours, and communicate with us through natural language. Here we study how to design artificial agents that can interact naturally with humans using the simplification of a virtual environment. This setting nevertheless integrates a number of the central challenges of artificial intelligence (AI) research: complex visual perception and goal-directed physical control, grounded language comprehension and production, and multi-agent social interaction. To build agents that can robustly interact with humans, we would ideally train them while they interact with humans. However, this is presently impractical. Therefore, we approximate the role of the human with another learned agent, and use ideas from inverse reinforcement learning to reduce the disparities between human-human and agent-agent interactive behaviour. Rigorously evaluating our agents poses a great challenge, so we develop a variety of behavioural tests, including evaluation by humans who watch videos of agents or interact directly with them. These evaluations convincingly demonstrate that interactive training and auxiliary losses improve agent behaviour beyond what is achieved by supervised learning of actions alone. Further, we demonstrate that agent capabilities generalise beyond literal experiences in the dataset. Finally, we train evaluation models whose ratings of agents agree well with human judgement, thus permitting the evaluation of new agent models without additional effort. Taken together, our results in this virtual environment provide evidence that large-scale human behavioural imitation is a promising tool to create intelligent, interactive agents, and the challenge of reliably evaluating such agents is possible to surmount.},
annote = {REF interpretability of an agen with natural language},
archivePrefix = {arXiv},
arxivId = {2012.05672},
author = {Abramson, Josh and Ahuja, Arun and Brussee, Arthur and Carnevale, Federico and Cassin, Mary and Clark, Stephen and Dudzik, Andrew and Georgiev, Petko and Guy, Aurelia and Harley, Tim and Hill, Felix and Hung, Alden and Kenton, Zachary and Landon, Jessica and Lillicrap, Timothy and Mathewson, Kory and Muldal, Alistair and Santoro, Adam and Savinov, Nikolay and Varma, Vikrant and Wayne, Greg and Wong, Nathaniel and Yan, Chen and Zhu, Rui},
eprint = {2012.05672},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abramson et al. - 2020 - Imitating Interactive Intelligence.pdf:pdf},
pages = {1--96},
title = {{Imitating Interactive Intelligence}},
url = {http://arxiv.org/abs/2012.05672},
year = {2020}
}
@article{Cortese2020,
annote = {REF value reconstructor leads to different abstractions},
author = {Cortese, Aurelio and Yamamoto, Asuka and Hashemzadeh, Maryam and Sepulveda, Pradyumna and Kawato, Mitsuo and Martino, Benedetto De},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cortese et al. - 2020 - Value Shapes Abstraction During Learning.pdf:pdf},
journal = {PsyArXiv},
keywords = {abstraction,fmri,hippocampus,multivoxel neurofeedback,neural reinforcement,reinforcement learning,sensory features,vmpfc},
pages = {1--36},
title = {{Value Shapes Abstraction During Learning}},
year = {2020}
}
@article{Mohammadi2020,
abstract = {The effective application of representation learning to real-world problems requires both techniques for learning useful representations, and also robust ways to evaluate properties of representations. Recent work in disentangled representation learning has shown that unsupervised representation learning approaches rely on fully supervised disentanglement metrics, which assume access to labels for ground-truth factors of variation. In many real-world cases ground-truth factors are expensive to collect, or difficult to model, such as for perception. Here we empirically show that a weakly-supervised downstream task based on odd-one-out observations is suitable for model selection by observing high correlation on a difficult downstream abstract visual reasoning task. We also show that a bespoke metric-learning VAE model which performs highly on this task also out-performs other standard unsupervised and a weakly-supervised disentanglement model across several metrics.},
annote = {ref contrastive loss},
archivePrefix = {arXiv},
arxivId = {2012.07966},
author = {Mohammadi, Salman and Uhrenholt, Anders Kirk and Jensen, Bj{\o}rn Sand},
eprint = {2012.07966},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohammadi, Uhrenholt, Jensen - 2020 - Odd-One-Out Representation Learning.pdf:pdf},
title = {{Odd-One-Out Representation Learning}},
url = {http://arxiv.org/abs/2012.07966},
year = {2020}
}
@article{Zhou2018,
abstract = {—Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular fingerprints, predicting protein interface, and classifying diseases require a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive GNNs have been found difficult to train for a fixed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on variants of graph neural networks such as graph convolutional network (GCN), graph attention network (GAT), gated graph neural network (GGNN) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.},
annote = {REF review of GNNs},
archivePrefix = {arXiv},
arxivId = {1812.08434},
author = {Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
eprint = {1812.08434},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2018 - Graph Neural Networks A Review of Methods and Applications.pdf:pdf},
journal = {arXiv},
keywords = {Deep Learning,,Graph Neural Network},
pages = {1--22},
title = {{Graph Neural Networks: A Review of Methods and Applications}},
year = {2018}
}
@article{Schmidhuber2009,
abstract = {I argue that data becomes temporarily interesting by itself to some self-improving, but computationally limited, subjective observer once he learns to predict or compress the data in a better way, thus making it subjectively simpler and more beautiful. Curiosity is the desire to create or discover more non-random, non-arbitrary, regular data that is novel and surprising not in the traditional sense of Boltzmann and Shannon but in the sense that it allows for compression progress because its regularity was not yet known. This drive maximizes interestingness, the first derivative of subjective beauty or compressibility, that is, the steepness of the learning curve. It motivates exploring infants, pure mathematicians, composers, artists, dancers, comedians, yourself, and (since 1990) artificial systems. {\textcopyright} 2009 Springer Berlin Heidelberg.},
annote = {REF for good abstraction as a consequence of compression},
archivePrefix = {arXiv},
arxivId = {0812.4360},
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1007/978-3-642-02565-5_4},
eprint = {0812.4360},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidhuber - 2009 - Driven by compression progress A simple principle explains essential aspects of subjective beauty, novelty, surpris.pdf:pdf},
isbn = {3642025641},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {April 2009},
pages = {48--76},
title = {{Driven by compression progress: A simple principle explains essential aspects of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes}},
volume = {5499 LNAI},
year = {2009}
}
@article{Volodin2020,
abstract = {Causal models could increase interpretability, robustness to distributional shift and sample efficiency of RL agents. In this vein, we address the question of learning a causal model of an RL environment. This problem is known to be difficult due to spurious correlations. We overcome this difficulty by rewarding an RL agent for designing and executing interventions to discover the true model. We compare rewarding the agent for disproving uncertain edges in the causal graph, rewarding the agent for activating a certain node, or rewarding the agent for increasing the causal graph loss. We show that our methods result in a better causal graph than one generated by following the random policy, or a policy trained on the environment's reward. We find that rewarding for the causal graph loss works the best.},
annote = {REF keychest env},
archivePrefix = {arXiv},
arxivId = {2002.05217},
author = {Volodin, Sergei and Wichers, Nevan and Nixon, Jeremy},
eprint = {2002.05217},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Volodin, Wichers, Nixon - 2020 - Resolving spurious correlations in causal models of environments via interventions.pdf:pdf},
journal = {arXiv},
title = {{Resolving spurious correlations in causal models of environments via interventions}},
year = {2020}
}
@article{Gomez2006,
annote = {reference for "humans use causality"

reference for "fast adaptation"

reference for "reuse of parts"

proposed environment},
author = {G{\'{o}}mez, R and Bootzin, R and Nadel, L},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/G{\'{o}}mez, Bootzin, Nadel - 2006 - N Aps P Romote a Bstraction in L Anguage -L Earning I Nfants.pdf:pdf},
pages = {358},
title = {{HALMA: humanlike abstraction learning meets affordance in rapid problem solving}},
year = {2021}
}
@article{Schrittwieser2019,
abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
annote = {REF mbrl, value instead of reconstructor},
archivePrefix = {arXiv},
arxivId = {1911.08265},
author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
eprint = {1911.08265},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schrittwieser et al. - 2019 - Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.pdf:pdf},
journal = {arXiv},
month = {nov},
publisher = {arXiv},
title = {{Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model}},
url = {http://arxiv.org/abs/1911.08265},
year = {2019}
}
@techreport{Jang,
abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax esti-mator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
annote = {!REF Gumbel-softmax},
archivePrefix = {arXiv},
arxivId = {1611.01144v5},
author = {Jang, Eric and Brain, Google and Gu, Shixiang and Poole, Ben},
eprint = {1611.01144v5},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jang et al. - Unknown - CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX.pdf:pdf},
title = {{CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX}},
year = {2017}
}
@article{Fallah2020,
annote = {REF sparse feature vectors},
author = {Fallah, Kion and Willats, Adam A and Liu, Ninghao and Rozell, Christopher J},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fallah et al. - 2020 - Learning sparse codes from compressed representations with biologically plausible local wiring constraints.pdf:pdf},
number = {NeurIPS},
title = {{Learning sparse codes from compressed representations with biologically plausible local wiring constraints}},
year = {2020}
}
@article{Ying2020,
annote = {REF other sparsity regularizer for causal nets},
author = {Ying, Jiaxi and Palomar, Daniel P and Analytics, Data and Bay, Clear Water and Kong, Hong},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ying et al. - 2020 - Nonconvex Sparse Graph Learning under Laplacian Constrained Graphical Model.pdf:pdf},
number = {NeurIPS},
pages = {1--13},
title = {{Nonconvex Sparse Graph Learning under Laplacian Constrained Graphical Model}},
year = {2020}
}
@article{Barcelo2020,
abstract = {In spite of several claims stating that some models are more interpretable than others -- e.g., "linear models are more interpretable than deep neural networks" -- we still lack a principled notion of interpretability to formally compare among different classes of models. We make a step towards such a notion by studying whether folklore interpretability claims have a correlate in terms of computational complexity theory. We focus on local post-hoc explainability queries that, intuitively, attempt to answer why individual inputs are classified in a certain way by a given model. In a nutshell, we say that a class {\$}\backslashmathcal{\{}C{\}}{\_}1{\$} of models is more interpretable than another class {\$}\backslashmathcal{\{}C{\}}{\_}2{\$}, if the computational complexity of answering post-hoc queries for models in {\$}\backslashmathcal{\{}C{\}}{\_}2{\$} is higher than for those in {\$}\backslashmathcal{\{}C{\}}{\_}1{\$}. We prove that this notion provides a good theoretical counterpart to current beliefs on the interpretability of models; in particular, we show that under our definition and assuming standard complexity-theoretical assumptions (such as P{\$}\backslashneq{\$}NP), both linear and tree-based models are strictly more interpretable than neural networks. Our complexity analysis, however, does not provide a clear-cut difference between linear and tree-based models, as we obtain different results depending on the particular post-hoc explanations considered. Finally, by applying a finer complexity analysis based on parameterized complexity, we are able to prove a theoretical result suggesting that shallow neural networks are more interpretable than deeper ones.},
annote = {REF complexity of the model as a proxy for interpretability},
archivePrefix = {arXiv},
arxivId = {2010.12265},
author = {Barcel{\'{o}}, Pablo and Monet, Mika{\"{e}}l and P{\'{e}}rez, Jorge and Subercaseaux, Bernardo},
eprint = {2010.12265},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Barcel{\'{o}} et al. - 2020 - Model Interpretability through the Lens of Computational Complexity.pdf:pdf},
number = {NeurIPS},
pages = {1--12},
title = {{Model Interpretability through the Lens of Computational Complexity}},
url = {http://arxiv.org/abs/2010.12265},
year = {2020}
}
@article{Vincent2002,
abstract = {Matching Pursuit algorithms learn a function that is a weighted sum of basis functions, by sequentially appending functions to an initially empty basis, to approximate a target function in the least-squares sense. We show how matching pursuit can be extended to use non-squared error loss functions, and how it can be used to build kernel-based solutions to machine learning problems, while keeping control of the sparsity of the solution. We present a version of the algorithm that makes an optimal choice of both the next basis and the weights of all the previously chosen bases. Finally, links to boosting algorithms and RBF training procedures, as well as an extensive experimental comparison with SVMs for classification are given, showing comparable results with typically much sparser models.},
annote = {reference for matching pursuit/sparse dictionary learning},
author = {Vincent, Pascal and Bengio, Yoshua},
doi = {10.1023/A:1013955821559},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vincent, Bengio - 2002 - Kernel matching pursuit.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Boosting,Kernel methods,Matching pursuit,Radial basis functions,Sparse approximation,Support vector machines},
number = {1-3},
pages = {165--187},
title = {{Kernel matching pursuit}},
volume = {48},
year = {2002}
}
@article{Sulam2020,
abstract = {Parsimonious representations are ubiquitous in modeling and processing information. Motivated by the recent Multi-Layer Convolutional Sparse Coding (ML-CSC) model, we herein generalize the traditional Basis Pursuit problem to a multi-layer setting, introducing similar sparse enforcing penalties at different representation layers in a symbiotic relation between synthesis and analysis sparse priors. We explore different iterative methods to solve this new problem in practice, and we propose a new Multi-Layer Iterative Soft Thresholding Algorithm (ML-ISTA), as well as a fast version (ML-FISTA). We show that these nested first order algorithms converge, in the sense that the function value of near-fixed points can get arbitrarily close to the solution of the original problem. We further show how these algorithms effectively implement particular recurrent convolutional neural networks (CNNs) that generalize feed-forward ones without introducing any parameters. We present and analyze different architectures resulting from unfolding the iterations of the proposed pursuit algorithms, including a new Learned ML-ISTA, providing a principled way to construct deep recurrent CNNs. Unlike other similar constructions, these architectures unfold a global pursuit holistically for the entire network. We demonstrate the emerging constructions in a supervised learning setting, consistently improving the performance of classical CNNs while maintaining the number of parameters constant.},
annote = {REF neural basis pursuit},
archivePrefix = {arXiv},
arxivId = {1806.00701},
author = {Sulam, Jeremias and Aberdam, Aviad and Beck, Amir and Elad, Michael},
doi = {10.1109/TPAMI.2019.2904255},
eprint = {1806.00701},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sulam et al. - 2020 - On Multi-Layer Basis Pursuit, Efficient Algorithms and Convolutional Neural Networks.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Multi-layer convolutional sparse coding,iterative shrinkage algorithms,network unfolding,recurrent neural networks},
number = {8},
pages = {1968--1980},
pmid = {30869611},
title = {{On Multi-Layer Basis Pursuit, Efficient Algorithms and Convolutional Neural Networks}},
volume = {42},
year = {2020}
}
@article{Javed2020,
annote = {REF causal graphs, sparse feature vectors, no decoder},
author = {Javed, Khurram},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2020 - Learning Online-Aware Representations using Neural Networks.pdf:pdf},
title = {{Learning Online-Aware Representations using Neural Networks}},
year = {2020}
}
@article{Javed2019,
abstract = {A continual learning agent should be able to build on top of existing knowledge to learn on new data quickly while minimizing forgetting. Current intelligent systems based on neural network function approximators arguably do the opposite—they are highly prone to forgetting and rarely trained to facilitate future learning. One reason for this poor behavior is that they learn from a representation that is not explicitly trained for these two goals. In this paper, we propose MRCL, an objective to explicitly learn representations that accelerate future learning and are robust to forgetting under online updates in continual learning. The idea is to optimize the representation such that online updates minimize error on all samples with little forgetting. We show that it is possible to learn representations that are more effective for online updating and that sparsity naturally emerges in these representations. Moreover, our method is complementary to existing continual learning strategies, like MER, which can learn more effectively from representations learned by our objective. Finally, we demonstrate that a basic online updating strategy with our learned representation is competitive with rehearsal based methods for continual learning. We release an implementation of our method at https://github.com/khurramjaved96/mrcl.},
annote = {ref sparse feature vector},
archivePrefix = {arXiv},
arxivId = {1905.12588},
author = {Javed, Khurram and White, Martha},
eprint = {1905.12588},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Javed, White - 2019 - Meta-Learning Representations for Continual Learning.pdf:pdf},
journal = {arXiv},
number = {NeurIPS},
pages = {1--15},
title = {{Meta-Learning Representations for Continual Learning}},
year = {2019}
}
@article{Oisy2019,
annote = {REF sparse models},
archivePrefix = {arXiv},
arxivId = {arXiv:1905.01591v1},
author = {Louizos, Christos and Welling, Max and Kingma, Diederik},
eprint = {arXiv:1905.01591v1},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oisy - 2019 - L Earning G Raph N Eural N Etworks.pdf:pdf},
number = {1},
pages = {1--5},
title = {{Learning sparse neural networks through l0 regularization}},
year = {2019}
}
@article{Dinh2015,
abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
annote = {REF future work fixed complexity of a representation},
archivePrefix = {arXiv},
arxivId = {1410.8516},
author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
eprint = {1410.8516},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dinh, Krueger, Bengio - 2015 - NICE Non-linear independent components estimation.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Workshop Track Proceedings},
number = {2},
pages = {1--13},
title = {{NICE: Non-linear independent components estimation}},
volume = {1},
year = {2015}
}
@article{Ehsan2018,
abstract = {We introduce AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had performed the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of an autonomous agent into natural language. We evaluate our technique in the Frogger game environment, training an autonomous game playing agent to rationalize its action choices using natural language. A natural language training corpus is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation and show the results of two experiments evaluating the effectiveness of rationalization. Results of these evaluations show that neural machine translation is able to accurately generate rationalizations that describe agent behavior, and that rationalizations are more satisfying to humans than other alternative methods of explanation.},
annote = {REF nlp explanations for rl},
archivePrefix = {arXiv},
arxivId = {1702.07826},
author = {Ehsan, Upol and Harrison, Brent and Chan, Larry and Riedl, Mark O.},
doi = {10.1145/3278721.3278736},
eprint = {1702.07826},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ehsan et al. - 2018 - Rationalization A Neural Machine Translation Approach to Generating Natural Language Explanations.pdf:pdf},
isbn = {9781450360128},
journal = {AIES 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
keywords = {ai rationalization,artificial intelligence,explainable ai,interpretability,machine learning,transparency,user perception},
pages = {81--87},
title = {{Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations}},
year = {2018}
}
@techreport{Hyvarinen2019,
abstract = {Nonlinear ICA is a fundamental problem for unsupervised representation learning, emphasizing the capacity to recover the underlying latent variables generating the data (i.e., identifiability). Recently, the very first iden-tifiability proofs for nonlinear ICA have been proposed, leveraging the temporal structure of the independent components. Here, we propose a general framework for nonlinear ICA, which, as a special case, can make use of temporal structure. It is based on augmenting the data by an auxiliary variable, such as the time index, the history of the time series , or any other available information. We propose to learn nonlinear ICA by discriminating between true augmented data, or data in which the auxiliary variable has been ran-domized. This enables the framework to be implemented algorithmically through logistic regression, possibly in a neural network. We provide a comprehensive proof of the identifi-ability of the model as well as the consistency of our estimation method. The approach not only provides a general theoretical framework combining and generalizing previously proposed nonlinear ICA models and algorithms, but also brings practical advantages.},
annote = {ref nonlinear ica},
archivePrefix = {arXiv},
arxivId = {1805.08651v3},
author = {Hyv{\"{a}}rinen, Aapo and Sasaki, Hiroaki and Turner, Richard E},
eprint = {1805.08651v3},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hyv{\"{a}}rinen, Sasaki, Turner - 2019 - Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning.pdf:pdf},
title = {{Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning}},
year = {2019}
}
@article{Wong2020,
abstract = {We introduce computational causal inference as an interdisciplinary field across causal inference, algorithms design and numerical computing. The field aims to develop software specializing in causal inference that can analyze massive datasets with a variety of causal effects, in a performant, general, and robust way. The focus on software improves research agility, and enables causal inference to be easily integrated into large engineering systems. In particular, we use computational causal inference to deepen the relationship between causal inference, online experimentation, and algorithmic decision making. This paper describes the new field, the demand, opportunities for scalability, open challenges, and begins the discussion for how the community can unite to solve challenges for scaling causal inference and decision making.},
annote = {REF causality in general},
archivePrefix = {arXiv},
arxivId = {2007.10979},
author = {Wong, Jeffrey C.},
eprint = {2007.10979},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wong - 2020 - Computational Causal Inference.pdf:pdf},
title = {{Computational Causal Inference}},
url = {http://arxiv.org/abs/2007.10979},
year = {2020}
}
@article{Bengio2012,
abstract = {We propose a theory that relates difficulty of learning in deep architectures to culture and language. It is articulated around the following hypotheses: (1) learning in an individual human brain is hampered by the presence of effective local minima; (2) this optimization difficulty is particularly important when it comes to learning higher-level abstractions, i.e., concepts that cover a vast and highly-nonlinear span of sensory configurations; (3) such high-level abstractions are best represented in brains by the composition of many levels of representation, i.e., by deep architectures; (4) a human brain can learn such high-level abstractions if guided by the signals produced by other humans, which act as hints or indirect supervision for these high-level abstractions; and (5), language and the recombination and optimization of mental concepts provide an efficient evolutionary recombination operator, and this gives rise to rapid search in the space of communicable ideas that help humans build up better high-level internal representations of their world. These hypotheses put together imply that human culture and the evolution of ideas have been crucial to counter an optimization difficulty: this optimization difficulty would otherwise make it very difficult for human brains to capture high-level knowledge of the world. The theory is grounded in experimental observations of the difficulties of training deep artificial neural networks. Plausible consequences of this theory for the efficiency of cultural evolutions are sketched.},
annote = {REF reuse of stuff by humans

Culture as non-convexity escape mechanism

training signal: predict what the teacher does -{\textgreater} loss for representation

animal training term 'shaping' {\~{}} curriculum

dividing and recombination of both, max(eyes gene) max(... gene) max (... gene)},
archivePrefix = {arXiv},
arxivId = {1203.2990},
author = {Bengio, Yoshua},
eprint = {1203.2990},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio - 2012 - Evolving Culture vs Local Minima.pdf:pdf},
pages = {1--28},
title = {{Evolving Culture vs Local Minima}},
url = {http://arxiv.org/abs/1203.2990},
volume = {2006},
year = {2012}
}
@article{Sitzmann2020,
abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.},
annote = {REF sine act fcn

sine activation function, map (x, y) -{\textgreater} pixel value},
archivePrefix = {arXiv},
arxivId = {2006.09661},
author = {Sitzmann, Vincent and Martel, Julien N. P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
eprint = {2006.09661},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sitzmann et al. - 2020 - Implicit Neural Representations with Periodic Activation Functions.pdf:pdf},
month = {jun},
title = {{Implicit Neural Representations with Periodic Activation Functions}},
url = {http://arxiv.org/abs/2006.09661},
year = {2020}
}
@techreport{Kalainathan,
abstract = {Ecole doctorale n • 580 Sciences et technologies de l'information et de la communication (STIC) Sp{\'{e}}cialit{\'{e}}Sp´Sp{\'{e}}cialitSp{\'{e}}cialit´Sp{\'{e}}cialit{\'{e}} de doctorat : Informatique Th{\`{e}}seTh`Th{\`{e}}se pr{\'{e}}sent{\'{e}}epr´pr{\'{e}}sentpr{\'{e}}sent´pr{\'{e}}sent{\'{e}}e et soutenu{\`{e}} a Gif-Sur-Yvette, le 17 D ´ ecembre 2019, par DIVIYAN KALAINATHAN},
annote = {REF structural gates for causal models

NEWMETHOD
number of active neurons in the f network as a measure of complexity},
author = {Kalainathan, Diviyan},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalainathan - Unknown - Generative Neural networks to infer Causal Mechanisms Algorithms and applications.pdf:pdf},
title = {{Generative Neural networks to infer Causal Mechanisms: Algorithms and applications}},
url = {https://hal.inria.fr/tel-02435986},
year = {2020}
}
@techreport{Kamalaruban,
abstract = {One of the central challenges faced by a reinforcement learning (RL) agent is to effectively learn a (near-)optimal policy in environments with large state spaces having sparse and noisy feedback signals. In real-world applications, an expert with additional domain knowledge can help in speeding up the learning process via shaping the environment, i.e., making the environment more learner-friendly. A popular paradigm in literature is potential-based reward shaping, where the environment's reward function is augmented with additional local rewards using a potential function. However, the applicability of potential-based reward shaping is limited in settings where (i) the state space is very large, and it is challenging to compute an appropriate potential function, (ii) the feedback signals are noisy, and even with shaped rewards the agent could be trapped in local optima, and (iii) changing the rewards alone is not sufficient, and effective shaping requires changing the dynamics. We address these limitations of potential-based shaping methods and propose a novel framework of environment shaping using state abstraction. Our key idea is to compress the environment's large state space with noisy signals to an abstracted space, and to use this abstraction in creating smoother and more effective feedback signals for the agent. We study the theoretical underpinnings of our abstraction-based environment shaping, and show that the agent's policy learnt in the shaped environment preserves near-optimal behavior in the original environment.},
annote = {REF state abstraction using a decoder},
archivePrefix = {arXiv},
arxivId = {2006.13160v1},
author = {Kamalaruban, Parameswaran},
eprint = {2006.13160v1},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kamalaruban - Unknown - Environment Shaping in Reinforcement Learning using State Abstraction.pdf:pdf},
keywords = {EPFL Adish Singla MPI-SWS,EPFL Rati Devidze MPI-SWS Volkan Cevher LIONS,LIONS},
title = {{Environment Shaping in Reinforcement Learning using State Abstraction}},
year = {2020}
}
@article{Guo2020,
abstract = {This work considers the question of how convenient access to copious data impacts our ability to learn causal effects and relations. In what ways is learning causality in the era of big data different from-or the same as-the traditional one? To answer this question, this survey provides a comprehensive and structured review of both traditional and frontier methods in learning causality and relations along with the connections between causality and machine learning. This work points out on a case-by-case basis how big data facilitates, complicates, or motivates each approach.},
annote = {REF causality overview},
archivePrefix = {arXiv},
arxivId = {1809.09337v4},
author = {Guo, Ruocheng and Cheng, L U and Hahn, P Richard and Liu, Huan and Cheng, Lu and Li, Jundong},
doi = {10.1145/3397269},
eprint = {1809.09337v4},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2020 - A Survey of Learning Causality with Data Problems and Methods.pdf:pdf},
keywords = {ACM Reference Format:,CCS Concepts: • Computing methodologies → Artifici,Probability and statistics,• Mathematics of computing → Probability and stati},
title = {{A Survey of Learning Causality with Data: Problems and Methods}},
url = {https://doi.org/10.1145/3397269},
year = {2020}
}
@article{Evans2018,
abstract = {Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regu-larisation methods, the common cure is to seek large amounts of training data'which is not necessarily easily obtained'that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework (?ILP), which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.},
annote = {REF logical causal models},
archivePrefix = {arXiv},
arxivId = {1711.04574},
author = {Evans, Richard and Grefenstette, Edward},
doi = {10.1613/jair.5714},
eprint = {1711.04574},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Evans, Grefenstette - 2018 - Learning explanatory rules from noisy data.pdf:pdf},
isbn = {9780999241127},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {5598--5602},
title = {{Learning explanatory rules from noisy data}},
volume = {2018-July},
year = {2018}
}
@techreport{Weinstein,
annote = {REF sparse feature vector

Can efficiently compute the whole LASSO path. Select hyper?

'where signals are sparse' = denoising

Basis pursuit

LARS-TD, LSTD algorithms -- SPARSE RL!

Making FEATURE VECTOR sparse (which features we use), NOT the model.},
author = {Weinstein, Alejandro J},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weinstein - Unknown - INFERENCE AND LEARNING IN HIGH-DIMENSIONAL SPACES.pdf:pdf},
title = {{INFERENCE AND LEARNING IN HIGH-DIMENSIONAL SPACES}},
year = {2013}
}
@techreport{Johnson,
annote = {REF psychedelics and sparsity (annealing)},
author = {Johnson, Michael Edward},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson - Unknown - Neural Annealing Toward a Neural Theory of Everything.pdf:pdf},
title = {{Neural Annealing: Toward a Neural Theory of Everything}},
year = {2020}
}
@techreport{Deng,
abstract = {There has been a gap between artificial intelligence and human intelligence. In this paper, we identify three key elements forming human intelligence, and suggest that abstraction learning combines these elements and is thus a way to bridge the gap. Prior researches in artificial intelligence either specify abstraction by human experts, or take abstraction as qualitative explanation for the model. This paper aims to learn abstraction directly. We tackle three main challenges: representation, objective function, and learning algorithm. Specifically, we propose a partition structure that contains pre-allocated abstraction neurons; we formulate abstraction learning as a constrained optimization problem, which integrates abstraction properties; we develop a network evolution algorithm to solve this problem. This complete framework is named ONE (Optimization via Network Evolution). In our experiments on MNIST, ONE shows elementary human-like intelligence, including low energy consumption, knowledge sharing, and lifelong learning.},
annote = {REF evolution strategy for edge selection
evolution to search for discrete edges
feature sparsity},
archivePrefix = {arXiv},
arxivId = {1809.03956v1},
author = {Deng, Fei and Ren, Jinsheng and Chen, Feng},
eprint = {1809.03956v1},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng, Ren, Chen - Unknown - Abstraction Learning.pdf:pdf},
title = {{Abstraction Learning}},
year = {2018}
}
@techreport{DasGupta,
abstract = {In this paper, we investigate the properties of L p norm (p ≤ 1) within a projection framework. We start with the KKT equations of the non-linear optimization problem and then use its key properties to arrive at an algorithm for L p norm projection on the non-negative simplex. We compare with L 1 projection which needs prior knowledge of the true norm, as well as hard thresholding based sparsification proposed in recent compressed sensing literature. We show performance improvements compared to these techniques across different vision applications.},
annote = {REF [add explicit projection] for l{\_}1 projection code

test for unused code and add to appendix as 'tried'},
author = {{Das Gupta}, Mithun},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Das Gupta - Unknown - Non-Convex P-norm Projection for Robust Sparsity.pdf:pdf},
title = {{Non-Convex P-norm Projection for Robust Sparsity *}},
year = {2013}
}
@techreport{Mairal2009,
abstract = {Sparse coding-that is, modelling data vectors as sparse linear combinations of basis elements-is widely used in machine learning, neuroscience, signal processing, and statistics. This paper fo-cuses on learning the basis set, also called dictionary , to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the audio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations , which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and large datasets.},
annote = {ref for dictionary learning},
author = {Mairal, Julien and {Bach FRANCISBACH}, Francis and Ponce, Jean and {Normale Sup{\'{e}}rieure}, Ecole and Sapiro, Guillermo},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mairal et al. - 2009 - Online Dictionary Learning for Sparse Coding.pdf:pdf},
title = {{Online Dictionary Learning for Sparse Coding}},
year = {2009}
}
@article{Babadi2014,
abstract = {In several sensory pathways, input stimuli project tosparsely active downstream populations that have more neurons than incoming axons. Here, we address the computational benefits of expansion and sparseness for clustered inputs, where different clusters represent behaviorally distinct stimuli and intracluster variability represents sensory or neuronal noise. Through analytical calculations and numerical simulations, we show that expansion implemented by feed-forward random synaptic weights amplifies variability in the incoming stimuli, and this noise enhancement increases with sparseness of the expanded representation. In addition, the low dimensionality of the input layer generates overlaps between the induced representations of different stimuli, limiting the benefit of expansion. Highly sparse expansive representations obtained through synapses that encode the clustered structure of the input reduce both intrastimulus variability and the excess overlaps between stimuli, enhancing the ability of downstream neurons to perform classification and recognition tasks. Implications for olfactory, cerebellar, and visual processing are discussed.},
annote = {REF Sparsity in the brain},
author = {Babadi, Baktash and Sompolinsky, Haim},
doi = {10.1016/j.neuron.2014.07.035},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Babadi, Sompolinsky - 2014 - Sparseness and Expansion in Sensory Representations.pdf:pdf},
issn = {10974199},
journal = {Neuron},
month = {sep},
number = {5},
pages = {1213--1226},
publisher = {Cell Press},
title = {{Sparseness and Expansion in Sensory Representations}},
volume = {83},
year = {2014}
}
@article{Runge2019,
abstract = {The heart of the scientific enterprise is a rational effort to understand the causes behind the phenomena we observe. In large-scale complex dynamical systems such as the Earth system, real experiments are rarely feasible. However, a rapidly increasing amount of observational and simulated data opens up the use of novel data-driven causal methods beyond the commonly adopted correlation techniques. Here, we give an overview of causal inference frameworks and identify promising generic application cases common in Earth system sciences and beyond. We discuss challenges and initiate the benchmark platform causeme.net to close the gap between method users and developers.},
annote = {REF types of causal inference

Types of causal inference. PC for time series!},
author = {Runge, Jakob and Bathiany, Sebastian and Bollt, Erik and Camps-Valls, Gustau and Coumou, Dim and Deyle, Ethan and Glymour, Clark and Kretschmer, Marlene and Mahecha, Miguel D. and Mu{\~{n}}oz-Mar{\'{i}}, Jordi and van Nes, Egbert H. and Peters, Jonas and Quax, Rick and Reichstein, Markus and Scheffer, Marten and Sch{\"{o}}lkopf, Bernhard and Spirtes, Peter and Sugihara, George and Sun, Jie and Zhang, Kun and Zscheischler, Jakob},
doi = {10.1038/s41467-019-10105-3},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Runge et al. - 2019 - Inferring causation from time series in Earth system sciences.pdf:pdf},
isbn = {4146701910},
issn = {20411723},
journal = {Nature Communications},
number = {1},
pages = {1--13},
publisher = {Springer US},
title = {{Inferring causation from time series in Earth system sciences}},
url = {http://dx.doi.org/10.1038/s41467-019-10105-3},
volume = {10},
year = {2019}
}
@article{Bengio2017,
abstract = {A new prior is proposed for representation learning, which can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by the phenomenon of consciousness seen as the formation of a low-dimensional combination of a few concepts constituting a conscious thought, i.e., consciousness as awareness at a particular time instant. This provides a powerful constraint on the representation in that such low-dimensional thought vectors can correspond to statements about reality which are true, highly probable, or very useful for taking decisions. The fact that a few elements of the current state can be combined into such a predictive or useful statement is a strong constraint and deviates considerably from the maximum likelihood approaches to modelling data and how states unfold in the future based on an agent's actions. Instead of making predictions in the sensory (e.g. pixel) space, the consciousness prior allows the agent to make predictions in the abstract space, with only a few dimensions of that space being involved in each of these predictions. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in the form of facts and rules, although the conscious states may be richer than what can be expressed easily in the form of a sentence, a fact or a rule.},
annote = {REF sparse causal model for interpretability and consciouness},
archivePrefix = {arXiv},
arxivId = {1709.08568},
author = {Bengio, Yoshua},
eprint = {1709.08568},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio - 2017 - The Consciousness Prior(2).pdf:pdf},
pages = {1--7},
title = {{The Consciousness Prior}},
url = {http://arxiv.org/abs/1709.08568},
year = {2017}
}
@article{Hutter2003,
abstract = {Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff's the- ory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameter-free the- ory of universal Artificial Intelligence. We give strong arguments that the resulting AIXI model is the most intelligent unbiased agent possible. We outline for a number of problem classes, including sequence prediction, strategic games, function mini- mization, reinforcement and supervised learning, how the AIXI model can formally solve them. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXItl, which is still effectively more intelligent than any other time t and space l bounded agent. The computation time of AIXItl is of the order t{\textperiodcentered}2l. Other discussed topics are formal definitions of intelligence order relations, the horizon problem and relations of the AIXI theory to other AI approaches.},
annote = {reference for AIXI},
author = {Hutter, Marcus},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hutter - 2003 - A gentle introduction to the universal algorithmic agent AIXI.pdf:pdf},
journal = {Artificial General Intelligence},
keywords = {agents,algorithmic probability,artificial intelligence,function minimiza-,kolmogorov complexity,ment learning,rational,reinforce-,sequential decision theory,solomonoff induction,strategic games,universal sequence prediction,value function},
title = {{A gentle introduction to the universal algorithmic agent AIXI}},
year = {2003}
}
@article{Doerig2019,
abstract = {How can we explain consciousness? This question has become a vibrant topic of neuroscience research in recent decades. A large body of empirical results has been accumulated, and many theories have been proposed. Certain theories suggest that consciousness should be explained in terms of brain functions, such as accessing information in a global workspace, applying higher order to lower order representations, or predictive coding. These functions could be realized by a variety of patterns of brain connectivity. Other theories, such as Information Integration Theory (IIT)and Recurrent Processing Theory (RPT), identify causal structure with consciousness. For example, according to these theories, feedforward systems are never conscious, and feedback systems always are. Here, using theorems from the theory of computation, we show that causal structure theories are either false or outside the realm of science.},
annote = {REF iit unfolding},
author = {Doerig, Adrien and Schurger, Aaron and Hess, Kathryn and Herzog, Michael H.},
doi = {10.1016/j.concog.2019.04.002},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doerig et al. - 2019 - The unfolding argument Why IIT and other causal structure theories cannot explain consciousness(2).pdf:pdf},
issn = {10902376},
journal = {Consciousness and Cognition},
keywords = {Causal structure,Consciousness,IIT,Neural networks,RPT,Theories},
number = {April},
pages = {49--59},
pmid = {31078047},
publisher = {Elsevier},
title = {{The unfolding argument: Why IIT and other causal structure theories cannot explain consciousness}},
url = {https://doi.org/10.1016/j.concog.2019.04.002},
volume = {72},
year = {2019}
}
@article{Rajeswaran2020,
abstract = {Model-based reinforcement learning (MBRL) has recently gained immense interest due to its potential for sample efficiency and ability to incorporate off-policy data. However, designing stable and efficient MBRL algorithms using rich function approximators have remained challenging. To help expose the practical challenges in MBRL and simplify algorithm design from the lens of abstraction, we develop a new framework that casts MBRL as a game between: (1) a policy player, which attempts to maximize rewards under the learned model; (2) a model player, which attempts to fit the real-world data collected by the policy player. For algorithm development, we construct a Stackelberg game between the two players, and show that it can be solved with approximate bi-level optimization. This gives rise to two natural families of algorithms for MBRL based on which player is chosen as the leader in the Stackelberg game. Together, they encapsulate, unify, and generalize many previous MBRL algorithms. Furthermore, our framework is consistent with and provides a clear basis for heuristics known to be important in practice from prior works. Finally, through experiments we validate that our proposed algorithms are highly sample efficient, match the asymptotic performance of model-free policy gradient, and scale gracefully to high-dimensional tasks like dexterous hand manipulation.},
annote = {REF decoder-model game Stackelberg game (decoder depends on the model). solution better than the Gradient descent-ascent

leader=model
follower=decoder

leader must learn slower than the follower

first-order approximation instead of implicit function

TRY different number of steps for MODEL/DECODER optimizers

REF future better model-based optimization (theoretically grounded)},
archivePrefix = {arXiv},
arxivId = {2004.07804},
author = {Rajeswaran, Aravind and Mordatch, Igor and Kumar, Vikash},
eprint = {2004.07804},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rajeswaran, Mordatch, Kumar - 2020 - A game theoretic framework for model based reinforcement learning.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{A game theoretic framework for model based reinforcement learning}},
year = {2020}
}
@article{Martic2020,
annote = {ref 'safe ai'-{\textgreater} causal RL explanations},
archivePrefix = {arXiv},
arxivId = {arXiv:2103.03938v1},
author = {Martic, Grau-moya Miljan and Genewein, Tim and Mcgrath, Tom},
eprint = {arXiv:2103.03938v1},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Martic, Genewein, Mcgrath - 2020 - Causal Analysis of Agent Behavior for AI Safety.pdf:pdf},
keywords = {agent,agent analysis,ai safety,allow for investigating and,black-box analysis,causal reasoning,nisms that underlie an,s behavior,such methodologies,uncovering the causal mecha-},
title = {{Causal Analysis of Agent Behavior for AI Safety}},
year = {2020}
}
@article{Nair2019,
abstract = {Causal reasoning has been an indispensable capability for humans and other intelligent animals to interact with the physical world. In this work, we propose to endow an artificial agent with the capability of causal reasoning for completing goal-directed tasks. We develop learning-based approaches to inducing causal knowledge in the form of directed acyclic graphs, which can be used to contextualize a learned goal-conditional policy to perform tasks in novel environments with latent causal structures. We leverage attention mechanisms in our causal induction model and goal-conditional policy, enabling us to incrementally generate the causal graph from the agent's visual observations and to selectively use the induced graph for determining actions. Our experiments show that our method effectively generalizes towards completing new tasks in novel environments with previously unseen causal structures.},
annote = {REF another paradigm RL gives causal graph updates

Neural graph encoding + images, attentoin? Multiple envs. Too simple. It seems, the environment must be special? Can we transform any env to this form?

Edge decoder instead of explicit edge weight matrices

Qs:
1. why not explicit edge matrix?
variables are known, input=pixel
correspondence pixels-nodes is learned
attention to focus on nodes -- sparsity (only to one noce!)
all trained end-to-end




2. which environments it works on?
long term: all
should work in atari


3. why non-standard benchmarks?
goal-conditioned behavior, clear relationships visual--goals

4. how complex are the graph
single update -- 1 time-step

learning on the training graph -- loss=|learned-true|, supervised

Causal Reasoning from Meta-reinforcement Learning
learning the policy instead
best intervention for learning

code being released


INTERESTED in our project:
1. use the graph to do downstream tasks -- not only for the graph!
they were using downstream performance (user study for interpretability) OR final performance (task should favour true learning and true graph) distributional shift

SHORT number of episodes to learn -{\textgreater} need to do things right.

Comparison baseline: accuracy on a limited number of episodes.
Learned agent is better only on a limited number of steps.},
archivePrefix = {arXiv},
arxivId = {1910.01751},
author = {Nair, Suraj and Zhu, Yuke and Savarese, Silvio and Fei-Fei, Li},
eprint = {1910.01751},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nair et al. - 2019 - Causal Induction from Visual Observations for Goal Directed Tasks.pdf:pdf},
pages = {1--13},
title = {{Causal Induction from Visual Observations for Goal Directed Tasks}},
url = {http://arxiv.org/abs/1910.01751},
year = {2019}
}
@article{Chalupka2015,
abstract = {We provide a rigorous definition of the visual cause of a behavior that is broadly applicable to the visually driven behavior in humans, animals, neurons, robots and other perceiving systems. Our framework generalizes standard accounts of causal learning to settings in which the causal variables need to be constructed from micro-variables. We prove the Causal Coarsening Theorem, which allows us to gain causal knowledge from observational data with minimal experimental effort. The theorem provides a connection to standard inference techniques in machine learning that identify features of an image that correlate with, but may not cause, the target behavior. Finally, we propose an active learning scheme to learn a manipulator function that performs optimal manipulations on the image to automatically identify the visual cause of a target behavior. We illustrate our inference and learning algorithms in experiments based on both synthetic and real data.},
annote = {REF sparsity and parts of the image

Trying to augment the image to see if it's the real cause

Microvariables and macrovariables.

causal coarsening -- equivalence classes in microvariables based on the same value of macrovariables


Independence tests, no learned decoder},
archivePrefix = {arXiv},
arxivId = {1412.2309},
author = {Chalupka, Krzysztof and Perona, Pietro and Eberhardt, Frederick},
eprint = {1412.2309},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chalupka, Perona, Eberhardt - 2015 - Visual causal feature learning.pdf:pdf},
journal = {Uncertainty in Artificial Intelligence - Proceedings of the 31st Conference, UAI 2015},
pages = {181--190},
title = {{Visual causal feature learning}},
year = {2015}
}
@techreport{Javed,
abstract = {Predictive models-learned from observational data not covering the complete data distribution-can rely on spurious correlations in the data for making predictions. These correlations make the models brittle and hinder generalization. One solution for achieving strong generalization is to incorporate causal structures in the models; such structures constrain learning by ignoring correlations that contradict them. However, learning these structures is a hard problem in itself. Moreover, it's not clear how to incorporate the machinery of causality with online continual learning. In this work, we take an indirect approach to discovering causal models. Instead of searching for the true causal model directly, we propose an online algorithm that continually detects and removes spurious features. Our algorithm works on the idea that the correlation of a spurious feature with a target is not constant overtime. As a result, the weight associated with that feature is constantly changing. We show that by continually removing such features, our method converges to solutions that have strong generalization. Moreover, our method combined with random search can also discover non-spurious features from raw sensory data. Finally, our work highlights that the information present in the temporal structure of the problem-destroyed by shuffling the data-is essential for detecting spurious features online.},
annote = {REF causal graph for RL

correlation of a spurious feature with target over time is not constant -- assumption

!! Read again [David]},
archivePrefix = {arXiv},
arxivId = {2006.07461v1},
author = {Javed, Khurram and White, Martha and {Bengio MILA}, Yoshua},
eprint = {2006.07461v1},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Javed, White, Bengio MILA - Unknown - Learning Causal Models Online.pdf:pdf},
title = {{Learning Causal Models Online}},
year = {2020}
}
@article{Battaglia2016,
abstract = {Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.},
annote = {REF GNN without a decoder/graph learning

fitting dynamics (time series)

graph not learned},
archivePrefix = {arXiv},
arxivId = {1612.00222},
author = {Battaglia, Peter and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo and Kavukcuoglu, Koray},
eprint = {1612.00222},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Battaglia et al. - 2016 - Interaction networks for learning about objects, relations and physics.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {4509--4517},
title = {{Interaction networks for learning about objects, relations and physics}},
year = {2016}
}
@article{Yang2020,
abstract = {Learning disentanglement aims at finding a low dimensional representation which consists of multiple explanatory and generative factors of the observational data. The framework of variational autoencoder (VAE) is commonly used to disentangle independent factors from observations. However, in real scenarios, factors with semantics are not necessarily independent. Instead, there might be an underlying causal structure which renders these factors dependent. We thus propose a new VAE based framework named CausalVAE, which includes a Causal Layer to transform independent exogenous factors into causal endogenous ones that correspond to causally related concepts in data. We further analyze the model identifiabitily, showing that the proposed model learned from observations recovers the true one up to a certain degree. Experiments are conducted on various datasets, including synthetic and real word benchmark CelebA. Results show that the causal representations learned by CausalVAE are semantically interpretable, and their causal relationship as a Directed Acyclic Graph (DAG) is identified with good accuracy. Furthermore, we demonstrate that the proposed CausalVAE model is able to generate counterfactual data through "do-operation" to the causal factors.},
annote = {REF: mask to learn SCMs},
archivePrefix = {arXiv},
arxivId = {2004.08697},
author = {Yang, Mengyue and Liu, Furui and Chen, Zhitang and Shen, Xinwei and Hao, Jianye and Wang, Jun},
eprint = {2004.08697},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2020 - CausalVAE Disentangled Representation Learning via Neural Structural Causal Models.pdf:pdf},
pages = {1--22},
title = {{CausalVAE: Disentangled Representation Learning via Neural Structural Causal Models}},
url = {http://arxiv.org/abs/2004.08697},
year = {2020}
}
@article{Mao2019,
abstract = {We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.},
annote = {REF reusble networks

concepts=vectors
neural operators f(v)=v1 (example: shape: obj -{\textgreater} shapes)
RL to train the symbolic part (finding closest from embeddings)},
archivePrefix = {arXiv},
arxivId = {1904.12584},
author = {Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum, Joshua B. and Wu, Jiajun},
eprint = {1904.12584},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2019 - The neuro-symbolic concept learner Interpreting scenes, words, and sentences from natural supervision.pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR 2019},
pages = {1--28},
title = {{The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision}},
year = {2019}
}
@techreport{Lipton,
abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but inter-pretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for in-terpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim inter-pretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretabil-ity, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
annote = {REF interpretabiliy in generl},
archivePrefix = {arXiv},
arxivId = {1606.03490v3},
author = {Lipton, Zachary C},
eprint = {1606.03490v3},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lipton - Unknown - The Mythos of Model Interpretability.pdf:pdf},
title = {{The Mythos of Model Interpretability}},
year = {2017}
}
@article{Maddison2017,
abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce CONCRETE random variables-CONtinuous relaxations of disCRETE random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
annote = {REF gradient for discrete},
archivePrefix = {arXiv},
arxivId = {1611.00712},
author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
eprint = {1611.00712},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maddison, Mnih, Teh - 2017 - The concrete distribution A continuous relaxation of discrete random variables.pdf:pdf},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
pages = {1--20},
title = {{The concrete distribution: A continuous relaxation of discrete random variables}},
year = {2017}
}
@article{Wang2020,
abstract = {A fundamental problem in the high-dimensional regression is to understand the tradeoff between type I and type II errors or, equivalently, false discovery rate (FDR) and power in variable selection. To address this important problem, we offer the first complete tradeoff diagram that distinguishes all pairs of FDR and power that can be asymptotically realized by the Lasso with some choice of its penalty parameter from the remaining pairs, in a regime of linear sparsity under random designs. The tradeoff between the FDR and power characterized by our diagram holds no matter how strong the signals are. In particular, our results improve on the earlier Lasso tradeoff diagram of [19] by recognizing two simple but fundamental constraints on the pairs of FDR and power. The improvement is more substantial when the regression problem is above the Donoho–Tanner phase transition. Finally, we present extensive simulation studies to confirm the sharpness of the complete Lasso tradeoff diagram.},
annote = {REF sparsity l1 theroy},
archivePrefix = {arXiv},
arxivId = {2007.11078},
author = {Wang, Hua and Yang, Yachong and Bu, Zhiqi and Su, Weijie J.},
eprint = {2007.11078},
file = {:home/ubuntu-mate/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2020 - The Complete Lasso Tradeoff Diagram.pdf:pdf},
journal = {arXiv},
number = {NeurIPS},
title = {{The Complete Lasso Tradeoff Diagram}},
year = {2020}
}


@article{genewein2020algorithms,
    title={Algorithms for Causal Reasoning in Probability Trees},
    author={Genewein, Tim and McGrath, Tom and D{\'e}letang, Gr{\'e}goire and Mikulik, Vladimir and Martic, Miljan and Legg, Shane and Ortega, Pedro A},
    journal={arXiv preprint arXiv:2010.12237},
    year={2020}
}

@article{zambaldi2018relational,
    title={Relational deep reinforcement learning},
    author={Zambaldi, Vinicius and Raposo, David and Santoro, Adam and Bapst, Victor and Li, Yujia and Babuschkin, Igor and Tuyls, Karl and Reichert, David and Lillicrap, Timothy and Lockhart, Edward and others},
    journal={arXiv preprint arXiv:1806.01830},
    year={2018}
}

@article{hahne2019attention,
    title={Attention on abstract visual reasoning},
    author={Hahne, Lukas and L{\"u}ddecke, Timo and W{\"o}rg{\"o}tter, Florentin and Kappel, David},
    journal={arXiv preprint arXiv:1911.05990},
    year={2019}
}

@article{brouillard2020differentiable,
    title={Differentiable Causal Discovery from Interventional Data},
    author={Brouillard, Philippe and Lachapelle, S{\'e}bastien and Lacoste, Alexandre and Lacoste-Julien, Simon and Drouin, Alexandre},
    journal={arXiv preprint arXiv:2007.01754},
    year={2020}
}

@article{bellec2018long,
    title={Long short-term memory and learning-to-learn in networks of spiking neurons},
    author={Bellec, Guillaume and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
    journal={arXiv preprint arXiv:1803.09574},
    year={2018}
}