%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EPFL report package, main thesis file
% Goal: provide formatting for theses and project reports
% Author: Mathias Payer <mathias.payer@epfl.ch>
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,11pt,oneside]{report}
% Options: MScThesis, BScThesis, MScProject, BScProject
\usepackage[MScThesis,lablogo]{EPFLreport}
\usepackage{amsmath,amssymb,amsthm} % math typesetting
\usepackage{dsfont} % for indicator function
\usepackage[utf8]{inputenc}
\usepackage{natbib} % how to make it work???
\usepackage{xspace} % possibly add space after something
\usepackage{hyperref} % autoref and WEB URLs
\usepackage{graphicx} % figures
\usepackage{todonotes} % notes used when writing the draft
\usepackage[autostyle=try, english=american]{csquotes}


%ALternative explantion: best reparametrization of differential equations that makes the equation most simple
%DiffEquation environment+, gw env+
% IIT similar -- dynamical system, interventions on the graph
% try nonlinear ICA for ve5?
%Nonlin ICA would work for vector increment,
% finish mendeley overview, try approaches (lagrange multi+, proj <= n and dn/dt, time-scale Stackelberg_competition+, proximal sparsity)
%But it is likely to fail for sparsemateix with a linear encoder
%Add vectorinctement where the component is defined by a separate digit that is incremented and decremented using a special counter
%Implement a simpler grifworod with 2 coordinates. Try the method. If it doesn't work, why? If it does, what's the difference with keychest?
%Maybe highdim keychest would actually work better because of better disentanglement?
% complex matrix -> diagnonalizable?
% batchnorm prevents loss->0
% very slow even on linear -- trying with larger time-scale sep

% ISSUE: probas don't go down/up as they should have been -- why??
% maybe some bug in the coefficients / losses
% gw(5x5, 2feat) solved!
% sm5(noact/act) solved!
% sm5_linear(act) ...
% ve5_obs ...
% 5x5_kc_onlycoord...
% 5x5_kc_full ...

% lagrange|gap|discrete|sparsity|smooth|constraint

% ReLU is generally pretty bad and noisy

% do nonlinear ICA after handing in the report

\newtheorem{example}{Example}[section]
\newtheorem{proposition}{Proposition}[section]


\newcommand{\sysname}{CauseOccam\xspace}
\newcommand{\var}{\mbox{Var}}
% CausalOccam
% SimpleCause
% BayesianCause
% TLDRCause

\title{\sysname: Learning Interpretable Abstract Representations in Reinforcement Learning Environments via Model Sparsity}
\author{Sergei Volodin}
\supervisor{Dr. Johanni Brea}
\adviser{Prof. Wulfram Gerstner}
%\coadviser{Second Adviser}
\expert{Carl-Johann Simon-Gabriel}

\dedication{\begin{raggedleft}
        The strongest evidence that we can obtain for the validity of a proposed induction method, is that it yields results that are in accord with intuitive evaluations in many different kinds of situations in which we have strong intuitive ideas.\\
        --- Ray J. Solomonoff\\
    \end{raggedleft}
}

\acknowledgments{
I thank my parents and my sister for the support in my studies, and Switzerland and EPFL in particular for making it possible. Thanks to the EPFL administration for processing my reuests ???.

I would like to also thank Nevan Wichers, Le Nguen Hoang, El-Mahdi El-Mhamdi, Louis Faucon David Krueger, Ivan Vendrov, and Jeremy Nixon for the productive discussions in 2019-2020 on Causality and Machine Learning in general.

I would like to additionally thank Roman Pogodin for introducing me to EPFL and to computational neuroscience. I would like to thank Konrad Seifert, Mattias Berg, Ailin Parsa, Romain Ensminger, Denis Drescher, Michael Pokorny for supporting me.

Special thank you to my supervisor Johanni Brea for guiding and supporting me during the thesis.

}

\address{EPFL IC/SV Laboratory of Computational Neuroscience (LCN)\\
    BÃ¢timent AAB \\
    offices 135-141 \\
    CH-1015 Lausanne}

\begin{document}
    \maketitle
    \makededication
    \makeacks

\listoftodos

\begin{abstract}

"I choose this restaurant because they have vegan sandwiches" could be a typical explanation we would expect from a human. However, current Reinforcement Learning (RL) techniques are not able to provide such explanations, when trained on raw pixels.
RL for state-of-the-art benchmark environments are based on neural networks, which lack interpretability, because of the very factor that makes them so versatile -- they have many parameters and intermediate representations.
Enforcing safety guarantees is important when deploying RL agents in the real world, and guarantees require interpretability of the agent.
%The best result that could be obtained are two rollouts from the internal model of the agent, showing that one policy would result in a higher reward than the another policy. These rollouts would be high-dimensional objects that are hard to compare together or reason about.
%Additionally, feature attribution could be applied to these rollouts, which would show that certain parts of the raw image observations influence the outcome more than others. However, this would require a careful manual analysis of the feature attribution maps for every possible input that the agent can encounter.
Humans use short explanations that capture only the essential parts.
%This is possible because the world contains hierarchical structures in which only a few factors are important, and the rest can be ignored.
In our thesis, we address the problem of making RL agents understandable by humans.
%Interpretability of the resulting agent is crucial if such an agent is deployed in the real world in mission-critical scenarios, because a black-box neural network could give wrong predictions on unexpected inputs, without even giving an ability to know the reason of the failure. Thus, interpreting Reinforcement Learning agents is an important step towards safely deploying them.
In addition to the safety concerns, the quest to mimic human-like reasoning is of general scientific interest, as it sheds light on the easy problem of consciousness.

%To make an agent that can reason like a human does, we first need to learn a representation that corresponds to high-level objects contained in the environment, and their properties, like the representation that the natural language uses.
The problem of providing interpretable and simple causal explanations of agent's behavior is connected to the problem of learning good state representations.
If we lack such a representation, any reasoning algorithm's outputs would be useless for interpretability, since even the "referents" of the "thoughts" of such a system would be obscure to us.
%: it would take many words of natural language to explain one concept that the system uses.
% Thus, a subgoal of making interpretable agents is to create representations that allow for simple explanations of causes and their effects in a given environment.

One way to define simplicity of causal explanations via the sparsity of the Causal Model that describes the environment: the causal graph has the fewest edges connecting causes to their effects. For example, a model for choosing the restaurant that only depends on the cause "vegan" is simpler and more interpretable than a model that looks at each pixel of a photo of the menu of a restaurant, and possibly relies as well on spurious correlations, such the style of the menu.

%In addition, sparse causal models describing the latent dynamics are hypothesized by Yoshua Bengio to be crucial in the functioning of human consciousness, since human explanations have only few important causes.

In this thesis, we propose a framework \sysname for model-based Reinforcement Learning where the model is regularized for simplicity in terms of sparsity of the causal graph it corresponds to.
The framework contains a learned mapping from observations to latent features, and a model predicting latent features at the next time-steps given ones from the current time-step. The latent features are regularized with the sparsity of the model, compared to a more traditional regularization on the features themselves, or via a hand-crafted interpretability loss.
To achieve sparsity, we use discrete Bernoulli variables with gradient estimation, and to find the best parameters, we use the primal-dual constrained formulation to achieve a target model quality.
The novelty of this work is in learning jointly a sparse causal graph and the representation taking pixels as the input on RL environments.
We test this framework on benchmark environments with non-trivial high-dimensional dynamics and show that it can uncover the causal graph with the fewest edges in the latent space.
We describe the implications of our work to developing priors enforcing interpretability.

%For the future work, the causal graph can be used for easier grounding on a set of natural language explanations, as the agent now "knows" about the high-level concepts that the environment contains. In addition, a simple model would likely be less prone to overfitting, which could result in an improved robustness of such agents to distributional shift.
\end{abstract}

\maketoc

%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{ch:intro}
%%%%%%%%%%%%%%%%%%%%%%

Reinforcement learning is a general paradigm in Artificial Intelligence (AI) that considers two entities: an environment and an agent which interact with each other over multiple time-steps. The agent takes actions in the environment, and the environment gives the agent an observation representing the state of the environment, and a reward. The goal of the agent is to execute actions that lead to highest total reward during the interaction.

While RL environments in general can have arbitrary complexity, practically interesting environments usually have a certain low-dimensional structure in them. For example, in the popular CartPole environment, the observation (as image) has 720 000 variables (pixels), while the dynamics of the cart can be explained using only 4 variables with a simple Newtonian update rule. Atari games, while having 100 800 variables in an observation, can be described using only 128 bytes (of the RAM state). The dynamics in the pixel space would be complex, with one pixel at the next time-step potentially depending on all of the pixels on the previous one. However, the dynamics in the latent space is much simpler. The real world has this property as well \cite{Hamming1980}: while a camera can capture many megapixels per second observing a drone, the equations describing the dynamics of the drone only have a few variables, and are much simpler.


RL agents could (and sometimes already do) solve real-world problems, such as steering the wheel of an autonomous car, controlling a plant, or determining which content to recommend to people on a social network. However, in many applications there exist highly undesirable outcomes, such as the car crashing due to a mistake in controlling it, or showing polarizing or controversial content to people. To prevent such cases, we need to have an understanding of what makes an agent take a certain action, why it "thinks" it is optimal, and why it "thinks" it does not violate any sensitive constraints. Most capable agents are based on neural networks (NNs), and, therefore, it is hard to interpret them. Both the large number of parameters in the network, and many intermediate representations at each layer -- the distinctive features that allow NNs to be so versatile in fitting different kinds of data, lead to problems with interpretability (or explainability) \cite{Lipton}, because a priori there is no structure in the network: all variables depend on all variables of the input.

Even the state-of-the-art approaches that look at individual neurons and try to determine their functions (such as a neuron responding to the images of dogs) do not allow to obtain a concise explanation, since, while some neurons are more important than others in predicting a particular outcome, all neurons contribute a non-zero amount to the outcome. Because of this, it becomes impossible to determine in advance what the prediction of a network would be in all of the input scenarios.

In order to make an agent interpretable (be able to answer questions of the form "why an action was taken given a particular observation"), the agent needs to grasp the high-level concepts and objects contained in the environment. This is a Representation Learning (or Abstraction Learning) task \cite{Bengio2007,Learning2014,Deng,Mitchell2020,Scholkopf2021}, with an objective of obtaining representations allowing for good explanations. In addition, regardless of the task the agent is solving, it has to be able to explain in simple terms the effects of its actions on the environment.

While deep neural networks allow to fit various kinds of data, but lack interpretability due to the large number of parameters, in contrast, traditional ("good-old-fashioned" or GOFA) Artificial Intelligence (AI) approaches are interpretable (explainable) but lack the versatility of neural networks: they are not applicable to all the tasks.
Such GOFA approaches are characterized by discrete symbols that are the basis of a reasoning system, i.e. first-order logic.
These symbols correspond to real-world concepts and objects, such as the coordinates of a robot on the floor.
The drawback of such systems is that the mapping from real-world "messy" data (e.g. images from a camera) to discrete symbols (e.g. the coordinates) is hand-designed and thus requires effort.
Unifying these two approaches yields the best of both worlds, with the versatility of deep AI in being able to fit almost any dataset and the explainability of traditional AI.
To do so, we learn the representation of observations using a deep network. These representations are used by a system involving discrete components to make explainable predictions about the environment.

The quality of explanations given by such an agent would depend on the learned representation. Specifically, a representation of the observation in which every variable depends on every variable would be practically useless: an action potentially changes all of the variables, and, therefore, an explanation of action's effect would require to list all of them. Since the length of an explanation is crucial, the effect of an action should be describable by a change in only a few variables \cite{Schmidhuber2009,Chari2020,Gomez2006}.

The current explainable RL agents can be classified into three following categories. First, many projects learn sparse representations in terms of the feature vector sparsity: the predicted learned features associated with an observation are sparse at each time-step as a vector. Such a condition, however, is neither necessary nor sufficient for the {\em explanations} of the dynamics to be simple. Indeed, any sparse coding technique will yield sparse feature vectors (for example, by assigning the first $k$ most sparse vectors to all the $k$ states of the MDP in case of a finite MDP), but a feature at the next time-step would depend on many features at the previous time-step. Indeed, without any assumption on the way the sparse features are obtained, two neighboring states can have drastically different representations. Thus, an action potentially changes all of the variables. This demonstrates that sparse feature vectors are not sufficient to have simple explanations. On the other hand, it is possible to have simple explanations without sparse feature vectors. Indeed, the state of a CartPole environment has all of the components being non-0 most of the time, and, yet, these components comply with a simple explainable kinematics equation. Thus, sparse feature vectors are not necessary for simple explanations.

The second approach is to apply feature attribution to the convolutional networks representing policies, and the analysis of circuits of the network is performed. This approach gives a) the parts of the image most relevant to predict the action and b) the visualization of what each neuron in the network is activated with. While such an approach is extremely robust to the type of the underlying agent (the analysis is performed after the training and does not require any changes in the architecture), the drawback is that it should be performed for every input image: we do not know how a particular new image would affect the outputs, since each output action still depends on all of the pixels.

Finally, a line of work is investigating {\em causal} explanations of the agents' actions. This means that a causal graph corresponding to the environment is obtained, and an explanation for an action is then a simple path-finding from the action node to the reward node in this graph. The drawback of the current projects is that the graph is either given manually (by hand), or learned with significant manual work, such as designing the feature space. In this project, we extend this line of work by making the graph learnable end-to-end from the raw observations.

We would like to automatically uncover the simple structure in the underlying complex high-dimensional observations that the environment dynamics generates -- uncover the succinct "laws of physics" \cite{Bakhtin2019} that the environment operates with.
This would make the actions of an agent explainable.
It makes the actions of the agent explainable in terms of the "laws of physics" that we learn. In terms of Model-Based RL \cite{DeBruin2018,Corneil2018,Kaiser2019} (an RL agent equipped with a model of the environment), we would like to learn a simple model of the environment.
Specifically, we would like each variable in the model to depend on the fewest possible variables at the previous time-steps. This would make actions explainable in terms of the variables an action changes, and the explanation would be short. The model is the "good-old-fashioned" component, since it includes discrete elements: the variable's causes consist of a set of elements, rather than an array of numbers of weights for all possible causes.
Mathematically, we require sparsity of the model's graph of dependencies -- the predicted output for a variable depends only on a few variables at the previous time-step. If represented as a graph (with cause-variables at the current time-step as parents and effect-variables at the next time-step as children), this graph needs to have fewest possible edges.

To allow for such a simple {\em model}, we need to change the representation \cite{Kamalaruban}, from raw high-dimensional observations to the features, which the model predicts for the next time-step. Such features are obtained from a deep learning {\em decoder}\footnote{We call it a decoder rather than an encoder because the observations are already {\em encoded} by the function mapping the simple environment states such as the cart's position and velocity into the high-dimensional observation}, a function mapping observations to features. Given a decoder, the model can be obtained by simply fitting it in a supervised way, and only selecting the cause variables that lead to an improvement in the loss. Therefore, the decoder determines the simplicity of the model.

Thus, the decoder needs to be trained jointly with the model, and it is indirectly regularized by the model: the model is directly regularized for sparsity (in terms of the number of edges), and the decoder needs to output data that such model can predict.
In our approach, the decoder is a neural network, and a model is also a neural network with a special mechanism to enforce the sparsity of the architecture.

If the the decoder is not additionally regularized for non-degeneracy, it is possible for the system to always predict constant features. This correspond to closing eyes and "predicting" successfully that everything that follows will be dark. The decoder needs to preserve the information that is relevant to the task the system is required to solve. Both in the real world, and in games, not all parts of the observations need to be predicted to achieve goals. For example, the color of an obstacle that a robot encounters is likely not important when it comes to avoiding it. However, this is task-specific, because, for example, a table is much less dangerous as an obstacle compared to water (a robot will be damaged if put into water, while an encounter with a table will likely only result in a loss of time when accomplishing a goal). Therefore, the model needs to predict task-specific features \cite{Cortese2020}.

In this project, we use the two popular types of regularization: we either predict the reward (thus, only the aspects of observations relevant for predicting the reward are kept), or the whole observation (all the aspects of the observation are represented in the latent features). This is achieved by introducing a {\em reconstructor} -- a network that predicts either the reward or the full observation given the latent features.

Our approach allows to obtain "laws of physics" of an environment, or, more specifically, a sparse {\em Causal Model} \cite{Pearl2020,Wong2020} representing the environment's dynamics.
Our work can be seen as an experimental confirmation of the Consciousness Prior proposal \cite{Bengio2017}, specifically, the part that simple dynamics models learn to interpretable representations \cite{Barcelo2020,Cranmer2020,Goyal2021}.
If an environment allows for a complete separation of some parts of its dynamics (for example, termination of the episode only depends on the health of the player, and another variable, such as the number of ammunition does not affect the health), out approach would capture it, because a graph with two components has less edges than a connected graph. This way, we {\em disentangle} the representation, as it consists of (recurrent) {\em independent} mechanisms.
Such a model is very useful for explanations, because an effect of each action can be described in terms of changes in only a few variables (out of many). For example, for Cartpole with images as observations, our approach is expected to output the 4 features representing cart's and pole's position and velocity.

The main challenge when implementing our approach is combining all the requirements (sparsity of the causal model, learning the decoder, and the non-degeneracy of the decoder) into a single training procedure. Specifically, we propose and compare several methods (simple aggregation of losses with coefficients, primal-dual method for optimization under constraints, and an adaptive scheme for choosing the loss coefficients) and employ several useful tricks (such as using a {\em relative} Mean Squared Error (MSE) loss to have an interpretable magnitude of the loss, and adding the sampled binary mask of selected cause features to the model that predicts the effects), and use different methods to enforce sparsity (from $l_1$-regularization to discrete variables with various methods to compute the gradients). We present an ablation study to show the effects of various tricks and techniques used.

We design a simple toy environment to illustrate learning of sparse causal models from high-dimensional data, and show that our approach successfully obtains the minimal causal graph on it. Additionally, we illustrate the same positive behavior on a grid-world environment. Surprisingly, the method find a simpler (but correct) graph for the toy environment than the authors anticipated.

{\bf Contribution.} This thesis presents, up to our knowledge, the first successful result on learning a sparse causal model jointly with the representation on a challenging high-dimensional problem with our approach \sysname. While the parts of the "recipie" used, such as discrete variables to learn a sparse causal graph can be found in existing literature, combining them into a working system that learns the representation and the sparse causal graph, along with the required modifications and the techniques, is a novel main result. The main difference with respect to existing methods of learning abstractions in RL is that our only prior is the sparsity (simplicity) of the learned model in the latent space. We do not add any additional priors or assumptions \cite{Francois-Lavet2018,Ying2020} on the feature space, on the environment, or on the agent. We briefly mention the implications of our work on the Integrated Information Theory \cite{Tononi2016} of consciousness.

The result of this work can be used to improve significantly the explainability of Reinforcement Learning agents. First, a learned causal graph opens the possibility to drastically reduce the amount of data required to {\em ground} \cite{Lahlou2019,Hui2020} the various aspects of the observations with natural-language sentences.
In this way, a dataset containing only two sentences "cart going left" and "cart going right" with two data points will likely result in a mapping of the sentences to the learned velocity variable. In contrast, if we ground raw images this way, the model is likely to overfit to the particular details that the image has (spurious correlations), and will not depend on the true cause.
Sparse causal graph-based or natural language-based explanations \cite{Madumal2019,Ehsan2018,Abramson2020} of agent's actions would allow for faster certification of agents for mission-critical applications.
Secondly, since the learned model is the simplest one, it is likely to be less prone to overfitting, and, therefore, more robust to distributional shift \cite{VanSteenkiste2020,Gomez2006,Priol2021}. In this way, it would be easier to adjust the agent to a novel environment either without modifications (because the model relies more on a discrete set of concepts, than on the whole vector of features), or with re-training the decoder only (for example, if the shape of an obstacle has changed, only the decoder needs to be updated). For the latter part, the training should take less time, because the bulk of the problem is already solved -- the model knows how to avoid obstacles.
Third, out prior to have the simplest model could make the agent require less data to train: since it expects the underlying dynamics to be simple, it will converge to the true model faster, because a more complex model (that a regular agent is likely to take as a first guess) will be discarded.
Next, having a disentangled representation with independent mechanisms allows to reuse components in the model \cite{Bengio2012,Veerapaneni2019,Didolkar2021,Gomez2006}, and learn it even faster. Many environments share the same dynamics -- for example, navigating in a 2-dimensional maze with 4 actions (up, down, left, right). The learned graph would be the same for all such environments (up and down actions affecting the vertical coordinate variable, and the left-right actions the horizontal one). We can store the commonly-occurring patterns like this one (another example is the velocity of an object being mirrored when colliding with an obstacle), we could represent every environment as a number of "stock" components, or recurrent independent mechanisms (coordinate variables, colliding objects) with parameters, with a small number of custom ones. Such an approach would lead to faster learning, as the agent would "guess" a component much faster, compared to learning it from scratch.
Finally, the method can be applied to challenging environments interesting in their own right. For example, the AlphaFold algorithm was applied to the protein folding problem. The model of protein folding that the agent learns is (aprori) not interpretable. However, using out method, we could obtain a sparse causal graph representing the dynamics of protein folding, which could be useful, because the problem becomes simpler. Indeed, our approach could discover interesting regularities in the way proteins fold, in terms of the final state depending only on the few variables in the initial state -- automating the process of discovery of the scientific laws of the natural world.

{\bf Source code.} The source code for the project is available at
\begin{center}
\href{https://github.com/sergeivolodin/causality-disentanglement-rl}{github.com/sergeivolodin/causality-disentanglement-rl}
\end{center}

%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
\label{ch:background}
%%%%%%%%%%%%%%%%%%%%

In order to proceed, we need to introduce several important concepts: model-based reinforcement learning, causal models, methods for enforcing sparsity and methods for computing gradients for discrete variables.

We use $\mathbb R$ to denote real numbers, $[n]$ to denote the first $n$ natural numbers ($[n]=\{1, 2, ..., n\}\subset \mathbb N$). For a vector with $n$ dimensions $x=(x_1,...,x_n)\in \mathbb R^n$, we use $\|x\|_p=\left(\sum_{i=1}^n|x_i|^p\right)^{1/p}$ the $p$-norm of $x$. The specific cases that we use are the $1$-norm $\|x\|_1=\sum\limits_{i=1}^n|x_i|$, and the $0$-norm $\|x\|_0=\sum\limits_{i=1}^n\mathds 1_{x_i\neq 0}$ -- number of non-zero components in $x$ ($\mathds 1_z=1$ if $z$ is true, and $0$ otherwise).
Given a distribution $X$, we write $x\sim X$ meaning that the random variable $x$ has the distribution $X$ ($x$ "sampled from" $X$). We write $\mathbb E_{x\sim X} x$ meaning the expected value of $x$ when sampled from a distribution $X$. For two vectors $x,y\in\mathbb R^{n}$, we define the element-wise product as $x\odot y=(x_1y_1,...,x_ny_n)$. For $p\in[0,1]$, $Be(p)$ is the Bernoulli distribution: if $b\sim Be(p)$, $P(b=1)=p$ and $P(b=0)=1-p$.

\section{Reinforcement Learning}
\label{sec:rl}
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{diagrams/rl}
    \caption{The reinforcement learning framework. An environment gives an observation $o_t$ and a reward $r_t$ at time-step $t$, and the agent responds with an action $a_t$. Then, the cycle repeats.}
    \label{fig:rl}
\end{figure}


The standard Reinforcement Learning framework consists of an {\em environment} $\mu$ and an {\em agent} $\pi$ (see \autoref{fig:rl}). The environment and the agent interact during an {\em episode} consisting of $T<\infty$ discrete time-steps. First, the agent receives an {\em observation} $o_1\in O$ from the environment, where $O$ is the {\em observation space}. Next, the agent executes an {\em action} $a_1\in A$ where $A$ is the {\em action space}. Next, the environment gives the agent a) a scalar reward $r_1\in R$ where $R\subseteq \mathbb R$ is the {\em reward space}, and b) the next observation $o_2\in O$. Next, either the environment terminates the interaction, in this case, $T=2$, or the cycle repeats (the agent executes an action, etc). We introduce the termination variable $d_t$ ("done") as $d_t=0$, $t<T$ and $d_T=1$.

A {\em history} (or an {\em episode}, or a {\em rollout}) of the interaction up to time-step $\tau$ consists of all the variables in the interaction in chronological order: $h_{\tau}=(o_1, a_1, r_1, o_2, ..., a_{\tau-1}, r_{\tau}, o_{\tau})$.

For each time-step, the following holds: $(o_t, r_t)\sim \mu(a_{t-1}, h_{t-1})$ and $a_t\sim \pi(h_{t})$. This means that the observation and the reward at step $t$ is defined by the environment $\mu$ given the previous history, and the action at the step $t$ is defined by the agent given the previous history.

We say that $h_T\sim (\mu\leftrightarrow\pi)$ if the history $h_T$ was obtained from an interaction between the environment $\mu$ and the agent $\pi$.

The {\em value} is defined as the discounted sum of rewards: $V(h_T)=\sum\limits_{t=1}^T\gamma^t r_t$, where $\gamma\in[0, 1]$ is the {\em discount factor}. The goal of the agent is to find such $\pi$ that $\mathbb E_{h_T\sim (\mu\leftrightarrow \pi)} V$ is maximized.

In our project, $A$ is discrete {\em discrete action}, such as $A=\{\mbox{up},\,\mbox{down},\,\mbox{left},\,\mbox{right}\}$\footnote{the model supports real-valued actions, they would be treated the same way as real-valued features without any difference}, and $O=\mathbb R^o$, where $o$ is the dimension of each observation. For example, $o=210\times 160\times 3$ for 210 rows, 160 columns and 3 RGB channels for an Atari observation, or $o=128$ for the $128$ RAM components in the RAM version. The reward space $R$ is bounded: $R=[r_{\min}, r_{\max}]$.

Additionally, we consider a {\em Markov} class of environments: one where next time-step only depends on the previous time-step in the history, and not on the ones before it\footnote{A typical workaround to make some non-Markov environments to comply with this property is to stack a few observations together}. For this class of environments,
$$
(o_t, r_t)\sim \mu(a_t, o_{t-1}),\,a_t\sim\pi(o_t)
$$

For example, for Cartpole, $o_t$ is obtained using the kinematics equation, and $a_t$ is computed based on the velocities and the positions of the system.

Special important classes are {\em deterministic environments} -- ones where $\mu$ is a constant for every history (and not a distribution), and {\em deterministic agents} -- ones where $\pi$ is a constant given a history (and not a distribution).

In what follows, we consider Markov deterministic environments with deterministic agents\footnote{To account for the stochasticity, the decoder and reconstructor should include an additional sampling step, and noise variables should be additionally injected as potential causes into the model}.


\section{Model-based reinforcement learning}
\label{sec:mbrl}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{diagrams/rl_model}
    \caption{A single time-step model $M_o$ predicts the next observations $\hat{o}_{t+1}\approx o_{t+1}$ given the current observations $o_t$ and the actions $a_t$.}
    \label{fig:rlmodel}
\end{figure}


Model-based reinforcement learning \cite{DeBruin2018,Corneil2018,Kaiser2019} agent additionally includes a {\em model of the environment} $M_o$\footnote{The index $o$ means that the model works in the observation space} which is a function mapping the current observation $o_t$ and the action taken $a_t$ to the predicted next observation $\hat{o}_{t+1}$ (see \autoref{fig:rlmodel}):
$$
\hat{o}_{t+1}=M_o(o_t, a_t)
$$

This function has the same signature as the environment. It is trained to fit the true dynamics: $\|o_{t+1}-\hat{o}_{t+1}\|\to\min$.

Since predicting the high-dimensional observations is computationally expensive, the observation model is often decomposed into three components: a decoder, a feature model, and a reconstructor:
$$
M_o(o_t, a_t)=R(M_f(D(o_t), a_t))
$$

Here, the decoder $D$ maps an observation into a low-dimensional latent {\em feature space} $f_t=D(o_t)$, $f_t\in \mathbb R^f$ with $f$ being the dimension of the feature space. The feature model $M_f$ predicts next features from the current ones $f_{t+1}=M_f(f_t,a_t)$, and the reconstructor $R$ maps the features back to the observation space: $\hat{o}_{t+1}=R(f_{t+1})$.

This way, the computationally expensive reconstructor can be fit on two kinds of data: end-to-end to predict the next features $\|\hat{o}_{t+1}-o_{t+1}\|\to\min$ where $\hat{o}_{t+1}=M_o(o_t, a_t)$, and to predict the current ones $\|\hat{o}_t-o_t\|\to\min$, where $\hat{o}_t=R(D(o_t))\equiv R(f_t)$. This way, the convergence is sped up.

Another technique prevents predicting high-dimensional observations altogether by replacing $R$ with a value function predictor \cite{Schrittwieser2019,Ayoub2020}, which is fit to predict the reward-to-go $R_t(h_T)=\sum\limits_{\tau=t}^{T}\gamma^{\tau-t+1}r_{\tau}$:
$\|\hat{R}_t-R_t\|\to\min$ where $\hat{R}_t=R_{value}(f_t)$.

\section{Causal modeling and deep causal modeling}
\label{sec:causal}
In a nutshell, {\em Structural Causal Models} \cite{Runge2019,Wong2020,Guo2020} are regular models fitting data from some distribution, and in which we explicitly care about which variables a given variable depends on. For example, if we have a dataset with a time series of the number of cancer patients, and one with a time series of the number of smokers, we could just fit an autoregressive model to predict both time series using both values. However, we could explicitly limit the effect of one time series on the other in our model, by simply not including it as the input to the model. In this way, we can discuss the differences in the losses when regressing the number of cancer patients given the time series with the smokers data: if this difference is not 0, it could be that this data suggests that smoking causes cancer.

More formally, suppose we have a set of variables $f_i$, $i\in[f]=(1, ..., f)$. These variables change with time and form time series $f_i^{t=1},f_i^{t=2},...,f_i^{t=\tau},...$ for $i\in[f]$\footnote{We write time indices below for the whole vector $f_{\tau}=(f_1^{t=\tau},...,f_t^{t=\tau})$, and we write them above, if we additionally specify the index of the feature $f_i^{t=\tau}$}.

A {\em deterministic structural causal model (SCM) with 1-step dependencies} is a set of functions $F_i$, $i\in[f]$, which output the value of the feature $i$ at the current time-step given a subset of features $PA_i$ at the previous time-step: $f_i^{t=\tau}=F_i(f_j^{t=\tau-1},j\in PA_i)$. In the following, we write $F_i(PA_i)\equiv F_i(f_j^{t=\tau-1},j\in PA_i)$. For example, if $PA_i=[f]$ for all $i\in[f]$, each feature $i$ depends on all features $j$ at the previous time-step. This is the most general case -- as all SCMs will fall into this category. However, we would like to find structure in the data: some features do not require {\em all} the features to predict them. For example, in a game the coordinate of the player is unlikely to depend on its ammunition. There could be a correlation (for example, having collected some inventory allows access to another room), but the ammunition is not required to predict the position, as we could just use the previous position, the action taken, and whether or not there are any obstacles around, instead.

Granger causality \cite{Barnett2015} is defined in the way as in the example above: a feature $j$ can be excluded from $PA_i$ if the best function (measured by the Mean Squared Error loss) that predicts $f_i$ from $PA_i\setminus\{j\}$ has the same loss as the best function that predicts $f_i$ from $PA_i$.

More generally, we set $\delta(PA_i,j)=\inf_{F_i}\mathbb E_t\|f_i-F_i(PA_i\setminus\{j\})\|-\inf_{F_i}\mathbb E_t\|f_i-F_i(PA_i)\|$. This is the difference in the losses without and with feature $j$ when predicting the feature $i$, given the current set $PA_i$.
Given a threshold $\varepsilon>0$, if $\delta(PA_i,j)<\varepsilon$, we could exclude $j$ from $PA_i$, and this would only worsen the prediction quality by $\varepsilon$.

\subsection{Neural Granger Causality}
\label{subsec:neural_granger}
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{diagrams/feature_mask_feature}
    \caption{The model $F_1(f^t\odot B_1)$ only uses features determined by the mask $B_1=(0,1)$ to estimate the value of $\hat{f}_i^{t+1}$}
    \label{fig:featuremaskfeature}
\end{figure}


In order to find $F_i$ in practice, we model them with neural networks \cite{Tank2018,Kalainathan2018, Ng2019} with input size $f$ (including all features). $F_i\colon \mathbb R^{f}\to\mathbb R$. In order to account for $PA_i$, we pre-multiply the input to each function $F_i$ by a binary mask $B_i\in\{0,1\}^f$ (see \autoref{fig:featuremaskfeature}):
$$
\hat{f}_i^{t+1}=F_i(f^t\odot B_i)
$$

Next, we fit functions $F_i$ using least squares regression:

$$
\mathcal L=\frac{1}{T}\sum\limits_{i=1}^f\sum\limits_{t=1}^T \left(f^{t+1}_i-\hat{f}_i^{t+1}\right)^2\to\min
$$

The gradient with respect to the parameters of $F_i$ is computed in standard way, and we present methods to enforce sparsity of $B_i$ in the next section.

\section{Enforcing sparsity in deep architectures}
\label{sec:sparse_deep}
Suppose that we are fitting a loss function which depends on a binary variable, like in the previous section:
$$
\mathcal L=\frac{1}{T}\sum\limits_{t=1}^T(y_t-F(x_t\odot B))\to\min\limits_{F,B}
$$

Here, $y_t\in\mathbb R^y$ is the target, $x_t\in\mathbb R^x$ is the input data, and $B\in\mathbb \{0,1\}^x$ is a binary mask.

One (very inefficient) method to solve this problem is to a) go over all possible masks $B$ b) for each of them, find the best model $F$ c) select the most sparse $\|B\|_0\to\min$, such that $\mathcal L(B)\leq\varepsilon$ for some threshold $\varepsilon$. The complexity of such search is $\mathcal O(2^x)$, which is not feasible even for $x=20$. In our setups, even for the simplest environment, $x>20$. In general, this problem would be NP-hard, as it is the feature selection problem.

We can use gradient descent on $B$ if we relax the problem. There are several methods for it.
\begin{enumerate}
    \item The obvious solution is to allow $B_i\in[0,1]$ instead of $B_i\in\{0,1\}$ and regularize the model with $\|B\|_1$ \cite{Tank2017}. This would enforce sparsity on $B$. However, since we are using a multi-layer network, this solution would not work: in practice, the coefficients in $B$ are never exactly $0$, and the subsequent layers can simply multiply the incoming value by a large constant. Therefore, while $B$ is sparse, the network could actually still be using all the features.
    \item In the rest of the methods \cite{Ng2019,Abid2019,Lamb2020,Kalainathan,Yang2020}, we sample (element-wise) $B\sim Be(P)$ from the Bernoulli distribution, where $P\in[0,1]^x$ is a matrix of probabilities. The computation of the gradient of the loss with respect to the parameters of the distribution is described in the next section
\end{enumerate}

\section{Methods to compute gradients of discrete variables}
\label{sec:grad_discrete}
This section describes standard methods to compute gradients with respect to a binary variable. We have the same setup as in the previous section.

We sample $B_i\sim Be(P_i(\theta_i))$, where $\theta_i$ parameterizes the probability $P_i$, and, therefore, the distribution $Be(P_i(\theta_i))$.

We want to compute a gradient of $\frac{\partial \mathcal L}{\partial \theta_i}$ for one of the elements $P_i$ The list of methods generally follows \cite{Bengio2013, Weib2011,Maddison2017} unless stated otherwise.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{plots/plot_gumbel_gradient.pdf}
    \caption{Gradient for Gumbel-Softmax for different parameters $\tau\in\{1/2,1,2\}$. We obtain it by considering the Gumbel-Softmax estimate $z_p=1/(1+\exp((\log(1-p)-\log p+g_q-g_p)/\tau)$ and noticing that $\delta=g_q-g_p$ has a Logistic distribution. We compute the expected gradient for $\tau=1$ and $\tau=1/2$ via symbolic integration with Wolfram Mathematica, and using numerical sampling for $\tau=2$ (the integral as a function of $p$ does not seem to have a symbolic value for $\tau=2$). Both the expectation and the variance of $\frac{dz_p}{dp}$ have asymptotes at $p=0,1$. Specifically, $\lim_{p\to 0,1}\mathbb E dz_p/dp=+\infty$, and $\lim_{p\to 0,1}\mathbb E \left(dz_p/dp\right)^2=+\infty$: the gradient has a high expected value and is also noisy. The gradient is well-defined at $p=1/2$: its expectation and variance have a limit at $p\to 1/2$, even though the symbolic expression contains $p=1/2$ in the denominator. For example, for $\tau=1$, $\mathbb E dz_p/dp=(2-4p+\log\frac{p}{1-p})/(2p-1)^3$.}
    \label{fig:gumbel_gradient}
\end{figure}

\begin{enumerate}
    \item REINFORCE gradient. Consider $\mathbb E L'_{\theta_i}=\frac{\partial}{\partial\theta_i}\mathbb E_{B_i\sim Be(p_i),\,i\in[x]}\mathcal L(b_1,...,b_x)$. We split the expectation into two: $\mathbb E L'_{\theta_i}=\frac{\partial}{\partial\theta_i}\mathbb E_{B_i\sim Be(p_i)}\mathbb E_{B_i\sim Be(p_j),\,j\in[x],\,j\neq i}\mathcal L(b_1,...,b_x)$. Next, we write $\mathbb E_{B_i}$ explicitly with $\mathbb E_{b_{\setminus i}}$ meaning $\mathbb E_{B_i\sim Be(p_j),\,j\in[x],\,j\neq i}$:
    $$
    L'_{\theta_i}=\frac{\partial}{\partial \theta_i}\left[p_i\mathbb E_{b_{\setminus i}}\mathcal L\big|_{B_i=1}+(1-p_i)\mathbb E_{b_{\setminus i}}\mathcal L\big|_{B_i=0}\right]
    $$
    Now, only $p_i$ depends on $\theta_i$, and the expected loss $\mathbb E_{b_{\setminus i}}\mathcal L$ does not. Therefore,
    $$
    L'_{\theta_i}=\frac{\partial p_i}{\partial\theta_i}\cdot \left[\mathbb E_{b_{\setminus i}}\left(L\big|_{B_i=1}-L\big|_{B_i=0}\right)\right]
    $$
    The second multiplier on the right-hand side in the equation above is the difference between the expected losses with the variable $B_i$ on and off, while the expectation is taken with respect to all the other binary variables.

    Now, if we have a batch with sampled masks $B_i$, in which the variable $B_i$ takes the value of $1$ $N_i^{+}$ times, and the value of $0$ $N_i^-$ times, we can set up a sample mean estimate:

    $$
    L'_{\theta_i}\approx \frac{\partial p_i}{\partial\theta_i}\cdot\left[\frac{1}{N_i^{+}}\sum\limits_{t=1,B_i^t=1}^TL(b_1,...,b_x)-\frac{1}{N_i^{-}}\sum\limits_{t=1,B_i^t=0}^TL(b_1,...,b_x)\right]
    $$

    Note that we have to have at least one sample with $B_i^t=0$ and one with $B_i^t=1$ for this expression to be valid. Otherwise, the gradient is undefined, and the update is not performed for this component of the gradient.

    The drawback of such an approach is high variance, pertinent to REINFORCE estimators. The advantage is its simplicity of tuning (it has no hyperparameters), and the fact that the gradient estimate is unbiased (with increases in batch sizes $T$, the gradient tends to its true value by the Central Limit Theorem).\footnote{This is called a REINFORCE gradient because it uses the same technique -- log-likelihood trick, as the REINFORCE algorithm for Reinforcement Learning. Another way to obtain the equation is to consider the log-likelihood trick: $\partial/\partial\theta_i\mathbb E_{B_i}\mathcal L=\partial/\partial\theta_i\int p(B_i)\mathcal L(B_i)dB_i=\int\partial p(B_i)/\partial\theta_iL(B_i)dB_i=\int\partial \log p(B_i)/\partial \theta_i p(B_i)L(B_i)dB_i=\mathbb E_{B_i}\partial \log p(B_i)/\partial\theta_i L(B_i)$. For the Bernoulli distribution, $\partial \log p(B_i)/\partial \theta_i=\frac{\partial p_i}{\partial\theta_i}\frac{1}{p_i}$ if $B_i=1$ or $-\frac{\partial p_i}{\partial\theta_i}\frac{1}{1-p_i}$ otherwise, and the denominators cancel out with the probability mass function.}

    In this approach, $\theta_i\equiv p_i$. In order to keep the values of $p_i$ in $[0,1]$, we simply project to $[0,1]$ after each gradient step: if $p'_i>1$, we set $p_i=1$, and the same for $p'_i<0$.

    \item Straight-through estimator. We set $\theta_i\equiv p_i$ and simply ignore the fact that there was sampling:
    $$
    \frac{\partial \mathbb E \mathcal L}{\partial p_i}=\frac{\partial \mathbb E \mathcal L}{\partial B_i}
    $$

    Here, we simply compute the gradient with respect to the variable $B_i$, as if it were a real-valued one.
    \item Gumbel-Softmax \cite{Jang}. In the forward pass (when computing the value of the loss), we sample from the Bernoulli distribution. In the Backward pass, we compute the gradient as follows. Consider the probabilities $p_i$ and $q_i=1-p_i$ corresponding to sampling either $1$ or $0$ (two categories).

    We define $l_p=\log p_i+g_p)$ and $l_q=\log q_i+g_q$. Here, $g_p$ and $g_q$ are independent samples from the {\em Gumbel} distribution: a continuous distribution with a density function $f_{g_p}(x)=\exp(-(x+\exp(-x)))$. It has the following property: $\arg\max [l_p, l_q]\overset{d}{=}Be(p)$: the distributions are equal.

    Now, we replace the $\arg\max$ with $\mbox{soft}\arg\max$:

    $$
    B_i:=z_p=\frac{\exp(l_p/\tau)}{\exp(l_p/\tau)+\exp(l_q/\tau)}
    $$

    When $\tau\to 0$, $z_p\to Be(p_i)$ ($\mbox{soft}\arg\max$ becomes $\arg\max$). If $\tau\to\infty$, $z_p\to1/2$.

    For $\tau\in(0,\infty)$, We set the (biased) gradient to be:

    $$
    \frac{\partial \mathbb E\mathcal L}{\partial p_i}:=\frac{\partial \mathbb E \mathcal L}{\partial B_i}\frac{\partial z_p}{\partial p_i}.
    $$

    The advantage is that the gradient is defined even for a single sample. Compared to the previous approach, there is no additional noise due to the fact that a sample estimate is used.

    The Figure \ref{fig:gumbel_gradient} shows the expected gradient of the estimator \cite{andriyash2018improved} with respect to the probability.
    The asymptotes at $p\to 0$ and $p\to 1$ mean that the gradient becomes bigger and bigger, as the entropy of the distribution decreases. Therefore, the probabilities never reach these terminal values, as the noise "pushes" them away from these values.

    In our project, it is important that the sampling probabilities actually approach the value of $p=1$ (see \autoref{subsec:proba_ranges}). Therefore, we would need to modify Gumbel-Softmax with an additional thresholding mechanism to update the model parameters.

%    Using the Taylor series, the gradients of Gumbel-Softmax at points $0$ and $1$ are:
%    $$
%    z_p=p\exp(g_{i1}-g_{i2})+o(p),\,
%    z_p=1-(1-p)\exp(g_{i2}-g_{i1})+o(1-p),\,p\to 1
%    $$
%
%    Therefore, the derivatives for this smooth function are:
%    $$
%    z'_{p,p_i}=\exp(g_{i1}-g_{i2})+o(p),\,p\to 0,\,z'_{p,p_i}=\exp(g_{i2}-g_{i1})+o(1-p),\,p\to 1
%    $$
%
%    For the point $p=0.5$, $z'_{p,p_i}=4e^{g_1+g_2}/(e^{g_1}+e^{g_2})^2$
%
%    The random variable $g_{i1}-g_{i2}$ is distributed logistically with parameters $\mu=0$, $s=1$, and, therefore, the value $\exp(g_{i1}-g_{i2})$ is heavy-tailed: it's mean value and the variance diverge. We expect arbitrary high values for this expectation. This means that as we get closer to the value of $p=0$ or $p=1$, the noise in the gradients increases. Other terms in the series cannot improve the convergence of the integral, as even the first term diverges.
%
%    In practice, it means that Gumbel-Softmax never reaches this value, and rather slows down.

%    In some implementations, the value of $g$ is truncated with the sampling equation $g=-\log(-\log(u+\varepsilon)+\varepsilon)$ with $\varepsilon=10^{-20}$ and $u\sim U[0,1]$ (uniform distribution). Such a variable will result in a converging integral for the mean and variance.



%    Since $g_{i1}$ and $g_{i2}$ are independent and identically distributed, we have $\mathbb E e^{g_1-g_2}=\frac{\mathbb E e^{g_1}}{\mathbb e^{g_2}}=1$, and the expected values of the derivatives become:
%    $$
%    z'_{p,p_i}=1+o(p),\,p\to 0,\,z'_{p, p_i}=1+o(1-p),\,p\to 1
%    $$



%    The standard implementation takes logits $h=\log p_i$ as input. Since $\partial z/\partial p=\partial z/\partial h\cdot \partial h/\partial p$, and $\partial h/\partial p=1/p$ we compute $\partial z/\partial h=p+o(p^2),p\to 0$. Therefore, the derivative w.r.t. the logits {\em vanishes} as we approach $0$ or $1$ in probability.

 %   \item Therefore, we propose an altered version -- we compute the trick with probabilities (not logits), project them to $[0,1]$, and, in case if $p<\varepsilon$ or $1-p<\varepsilon$, we output $1$ as the gradient (the limit value), to avoid numerical instabilities.\footnote{Usually, variables are only learned once -- if the gradient is positive at the beginning, the variable is likely to be used -- the data distribution is stationary. However, our setup has a learned decoder, which makes the distribution non-stationary. Therefore, a low value of a logit for a disabled switch variable will take a long time to update using Gradient Descent, because of the issue above. In addition, usually it is fine that the value of a variable is not fully $1$ or $0$ during training. However, in our setup, we require to know the value of the loss and whether it satisfies constraints. One item in a batch with the wrong variable turned off can increase the batch average significantly and act as a false positive in terms of constraint satisfaction.}


    \item Straight-through Gumbel-Softmax. In this method, we sample from the true Bernoulli distribution on the forward pass, and use the Gumbel-Softmax technique only to compute the gradient. The advantage is that the model never "sees" intermediate values $B_i\in(0,1)$ during the forward pass -- features are truly either on or off. We require this, because a variable partially turned off does not prevent the model from using it -- it could just learn to increase the weights at the subsequent layer.
%    \item Custom gradient from LCN \cite{bellec2018long} [compare with no-logits-Gumbel]
\end{enumerate}

In our setup, we use REINFORCE implementation as the simplest one.%, as well as the straight-through no-logits version of Gumbel-Softmax. We compare it to the LCN version in terms of convergence speed {\em for our problem} in the ablation study.
The Gumbel-Softmax trick would result in duplicated variables, as the probabilities never reach $p=1$, see \autoref{subsec:proba_ranges}.


%%%%%%%%%%%%%%%%
\chapter{Design of \sysname}
\label{ch:design}
%%%%%%%%%%%%%%%%
\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{diagrams/step_spaces}
    \caption{Step representations used in the project. At the bottom, the environment's Game Engine (GE) generates the next state $s_{t+1}$ given the previous state $s_t$ and the action $a_t$. Next, the states are encoded into high-dimensional observations $o_t=E(s_t)$. The true state dynamics $s_t$, as well as the encoder $E$ are unknown. The proposed methods finds a feature space $f_t=D(o_t)$ by applying the decoder $D$. The dynamics in the space of features is modeled by the function $M_f$. On the right, the parts of the dynamics model are shown for the MsPacman\cite{brockman2016openai} game.}
    \label{fig:step_spaces}
\end{figure}


We have an agent $\pi$ interacting with the environment $\mu$. We start from the model-based Reinforcement learning setup with a decoder and a reconstructor. We would like to obtain the feature space in which the model $M_f$ becomes most simple in terms of a metric $K$. This is the Bayesian approach: with the sparsity prior on the model, we would like to find the posterior distribution over models given the data from the environment.

We consider a model of the environment $\hat{f}_{t+1}=M_f(f_t,a_t)$ in the sense of \autoref{sec:mbrl}. Additionally, we would like some components of $f_t$ and $a_t$ to be ignored when predicting some component of the whole feature vector $f_{i,t+1}$. We view the model $M_f=[M_f^1,...,M_f^f]$ (splitted into scalar functions predicting each feature) as a causal model in the sense of \autoref{sec:causal}. The set of variables of this causal model contains all features, all actions, and additionally the variable $r$ for the reward and the variable $d$ for the "done" (see \autoref{sec:rl} for definitions):
$$
\begin{array}{lcl}
X&=&\{f_1,...,f_f,a_1,...,a_a,r,d\},\\
X_{in}&=&\{f_1,...,f_f,a_1,...,a_a\}\\
X_{out}&=&\{f_1,...,f_f,r,d\}
\end{array}
$$

The variables $X_{in}\subset X$ are the input (ones used for prediction), and the variables $X_{out}\subset X$ are the output (ones being predicted).
We would like to predict the next features, as well as the reward $r$ and the done $d$ at time-step $t+1$ given features and actions at the time-step $t$. The prediction of the model for a variable $x$ consists of pre-multiplying the input features (variables $f_t$ and $a_t$, written as a vector in what follows as $fa_t=[f_t,a_t]\in\mathbb R^{f+a}$) with the binary mask $B_x$, to select the input features, and then applying the SCM function $F_x$: $x_{t+1}=F_x(fa_t\odot B_x)$, $x\in X_{out}$.
$$
\begin{array}{lcllrllr}
\hat{f}_{t+1}^1&=&M_f^1(f_t,a_t)&=&F_{f_1}(&[f_t,a_t]&\odot B_{f_1}&)\\
&...&\\
\hat{f}_{t+1}^f&=&M_f^f(f_t,a_t)&=&F_{f_f}(&[f_t,a_t]&\odot B_{f_f}&)\\
\hat{r}_{t+1}&=&M_f^r(f_t,a_t)&=&F_{r}(&[f_t,a_t]&\odot B_{r}&)\\
\hat{d}_{t+1}&=&M_f^d(f_t,a_t)&=&F_{d}(&[f_t,a_t]&\odot B_{d}&)\\
\end{array}
$$

The \autoref{fig:example_rl_scm} shows an example SCM.

\begin{figure}
    \centering
    \includegraphics[width=0.3\linewidth]{diagrams/example_rl_scm}
    \caption{An example of a Structural Causal Model we are looking for. Here, the number of features $f=2$ and the number of actions $a=2$. The equations can be written as $\hat{f}^1_{t+1}=F_{f_1}(f^1_t,a^1_t)$ (next time-step $f_1$ depends on itself at the previous time-step, and on whether the action $a_1$ was taken), $\hat{f}^2_{t+1}=F_{f_2}(a_t^1,a_t^2)$, $\hat{r}_{t+1}=F_r(f_t^1)$, $\hat{d}_{t+1}=F_d(f^2_t)$. The masks corresponding to this graph equal $B_{f_1}=(1,0,1,0)$ (in order $f_1,f_2,a_1,a_2$), $B_{f_2}=(0,0,1,1)$, $B_r=(1,0,0,0)$, and $B_d=(0,1,0,0)$}
    \label{fig:example_rl_scm}
\end{figure}


\section{Measure of complexity}

\subsection{Sparse features are neither necessary nor sufficient for interpretability}
First, we give a detailed example of why feature sparsity does not imply interpretability, and interpretability does not imply feature sparsity.

\begin{example}
    Consider the $2D$ grid-world environment from Figure \ref{fig:env2d2x2} with $H,W>2$. It has $HW$ states. Consider binary features $f_i\in\{0,1\}$. Given the feature dimensionality $f$, we select the first $HW$ most sparse vectors in $\{0,1\}^f$ to map to the states (this is possible in case if $2^f\geq HW$).

    In case if $f=HW$, we have maximal feature sparsity: each feature vector has at most $1$ component. Feature vectors become $1-hot$ vectors coding for the state numbers (except for the $0$ vector). However, the model lacks interpretability because it does not capture the structure of the environment -- it has $2$ independent components $x$ and $y$, and actions affect only the corresponding coordinates.

    One might argue that the feature dimensionality is too high in this case, and this is the cause of the lack of interpretability.

    In case if $HW\approx f+f(f-1)/2$, we use all vectors with $1$ component and all vectors with $2$ components. However, this model is also not interpretable -- it also does not capture the two-dimensional dynamics.

    This shows that feature sparsity in a resonable example is not sufficient for interpretability.

    Now, a coding with two features with $f_1=x$ and $f_2=y$ solves the problem -- each action corresponds to one feature, and features have a simple dynamics.
    However, this coding is not sparse -- in fact, the values of the features are never $0$. This shows that feature sparsity is not necessary for interpretability.
\end{example}

To be fair, sparse features do help interpretability in case if an observation can contain a limited number of objects, one at a time. For example, an observation either contains an enemy, or the health bar. However, these two variables then can be replaced with one and modeled with our framework. We note that our approach also has flaws (see the failure modes below), and the question of defining human "simplicity prior" mathematically in all cases remains open.

In addition, we must clarify the following point. In what follows, we will induce sparsity on the feature vectors, but in a very specific way. First, we consider sparse feature vectors (with some components zeroed out) as an {\em input to the model predicting the same features}, i.e. a dynamics model. The model takes only certain features as the input. Importantly, the sparsity mask is different for predicting different features, and is not reducible to the sparsity of the feature vector as a whole. In fact, for all of the environments that we use, the feature vector is not sparse as a whole.

In contrast, most known interpretable models (natural language inter-sentence dependencies uncovered with attention, causal models used for medical data, physics equations only capturing the neighbours of a particle to describe its dynamic, sparse linear regression) only have few components: they are sparse. In what follows, we will show that some sparse models are not interpretable, and give methods to overcome that.

\subsection{Sparse fan-in models sometimes are sufficient for interpretability, and usually necessary}
We set the proxy for the Kolmogorov complexity for our class of models as the total number of non-zero components in the mask:
$$
\tilde{K}(B)=\sum\limits_{x\in X_{in}}\|B_x\|_0
$$
Note that we only count the complexity of the mask, and do {\em not} count the complexity of the functions $F_x$, or of the decoder $D$. This stems from two reasons. The first reason is purely practical -- there does not exist a measure of complexity for neural networks $F_x$ or $D$ that could be easily optimized for\footnote{One candidate is the norm of the Jacobian as a measure of complexity, however, this norm would be lower for a $\sin(x)$ function than for a linear function $x$ for some arguments, because the derivative is lower. Another option is to use the same technique with disabling features \cite{Oisy2019} to limit the fan-in of the neurons in the network to induce structural sparsity. However, this would result in an even more non-convex and hard-to-optimize problem than this thesis presents. This approach is out of the scope of this thesis.}. The second reason is there by design. Namely,
This issue is somewhat alleviated by the fact that neural network of fixed architecture can only implement functions of bounded Kolmogorov complexity: it only has a finite number of arithmetic operations. This means that it is important to set the class of functions that represent the decoder $D$ and the model $F_x$ properly. Otherwise, the following situation can occur.

\begin{proposition}{(Too complex decoder)}
    Consider an arbitrary deterministic environment $\mu$ and a deterministic policy $\pi$. With a suitable decoder, it is possible to obtain $\tilde{K}(B)=1$ (minimal value) and $L_{rec}=L_{fit}=0$
\end{proposition}
\begin{proof}
    The main idea is to map a multidimensional space into a single-dimensional one. There are several ways to do that

    \begin{itemize}
    \item Consider the space of all possible states $S=\mathbb R^s$ of the true environment.

    It is possible to bijectively map a unit interval $(0,1)$ to a unit square $(0,1)^2$. The mapping is highly non-smooth, like the Peano curve.
    Using this technique, we map $\mathbb R^s$ to $\mathbb R^{s-1}$. Repeating the process, we eventually map $\mathbb R^s$ to $\mathbb R^1$. Now, there is only a single feature describing the state.

    \item For discrete state spaces, an even simpler approach is possible: since the number of states is finite, it is possible to represent each state using its number. This trick can also work for continuous states with discretization.
    \end{itemize}

    Now, $f=1$ (only $1$ feature is required). Given features $f_t$, we can compute $f_{t+1}$ by first reconstructing the observation (here the bijectivity matters), and then applying the Game Engine:
    $$
    f_{t+1}=D(E(GE(E^{-1}(D^{-1}(f_t))),a_t))
    $$

    Here, the model $M_f$ depends on a single feature (because we only have a single feature), the reconstruction is perfect, and prediction is perfect.

    The complexity $K(B)=3$: the only feature is used to predict itself, as well as the attributes $d$ and $r$.

    While the complexity of the binary mask $B$ is small, the complete model is extremely complex: the functions $D$ and $D^{-1}$ compute the non-smooth bijection mapping a unit interval to a unit square.
\end{proof}

While in practice, it would be hard to find such a function with gradient descent, as it is highly non-convex, there is another example which shows that the complexity of $D$ and $M_f$ should not be exceedingly high:

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{diagrams/env_2d_2x2.pdf}
    \caption{2D grid-world with 4 cells: 2 rows and 2 columns. Actions $\pm x$ and $\pm y$ change the values of the coordinates. The single feature $f\in\{1,2,3,4\}$ bijectively maps to the state space. This example extends naturally to $H$ rows and $R$ columns with $x\in[H]$ and $y\in [W]$, and the same set of actions $\pm x$, $\pm y$.}
    \label{fig:env2d2x2}
\end{figure}

\begin{example}
    Consider a grid-world with $2$ components of the player: $x$ (horizontal) and $y$ (vertical) taking discrete values: $x\in [H]$ and $y\in [W]$. The actions $-x$ and $+x$ affect the horizontal coordinate $x$. The actions $-y$ and $+y$ affect the vertical coordinate $y$. We can represent this environment by $2$ features, one for each of the coordinates.

    However, consider the case of $H$ and $W$ being small (for example, $H=W=2$). In this case, we can design a transition model using only a single feature $f_1$ which takes the following values:

    $
    \begin{array}{ccc}
        f_1 & x & y \\
        1 & 1 & 1 \\
        2 & 1 & 2 \\
        3 & 2 & 1 \\
        4 & 2 & 2
    \end{array}
    $

    For example, if $x=1$ and $y=2$, we set $f_1=2$.

    Now, the transition model is shown in Figure \ref{fig:env2d2x2}. It is quite complex: each feature variable can potentially result in $3$ different states! However, a neural network with enough layers could fit this dynamics.

    A much simpler model appears if we map each coordinate to a separate feature: $f_1=x$ and $f_2=y$. In this case, the dynamics for each coordinate is much more simple:
    $f_1^{t+1}=\max(1, \min(2, f_1^t+\mathds 1_{a=+x}-\mathds 1_{a=-x}))$

    Thus, too high model complexity will result in less features used, but the interpretability will be lost.

    Luckily, this pathological behavior disappears as $H, W\to\infty$. While the complexity of the coordinate-wise approach does {\em not} become more complex, and is always described as $f_1^{t+1}=\max(1, \min(H, f_1^t+\mathds 1_{a=+x}-\mathds 1_{a=-x}))$, the single-feature approach's complexity increases. First, we show it for the case of "left-to-right" numbering.
    For the single-feature model, we set $f=W\cdot (x - 1)+y$, and we can reconstruct $y=f\mod W$ and $x=(f-y)/W+1$.
    Now, implementing the $z\mod n$ function for $z\in [kn]$ with a neural network of constant complexity and varying $n$ and $z$ does not seem to be possible (unless using a periodic activation functions \cite{Sitzmann2020})\footnote{For example, a ReLU network is (together) a piecewise-linear function, and the number of "switches" is determined by the number of neurons. Note that the $\mod$ function requires at least as many switches as the number of "wraps" of the argument $z$ w.r.t. the $n$}.

    Therefore, for more states in a dimension ($x$ or $y$), the model would prefer using two features instead of $1$.

    However, this does not prevent at all to learn a linear combination of features: $f_1=\alpha x+(1-\alpha)y$ and $f_2=\beta x+(1-\beta)y$. The dynamics still stays simple.
\end{example}

{\bf "Reusable" decoders and models.} This argument shows a natural setting in which it is not required to count the complexity of the decoder $D$, and of the SCM functions $F_x$. Humans do not re-learn the visual cortex when a new game is presented. Instead, the same architecture is reused \cite{Bengio2012,Veerapaneni2019,Ibrahim2020,Didolkar2021,Gomez2006,Mao2019}, and only the novel high-level behavior is learned. Additionally, humans do not learn the rules of every game from scratch (for example, learning again that there are $x$ and $y$ coordinates in every shooter game). Instead, the existing abstractions (such as $WASD$ keys used to control the player) are re-used. With this in mind, a natural extension of this work is to re-use the decoder and parts of the model (see section \ref{ch:conclusion} for the concrete future direction proposal). If we reuse some parts of the decoder, or of the model, their complexity (in terms of the shortest program describing them) is greatly reduced, when the "bank" of existing abstractions is given, and only an "index" in this "bank" needs to be specified, instead of learning the complete model from scratch.


{\bf Linear models.} A reasonable questions might occur: would it be possible to use the simplest possible class of models to define functions $F$ -- linear models, and "transfer" all of the non-linear complexity into the decoder? This would be extremely beneficial, as environments would be extremely interpretable in this space. The following example shows that even for one of the simplest RL environments, there is no linear feature space that preserves the dynamics and maps bijectively into observations.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{diagrams/env_line_3}
    \caption{Line environment with 3 states. Action $a_1$ corresponds to moving right, and $a_2$ corresponds to moving left. Going left from the left-most state does not change the sate, and same for the right-most state. The variable $x$ increases when going right and takes integer values starting from $1$. Each state with $x=i$ has an associated feature vector $f_i$}
    \label{fig:env_line3}
\end{figure}


\begin{example}
    Consider the class of linear models: $M_f(f_t,a_t)=M_ff_t+M_aa_t+b$, where $M_f$ is a matrix of size $f\times f$, $M_a$ is a matrix of size $f\times a$, and $b$ is the bias vector of size $f$. We are interested in all environments that can be characterized by linear latent dynamics. In other words, what are all the environments, for which there exist a decoder $D$ and a reconstructor $R$, such that there exist a bijection between the set of observations and the set of feature vectors, and the feature dynamics is linear?

    We show that this class is very small, even with a non-linear decoder. Indeed, consider the Line-3 environment (navigating on a line with 3 states) -- see Figure \ref{fig:env_line3}. It does not fit into our definition.

    Indeed, first, the bias term in the linear model can be shifted into the actions $M_a$. Suppose that the decoder gives some features for the states $f_1, f_2, f_3$ (does not matter how, it's only important that the features are fixed). Multiplying $M_aa_i=A_i$ by definition, this gives two vectors $A_1$ and $A_2$.

    Then, $f_1=M_ff_1+A_2$ (going left from $f_1$ gives $f_1$), and $f_3=M_ff_3+A_1$ (going right from $f_3$ gives $f_3$). Now consider what happens in $f_2$. If we go left, we get $f_1$: $M_ff_2+A_2=f_1$. If we go right, we get $f_3$: $M_ff_2+A_1=f_3$. Subtracting these two gives $f_3-f_1=A_1-A_2$. On the other hand, the "stuck at the wall" equations give us $f_3-f_1=M_ff_3+A_1-M_ff_1-A_2=M_f(f_3-f_1)+A_1-A_2$. So, equating these two, we get
    $f_3-f_1=M_f(f_3-f_1)+A_1-A_2=A_1-A_2$. This means that $M_ff_3=M_ff_1$.

    Now, consider the "stuck at the right wall" $f_3=M_ff_3+A_1$, and going right from $f_1$: $f_2=M_ff_1+A_1$. But $M_ff_1=M_ff_3$, which means that $f_2=M_ff_3+A_1$. Note the same RHS as for the "stuck at the right wall". Therefore, $f_2=f_3$, {\em we cannot distinguish between $f_2$ and $f_3$, degenerate representation.}

    This contradiction shows that for this environment, features are either {\em degenerate} (a single feature vector corresponds to two different observations), or the model should be non-linear.

    Note that a simple non-linear model is applicable for this environment:
    $f_1=x$, $f_1^{t+1}=\min(3, \max(1, f_1^t+\mathds 1_{a=a_1}-\mathds 1_{a=a_2}))$.
\end{example}

These examples show that it is important to set the complexity of the model's neural networks correctly. A too simple model would never fit the environment, and a too complex model can give non-interpretable features.

A practical way to choose the measure of complexity is to first fit a model without sparsity constraints, select one with minimal complexity still fitting the data, and then run \sysname.

{\bf Examples.} We discuss how we expect this approach to work in a wide range of environments, and compare it to another interpretability technique where each action is associated \cite{Thomas2018} in a change in a single feature. We call the latter the "single-feature action influence framework".

The single-feature action influence framework fails in case if an action influences two features, and it does not put constraints on features that are not directly affected by agent's actions (such as movements of objects in the environment). Our approach can be seen as an extension of the single-feature action influence framework in a sense that in case if an environment allows for such a parameterization, our approach would find it as well. Indeed, a single action influencing a single feature exactly corresponds to a minimal number of edges from actions to features.

We also note that the Non-Linear Independent Component Analysis \cite{Dinh2015,Hyvarinen2019} would not yield the desired representations in cases where there is significant dependence between features. Indeed, it ignores the influence of actions on the time series, and, therefore, it would not account for the number of edges between action-nodes and feature-nodes.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{diagrams/env_gw}
    \caption{The 2D grid-world environment and the SCM that could be extracted from it. The causal model contains $2$ feature components, one for the horizontal coordinate ($x$) and one for the vertical coordinate $y$. The actions left and right affect $x$, and the actions down and up affect $y$.}
    \label{fig:env_scm_gw}
\end{figure}


\begin{example}{(2D grid-world navigation task, Figure \ref{fig:env2d2x2} and \ref{fig:env_scm_gw})}
    Consider an environment giving 2D images of size $H\times W$ as an observation, and taking 4 actions (left (decrease horizontal coordinate), right (increase horizontal coordinate), up (increase vertical coordinate), down (decrease vertical coordinate)). While the actions correspond to coordinates, the coordinate variables are latent (hidden) from the agent, as it receives observations instead. In the latent space, the dynamics is very simple: each action only influences one coordinate, and the transition equation is simple as well.

    If we apply our approach to this environment, it would uncover this latent representation, guided only by the simplicity of the resulting transition model.
    The model in this space has $6$ edges: each of the two features depends on itself from the previous iteration (2 edges), and each action influencing one feature component (4 edges).

    For this environment, the simpler approach to associate an action with a feature component works as well. However, the loss would not be $0$, as an action does not always influence the state (at the boundaries an action does not do anything). Note that this approach also works for 3D navigation with $3$ latent feature components and $9$ edges in the graph.
\end{example}

\begin{example}{(MountainCar)}
    We consider two possible parameterizations. One uses Cartesian coordinates $x$ and $y$ (and their velocities), and the other uses the linear position of the car on the track (and its velocity).

    The Cartesian coordinates result in an interpretable model with physically realistic equations. However, in this parameterization, an action influences both of the coordinates, as the car moves both in $x$ and $y$ directions.

    The linear coordinate is compliant with the single-feature framework, but the dynamics equation becomes more complex (the gravity depends on the angle of the track).
\end{example}


\begin{example}{(CartPole with images as observations.)}
    Consider the CartPole environment from OpenAI Gym. The latent state has 4 components (the positions and the velocities of the cart and the pole).
    The model in the latent space has $8$ edges: features depend on themselves (4 edges), velocity influencing the coordinate (2 edges) and action influence (2 edges changing the coordinate slightly).
\end{example}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{diagrams/env_breakout}
    \caption{A causal graph for the Atari Breakout environment. The actions left and right move the pallet (red line below on the left image). The ball's (a single red dot at the dark space on the left image) coordinates update via the velocity variables: the horizontal coordinate $x_{t+1}=x_t+v_x \delta t$ and the vertical coordinate $y_{t+1}=y_t+v_y\delta t$. If the pallet hits the ball, the ball is reflected, hence $v_y$ depends on $x$, $y$ and the pallet position: $v_y^{t+1}=-v_y^t$ in case if $x=x_{pallet}$ and $y=0$. The ball can be reflected from the borders of the environment as well, hence $v_x$ depends on $x$ and $y$. If the ball hits one of the $M$ targets (colored items at the top on the left image), the target disappears, and a reward is given: $hit_i=1$ if $x=x_i$ and $y=y_i$. If the ball is below the pallet, the episode terminates, hence $d$ depends on $y$ of the ball.}
    \label{fig:env_breakout}
\end{figure}


\begin{example}{(Atari Breakout, \autoref{fig:env_breakout}])}
    The objects can be grouped into the pallet that is controllable by the player (1 horizontal coordinate), two coordinates for the ball, and 2 coordinates of each of the individual pieces that need to be destroyed.

    Note that while the single-feature influence model disentangles the coordinate of the pallet from the rest of the features, it does not put any constraints on the representation for the ball or for the destroyable pieces: they can be represented in any possible way.
\end{example}

\begin{example}{(MsPacman)}
   The number of lives is a separate feature, the positions of the ghost, of food pallets, of the cherry, of the player are features as well.

   Note that the single-feature action influence approach again would obtain a good representation for the Pacman (because its coordinates are affected by the actions directly), but does not enforce any constraints to obtain a good representation for the ghosts, the food, and the cherry.
\end{example}

\begin{example}{(Montezuma revenge)}
    The representation contains the coordinates of the player and other objects, as well as the current room the player is in.
\end{example}

\begin{example}{(Humanoid)}
    The representation contains the joint position and velocities, as defined in the MuJoCo environment file. The dynamics equation corresponds to the physics equations from the simulator.
\end{example}


\section{Optimization problem for \sysname}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{diagrams/architecture_losses}
    \caption{The diagram of losses for \sysname. The batch of observations $o_t$ (left) is fed into the {\em decoder} $D$, which outputs {\em features} $f_t$. The next features $f_{t+1}$ are predicted using the model $M$ given the actions $a_t$. The model is regularized with a sparsity constraint $L_{sparse}$, and fit using losses $L_{fit,feat}$ in the feature space (predicting the next features) and in the observation space (predicting the next observation). The model also predicts the additional reward (r) and done (d) features. The reconstruction loss is not shown on this figure. See this section for more details on the losses.}
    \label{fig:architecture_losses}
\end{figure}


Overall, we would like to obtain a sparse model without loss of fit quality. Our search space consists of continuous parameters for $D,R,F_x$, and discrete parameters $B_x$ for $x\in X_{in}$ (see above). The figure \ref{fig:architecture_losses} shows the trainables and the losses described in this section.

\subsection{Relative mean-squared error loss}
In order to know how well the model predicts the next state, we use a {\em relative} mean-squared error loss, instead of the standard "absolute" one. Specifically, if the model predicts $\hat{y}_t$, for $t\in[T]$, and the true values are $y_t$, $t\in[T]$, the relative MSE error is defined as:
\begin{equation}
\label{eq:rel_mse}
\rho(y_t, \hat{y}_t|t\in[T])=\frac{1}{T}\sum\limits_{t=1}^T\left(\frac{y_t-\hat{y}_t}{\sigma[y]}\right)^2
\end{equation}
Here, $\sigma[y]$ is defined as standard deviation of $y$ in case if it is non-zero, and it is defined as $1$ otherwise.

\begin{example}
    Suppose that $T=2$, $y_{\cdot}=\varepsilon\cdot (0,1)$ and $\hat{y}_{\cdot}=\varepsilon\cdot (1, 0)$. We compute $\sigma[y]=0.5\varepsilon$, and $\rho(y_t,\hat{y}_t)=4$. This value does not depend on the scale $\varepsilon$ of $y$ and $\hat{y}$. In contrast, the traditional mean-squared error would yield $\rho_{mse}(y_t,\hat{y}_t)=\frac{1}{T}\sum\limits_{t=1}^T\left(y_t-\hat{y}_t\right)^2=0.5\varepsilon^2(1^2+1^2)=\varepsilon^2$, which depends on the scale of the features.

    It is important that the quality metric does not depend on the magnitude of the inputs, as the magnitude of the predicted features can be arbitrary high or low.
\end{example}

\begin{example}
    Suppose that $y\sim Y$ (i.i.d. samples from a distribution $Y$), and $\hat{y}=c$ is a constant predictor. Then, the best value of the relative MSE loss $\rho$ is $1$ with $c=\mathbb EY$. Since neural networks can fit a constant, we expect that $\rho<1$ in our experiments. However, at the beginning of the training, this value can be larger than $1$.
\end{example}

\begin{example}{(Discrete-valued variables)}
    Suppose that $y\sim Uniform([n])$ -- a discrete uniform variable taking values in $\{1,...,n\}$. $\mathbb Ey=n/2$ and $\var y=(n^2-1)/12$. In case if $\rho(y,\hat{y})<\varepsilon<\frac{1}{2n}$, we compute using the Chebyshev's inequality: $p_{bad}=P(y\neq \hat{y})=P(|y-\hat{y}|>1)\leq \mathbb E(y-\hat{y})^2/1^2=O(\rho n^2/12)$. If we want $p_{bad}=1/n$, we obtain a requirement $\rho<12/n^3$. For a discrete variable with $n=20$ to obtain $p_{bad}<1/20$, we will have to have $\rho\approx1.5\times 10^{-3}$.

    However, if we specify $|y-\hat{y}|\sim Uniform([0, a])$, we have $\var |y-\hat{y}|=(a^2)/12$, and the condition $P(|y-\hat{y}|>1)\leq 1-1/a$. Setting $P(|y-\hat{y}|>1)<1/n$ gives $a<n/(n-1)$. Since $\rho=O(a^2/n^2)$, we arrive at a looser condition $\rho< 1/n^2$. For example, for a discrete variable with $n=20$, to obtain $p_{bad}<1/20$, we will have to have $\rho\approx 3\times 10^{-2}$.
\end{example}

\begin{example}
    Suppose that $y=0$ (a constant), and $\hat{y}\sim N(0, \sigma^2)$ -- the normal distribution. In this case, $\sigma[y]=1$ (because the true variance is $0$), and $\rho(y, \hat{y})=\mathbb E_{\hat{y}\sim N(0, \sigma^2)}(0-\hat{y})^2=\sigma^2$.
\end{example}

\begin{example}
    Suppose that $y\sim N(0, \sigma^2)$ (normal distribution), and $\hat{y}=0$ -- a constant. In this case, $\sigma[y]=\sigma$ and $\rho(y, \hat{y})= 1/\sigma^2 \mathbb E_{y\sim N(0, \sigma^2)}(y-0)^2=\sigma^2/\sigma^2=1$.
\end{example}

\subsection{Degenerate decoder}
The decoder needs to be invertible (or, at least, "partially" invertible). Note that, technically, it is possible for the decoder to always output $0$ for all observations. In this case, there exist a simple constant model that "predicts" the next state perfectly (if prediction for $d$ and $r$ is disabled). However, such a model is useless, as it does not describe any aspect of the dynamics. One way to solve this issue is to introduce a reconstructor predicting observations from features. See \autoref{sec:mbrl} for further discussion.

The reconstruction loss is defined in terms of the expected distance between the reconstructed and true observations:
\begin{equation}
\label{eq:reconstruction_loss}
L_{rec}=\mathbb E_{h\sim (\mu\leftrightarrow \pi)}\rho(R(D(o_t)), o_t|t\in [T])
\end{equation}

\subsection{Model fit in observation and feature space}
We also would like the model to predict the future time-steps correctly. This can be achieved in one of two ways.

\begin{enumerate}
    \item Feature-space loss. We penalize the models' features if they deviate from the features predicted by the decoder at the next time-step:
    \begin{equation}
    \label{eq:loss_fit_feature}
    L_{fit,feat}=\mathbb E_{h\sim (\mu\leftrightarrow\pi)|T>1}\rho(M_f(D(o_t), a_t), D(o_{t+1})|t\in [T-1])
    \end{equation}

    Here, we only consider episodes that last more than $1$ time-step, because otherwise there is no "next" time-step to predict.\footnote{We do not model the "initial state" distribution, because it does not affect model simplicity in terms of $K(B)$, and it does not affect the SCM functions $F_x$, and in this project we are only interested in learning the causal graph.}

    \item Observation space loss. We penalize the model if the {\em reconstructed observation} given the predicted features matches the true next observation:

    \begin{equation}
    \label{eq:loss_fit_obs_space}
    L_{fit,obs}=\mathbb E_{h\sim (\mu\leftrightarrow \pi)|T>1}\rho(R(M_f(D(o_t), a_t)), o_{t+1}|t\in [T-1])
    \end{equation}
\end{enumerate}

Next, we compare the two approaches. First, if $L_{fit, feat}=0$, it follows that $L_{fit, obs}=0$. Indeed, the reconstruction loss $L_{rec}$ ensures that the next-step features result in correct observations. And $L_{fit, feat}=0$ ensures that the features are predicted correctly.

In contrast, $L_{fit,obs}=0$ does {\em not} imply that $L_{fit, feat}=0$. Indeed, the model can add the $1$-st feature to all features: $\hat{f}_i=f_i+f_1$ and indicate that by adding a large enough number to another feature: $\hat{f}_s=f_s+10\sigma[f_s]$. The decoder first determines if $10\sigma[f_s]$ was added to $f_s$, and then either subtracts $f_1$ or keeps it. This way, $L_{fit, feat}=0$, but the model does not predict the correct features. This mean that in practice, we need to use $L_{fit,feat}$, and, otherwise, the learned model function does actually predict the next features, and, thus, the causal graph might be incorrect.

\begin{example}{(Incorrect causal graph when only using $L_{fit, obs}$)}
    Consider the Game Engine from Figure \ref{fig:env2d2x2}. The features predicted by the decoder are $x$ and $y$ (not the ones in the figure).
    The model is the following function: $\hat{x}_{t+1}=x_t+10\cdot\mathds 1_{a=+y}-10\cdot\mathds 1_{a=-y}$, $\hat{y}_{t+1}=y_t+10\cdot\mathds 1_{a=+x}-10\cdot\mathds 1_{a=-x}$.
    It is possible to obtain $L_{fit, obs}=0$, because we can reconstruct both the previous state, and the action from $\hat{x}$, $\hat{y}$. Indeed, note that $|x_t|,|y_t|<2$. This means that, if $\hat{x}>10$, an action $+y$ was taken. The rest of the actions are reconstructed in the same way.
    Next, knowing the actions we obtain $x_t$ and $y_t$. Next, the reconstructor performs "the function of the model", computes the (true) next $x_{t+1}$ and $y_{t+1}$, and then performs the reconstruction.
    In case if both of input arguments to the reconstructor are $|x_t|<10$ and $|y_t|<10$, the model does not perform the time-step prediction.

    The causal graph in this case is incorrect: $x$ depends on $+y$ and $-y$, and $y$ depends on $+x$ and $-x$.
\end{example}

Therefore, we have to use $L_{fit,feat}$. However, in practice, even when using the relative loss, $L_{fit, feat}<\varepsilon$ for a small $\varepsilon\in(0,1)$ does {\em not} mean that the prediction quality is good.

\begin{example}
Consider the Game Engine from Figure \ref{fig:env2d2x2}. It has $2$ latent variables: $x$ and $y$. Now, suppose that the features (not on the figure) are $f_1=x$ and $f_2=\alpha y+(1-\alpha)x$ for $\alpha \ll 1$. Suppose that $L_{fit, feat}<\varepsilon$. Does it mean that the model "preserves" the information both about $x$ and $y$? No.

Indeed, for the variable $x$, $L_{fit, feat}<\varepsilon$ implies that $\rho(x,\hat{x})<\varepsilon$, as required. However, for $y$, it does not. Indeed, we can show that it is possible to obtain any $\varepsilon$ bound on th e loss given $\hat{y}=\mathbb y$ (the constant prediction giving $\rho(y,\hat{y})=1$. Assume that $x$ is predicted perfectly. Next, consider

$$\rho(\alpha y+(1-\alpha)x,\alpha \hat{y}+(1-\alpha)\hat{x})=\frac{\mathbb E \left(\alpha(y-\hat{y})+(1-\alpha)(x-\hat{x})\right)^2}{\sigma[\alpha y+(1-\alpha)x]^2}$$.

We assume that $x$ and $y$ are independent under $\pi$ and have the same variance of $c$. Then we compute $\sigma[\alpha y+(1-\alpha)x]^2=\sqrt{\alpha^2+(1-\alpha)^2}c$.

Next, since $\hat{x}=x$, $\rho=1/\sigma^2\alpha^2\mathbb E(y-\hat{y})^2=\frac{\alpha^2}{\alpha^2+(1-\alpha)^2}$. Since this equation has a limit of $0$ when $\alpha\to 0$, and since the decoder is free to set $\alpha$ arbitrarily small, we can make $L_{fit, feat}$ arbitrarily small, while the model only predicts the value of $x$, but not the value of $y$.

Note that, without the relative loss, it is not even required for the model to predict the value of $x$. Indeed, to make the loss arbitrary small, it is sufficient for the decoder to reduce the magnitude of the features, and for the reconstructor to increase the input weights.
\end{example}

Therefore, we have to use both losses, one in the feature space, and one in the observation space.

\subsection{Sparsity loss}
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{diagrams/feature_proba_mask_feature}
    \caption{The model $F_1$ predicts the value of the first feature $f_t^{1}$ at the next time-step, given the current features $f_{t+1}$. The model only uses features determined by the binary mask $B_1\sim Be(P_1)$.}
    \label{fig:featureprobamaskfeature}
\end{figure}


In \autoref{sec:grad_discrete} we outline techniques for parameterizing $B\sim Be(P)$ and then computing the gradient $\partial B/\partial P$, so it becomes possible to find the values of $B$ using gradient descent.

Now we consider the original objective:
$$
K(B)=\sum\limits_{x\in X_{in}}\|B_x\|_0
$$

Since $B$ is sampled from a distribution $P$ (see \autoref{fig:featureprobamaskfeature}), we compute the expected value:
$$
\mathbb E_{B\sim Be(P)}K(B)=\sum\limits_{x\in X_{in}}\|P_x\|_1
$$.

This happens because for each component, $\mathbb E_{b\sim Be(p)}b=p$. Thus, we naturally arrive at the $l_1$-relaxation of our discrete problem \cite{Bengio2013}

We define this loss as:
$$
L_{sparse}=\mathbb E K(B)=\sum\limits_{x\in X_{in}}\|P_x\|_1
$$

\subsection{Ranges of probabilities and exploration}
\label{subsec:proba_ranges}
In case if for some of the probabilities $P_{xx'}=0$, or $P_{xx'}=1$, that feature is {\em never} selected and is never active, or is {\em always} selected, and the model never explores cases where this feature is not enabled.

However, we would like to have the case $P_{xx'}=1$ possible in our system. Indeed the difference in losses $L_{fit}\big|_{B_{xx'}=0}$ and $L_{fit}\big|_{B_{xx'}=1}$ can be drastic, and in the worst case we have $L_{fit}\big|_{B_{xx'}=0}-L_{fit}\big|_{B_{xx'}=1}=f+2$ (without using the important feature, each output feature can only be predicted as a constant, and the same for reward and done). Therefore, if $p_{\max}<1$, we have $L_{fit}=(f+2)(1-p_{\max})$, and the loss will no go lower than this value.

Another issue that $p_{\max}<1$ yields is duplicated features. Indeed, if an important feature is only given with probability $p_{\max}<1$ and $\varepsilon=1-p_{\max}$, it would be beneficial to decrease the loss $L_{fit}$ to duplicate this feature (the decoder uses a spare feature to output the same value). Then, the model would {\em not} receive the value for an important feature with probability $\varepsilon^2$, instead of $\varepsilon$. This decreases the loss $\varepsilon$ times. Therefore, training the model with $p_{\max}<1$ would likely lead to feature duplication.

In contrast, $p=p_{\min}>0$ would not have significant effects on the loss, as adding a non-important feature cannot improve the loss significantly, and definitely cannot worsen the loss (because more features are used). Now, since $p=p_{\min}\approx 0$, we have
$$
L_{fit}=p\underbrace{L_{fit}\big|_{B_{xx'}=1}}_{L_1}+(1-p)\underbrace{\L_{fit}\big|_{B_{xx'}=0}}_{L_0}
$$
Since $L_1\leq L_0$, $L_{fit}\leq pL_0+(1-p)L_0=L_0$, and $L_0\geq L_{fit}$. On the other hand, $L_{fit}\geq (1-p)L_0$, which gives $L_0\leq \frac{L_{fit}}{1-p}= L_{fit}(1+p+O(p^2))\approx L_{fit}(1+p)$.

This means that $L_0\in (1, 1/(1-p))L_{fit}$. For $p=p_{\min}\approx 0$, the gap between the two solutions is small: $L_0\approx L_{fit}$.

It means that the loss with $B_{xx'}$ disabled is upper-bounded by the current loss $L_{fit}$ (where $B_{xx'}$ is only enabled with a small probability). To output the final answer causal graph, we could threshold these probabilities to $0$, and the answer would only worsen a small amount in terms of the fit quality.

\begin{example}
    Let's consider different cases, assuming that the model was fitted to convergence on a set of features with a fixed tensor of probabilities. Consider the case of $2$ input and output features, with features being equal and having a deterministic relationship (from input to output).
    \begin{enumerate}
        \item $p=0$ for all pairs. In this case, no features are given to the model, and the model will learn to predict the mean of the output features. The causal graph is empty. The loss is equal to $2$ (since standard deviation for each feature is $1$ w.l.o.g. because of the relative error).
        \item $p=1$ for all pairs. In this case, the model uses all features, and the error is minimal. The loss is $0$.
        \item $p=0.5$ for all pairs. In this case, the model will try to extract as much information as possible when the feature is present. Essentially, with probability $0.25$, the model will give the mean output feature, and with probability $0.75$ it will use either one of the input features, or both of them. The loss is equal to $2\times 0.25=0.5$.
        \item In the general case $p\in [0, 1]$, the model has an error of $2$ with probability $(1-p)^2$ and a loss of $0$ with probability $1-(1-p)^2$. The expectation is $2(1-p)^2$.
    \end{enumerate}

    Now consider a similar case when the mutual information between features is $0$ (they are independent). In this case, for a probability $p$, we have the loss being $2$ with probability $(1-p)^2$, a loss of $1$ with probability $2p(1-p)$ and a loss of $0$ with probability $p^2$. The expectation is $2(1-p)^2+2p(1-p)=2-4p+2p=2-2p=2(1-p)$. This loss is higher than for the case of equal features for all $p\in (0,1)$.

    To avoid a case when there are no samples with $\xi=1$ (with low $p$), we limit the range of $p$ to be $p\geq p_{\min}=0.01=1\%$. Setting this value higher results in feature duplicates, as the model tries to predict the outputs even from partially present features. A value of $1\%$ with a batch size of $T=5000$ gives $50$ samples with $\xi=1$ in the worse case (on average), which is empirically sufficient to turn features on. To turn the features off, we introduce a sparsity regularizer.
\end{example}

\subsection{Non-stationarity and adding the mask to the model}
Consider the data that the functions $F_x$ are fit on, when the probabilities $P$ change:
$$
fa_t\odot B_x\to x_{t+1}
$$

Since $B_x$ depends on $P_x$, the dataset clearly depends on $P$. Since the probabilities change during the training (a feature could be turned on and off repeatedly in search of the best graph), the problem becomes highly non-stationary -- effectively, the model has to re-relearn to rely on the new set of features that it did not see before. In addition, in case of discrete features, the model might not be able to distinguish between the case where $fa_t=0$ or $B_x=0$.
It may happen that in case if $f_1=0$ is given, the model should rely on $f_1=0$. However, if a feature is not given, the model should rely on another, more noisy feature.
However, an input of $0$ does not allow to distinguish between these cases, and the model would be forced to "guess" (output the expected value between the two datasets).
A solution for this is to feed the mask to the model as well\footnote{A similar approach can be found in \cite{Mooij2016,DeHaan2019}, but the ablation study was not done for whether or not it improves the setup}:
$$
\hat{x}=F_x(fa_t\odot B_x,B_x)
$$

\subsection{Separate exploration loss}
To speed up the convergence even more for the cases $p\approx 0$ and $p\approx 1$, we temporarily threshold the probabilities to be in the range $[p^{expl}_{\min},p^{expl}_{\max}]$, and fit the models with masks $B$ sampled from this distribution. This results in the loss
\begin{equation}
L_{fit}\big|_{p:=p^{expl}}=\mathbb E_{B\sim Be(p^{expl})} L_{fit}
\end{equation}

\subsection{Additional feature transformation ("rotation")}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{diagrams/architecture_rotation}
    \caption{Two additional linear feature transformations (purple) applied before and after the model $M$ in order to speed up the convergence. The reconstruction loss path (dotted) bypasses the additional transformations. See \autoref{fig:architecture_losses} for the full description of the components}
    \label{fig:architecture_rotation}
\end{figure}


During training, the distribution for the decoder is non-stationary, because the feature space changes to optimize for sparsity. This happens because the model affects the decoder, and the reconstructor must match the decoder. In order to alleviate this issue, two separate linear transforms are added right before and after the model (see Figure \ref{fig:architecture_rotation}). These transforms allow to quickly enable or disable features, or to change the linear combination of them:
$$
\begin{array}{clll}
L_{fit, obs}&=\rho(R(Z_{post}(M_f(Z_{pre}(D(o_t)),a_t))), o_{t+1});& R\circ Z_{post}\circ M_f \circ Z_{pre}\circ D&\mapsto o_{t+1},\\
L_{fit, feat}&=\rho(Z_{post}(M_f(Z_{pre}(D(o_t)),a_t)), D(o_{t+1}));&Z_{post}\circ M_f\circ Z_{pre}\circ D&\mapsto Do_{t+1},\\
L_{fit, m-feat}&=\rho(M_f(Z_{pre}(D(o_t)),a_t), Z_{pre}(D(o_{t+1})));&M_f\circ Z_{pre}\circ D&\mapsto Z_{pre}\circ Do_{t+1}.\\
\end{array}
$$

\subsection{NP-hardness in the general case}
As in the linear case of the semester project, we prove the NP-hardness of finding the most sparse causal graph that fits the data. While it was shown \cite{Claassen2013} that finding the causal graph without a decoder is not NP-hard, it is if we consider a learned representation as well.

However, even in the case of a linear model, linear decoder and a linear reconstructor, the problem becomes NP-hard. Indeed, consider the linear dynamics $f_{t+1}=M_ff_t+M_aa_t$, where $M_f$ and $M_a$ are matrices of sizes $f\times f$ and $f\times a$. The decoder is also linear: $f_t=Do_t$ where $D$ is a matrix of size $f\times o$. The reconstructor $R$ s.t. $o_t=Rf_t$ is linear as well and has size $o\times f$. In the case where $f=o$ and $D$ is invertible, we can set $R=D^{-1}$. Now, the model for observations is linear as well: $o_{t+1}=D^{-1}\left(M_fDo_t+M_aa_t\right)$. Since the dynamics of the environment is fixed, the matrices $M_o=D^{-1}M_fD$ and $M_a^{o}=D^{-1}M_a$ are determined by the environment. Therefore, our task of finding a sparse model in this case boils down to finding a decoder matrix $D$ giving the most sparse feature transition matrix $DM_oD^{-1}$ and the action transition matrix $DM_a^{o}$:
$$
\|DM_oD^{-1}\|_0+\|DM_a^{o}\|_0\to\min\limits_{\exists D^{-1}}
$$

Now, if $M_o=0$ (the environment does not have a state), the problem becomes Sparse Dictionary Learning \cite{Vincent2002}, a known NP-hard problem.

Another extreme $M_a^o=0$ requires to find a basis given by $D$ in which a matrix $DM_oD^{-1}$ is most sparse. If we allow for complex variables, this problem is polynomial-time computable, as we only need to diagonalize the matrix $M_o$.

Note that the case of one action corresponding exactly to one feature \cite{Thomas2018,Corneil2018} means that $DM_a^o$ has only a single element in each column. Since $D$ transforms the columns of $M_a^o$, in this linear example, it would mean that the columns of $M_a^o$ are collinear, and we can find actions mapping to the same feature by finding sets of collinear vectors in polynomial time.
%If we know the target matrix $M_a$ (for example, in a grid-world with actions (left, right, up, down) and two state components, it would be $\left(\begin{array}{cccc}1&-1&0&0\\0&0&1&-1\end{array}\right)$). Then, $D$ can be found via the pseudo-inverse.

\subsection{Optimization schemes}
Now, we have a number of losses (see Figure \ref{fig:architecture_losses} for a diagram of all components) that we would like to minimize together

\begin{equation}
\label{eq:all_losses}
\begin{array}{lc}
L_{sparse}&\to\min\limits_P\\
L_{rec}&\to\min\limits_{D,R}\\
L_{fit, feat}&\to\min\limits_{D,M,P,R}\\
L_{fit, obs}&\to\min\limits_{D,M,P,R}
\end{array}
\end{equation}

Ideally, we would like to find a point which jointly minimizes all of these losses. In the case of a deterministic environment, it is possible to achieve all of the losses being $0$, $L_{sparse}$ having the minimal possible discrete value.

The traditional practical reformulation of this problem in terms of a Machine Learning problem would result in a loss consisting of two components: one to fit the parameters of the posterior to the data, and one to regularize the model for sparsity, with a coefficient $\lambda$ controlling how "strong" is the posterior \cite{Seeger2007}.
Such an approach, however, requires to "guess" the parameter $\lambda$ \cite{Luneau2020,Wang2020}.
A much more convenient way is to either guess the target number of edges in the graph $K(B)$, or the losses constraint $L_i\leq c_i$ for $i\in\{fitobs, fitfeat, rec\}$.
We present the approaches below:


\begin{itemize}
    \item Linear combination of losses with constant coefficients:

    \begin{equation}
    \label{eq:opt_scheme_linear_comb}
    \mathcal L=\lambda_{sparse}\cdot L_{sparse}+\lambda_{fit,feat}\cdot L_{fit, feat}+\lambda_{fit,obs}\cdot L_{fit, obs}+\lambda_{rec}\cdot L_{rec}\to\min\limits_{D,M,R,P}
    \end{equation}

    \item Constrained problem with sparsity guess:
    \begin{equation}
    \label{eq:opt_scheme_sparsity_guess}
    \begin{array}{ll}
    \min\limits_{D,M,R,P}& \lambda_{fit,feat}\cdot L_{fit, feat}+\lambda_{fit,obs}\cdot L_{fit, obs}+\lambda_{rec}\cdot L_{rec}\\
    s.t.& L_{sparse}\leq c_{sparse}
    \end{array}
    \end{equation}

    \item Constrained problem with fit guess and a linear combination:
    \begin{equation}
    \label{eq:opt_scheme_fit_guess_lin_comb}
    \begin{array}{ll}
    \min\limits_{D,M,R,P}& L_{sparse}\\
    s.t.&\lambda_{fit,feat}\cdot L_{fit, feat}+\lambda_{fit,obs}\cdot L_{fit, obs}+\lambda_{rec}\cdot L_{rec} \leq c_{fit, comb}
    \end{array}
    \end{equation}

    \item Constrained problem with granular fit guess:
    \begin{equation}
    \label{eq:opt_scheme_fit_guess_granular}
    \begin{array}{ll}
    \min\limits_{D,M,R,P}& L_{sparse}\\
    s.t.&L_{fit, feat}\leq c_{fit, feat}\\
    s.t.&L_{fit, obs}\leq c_{fit, obs}\\
    s.t.&L_{rec}\leq c_{rec}
    \end{array}
    \end{equation}

    \item If the target sparsity is known, a projection \cite{DasGupta} to the $l_1$ simplex can be applied, possibly via the proximal gradient descent method \cite{Deleu2015}.

    \item Adaptive sparsity \cite{Dieleman2021} with a multiplicative update for the $\lambda$ parameter given the reconstruction losses. Given a target constraint for the linear combination of all losses but the sparsity loss $L_{\setminus sparse}\leq\varepsilon$, we update the parameter $\lambda_{sparse}$ in \autoref{eq:opt_scheme_linear_comb}. If $L_{\setminus sparse}\leq\varepsilon$, we increase $\lambda_{sparse}:=\lambda_{sparse}\cdot (1+\delta)$, and otherwise decrease it $\lambda_{sparse}:=\lambda_{sparse}\cdot (1-\delta)$. We note that this approach is ideologically similar to the Lagrangian update, but does not consider the magnitude of the reconstruction loss compared to the threshold.
\end{itemize}

We compare the versions \ref{eq:opt_scheme_linear_comb}, \ref{eq:opt_scheme_sparsity_guess}, \ref{eq:opt_scheme_fit_guess_lin_comb}, \ref{eq:opt_scheme_fit_guess_granular} in \autoref{ch:evaluation}.

In terms of the ease of use, \autoref{eq:opt_scheme_fit_guess_granular} requires least knowledge about the problem. Indeed, we only set the relative thresholds for the losses (values less than $1$), and obtain the most sparse causal graph allowing for such a loss.

{\bf Solving constrained problems.} To solve the constrained problems with gradient descent we use the Lagrange primal-dual method \cite{Adegbege2021}. We consider the Lagrange function for the \autoref{eq:opt_scheme_fit_guess_granular}:
$$
L_{agrange}=L_{sparse}+\lambda_{fit, feat}(L_{fit, feat}-c_{fit, feat})+\lambda_{fit, obs}(L_{fit, obs}-c_{fit, obs})+\lambda_{rec}(L_{rec}-c_{rec})
$$

We note that solving \autoref{eq:opt_scheme_fit_guess_granular} corresponds to solving
\begin{equation}
\label{eq:opt_lagrange}
\min\limits_{D,M,R,P}\max\limits_{\{\lambda_i\geq 0\}}L_{agrange},\,i\in\{fitfeat, fitobs, rec\}
\end{equation}

Indeed, an an optimal point of \autoref{eq:opt_scheme_fit_guess_granular} is a saddle point of \autoref{eq:opt_lagrange} \cite{Franceschi2019}. Roughly speaking, this happens because if a constraint is violated, any $\lambda>0$ results in an infinite value of the loss.

To solve this problem, a primal-dual descent-ascent method can be applied \cite{Franceschi2019} (here $x=(D,M,R,P)$ -- the parameters of all neural networks):

\begin{equation}
\label{eq:primaldual}
\begin{array}{rl}
x_{t+1}&=x-\eta\frac{\partial L_{agrange}}{\partial x}\\
\lambda_i&=\lambda_i+\eta\frac{\partial L_{agrange}}{\partial \lambda_i},\,i\in\{fitfeat,fitobs,rec\}
\end{array}
\end{equation}

For convex functions, this algorithm is known to converge to a saddle point of \autoref{eq:opt_lagrange}. The algorithm has the following intuitive interpretation. Suppose that $\lambda_i$ is small, and $i'th$ constraint is violated: $L_i>c_i$. Since $\frac{\partial L_{agrange}}{\partial \lambda_i}=L_i-c_i>0$, the parameter $\lambda_i$ would increase. If, in contrast, the constraint has a slack (is not violated) and $L_i<c_i$, the parameter $\lambda_i$ would decrease to allow optimizing for the objective $L_{sparse}$.
Therefore, we expect the following dynamics: first, the algorithm would turn most of the features on in order to satisfy the constraints, and then it would optimize for sparsity.

If we apply this algorithm for the other (symmetric) formulation of our problem (\autoref{eq:opt_scheme_sparsity_guess}), it would first disable features, and then, once the target sparsity level was reached, it would care about fitting the reconstructor and the SCM functions.

%{\bf Sparsity has to be lower than required sometimes.}



%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
\label{ch:implementation}
%%%%%%%%%%%%%%%%%%%%%%%%
In this chapter we discuss the specific of our implementation of \sysname. We touch such issues as the initialization of the probability metrics, tips and know-how during training, and the project structure.

\subsection{Initialization}
We initialize the probability matrix to be an identity for the features (a feature only affects itself), and to a full matrix for the rest (actions, reward and done). We this method to initializing with all zeros, all ones, or a random initialization in the next section

\subsection{Metrics}
In order to assess the quality of the solution, we track the following quantities:
\begin{itemize}
    \item Sparsity gap. We compute $L_{fit}$ with the current mask $B\sim Be(P)$, as well as with a fully-enabled mask (or the complete causal graph) $B_{xx'}=1$. This gives losses $L_p=L_{fit}^{p}$ and $L_{full}=L_{fit}^{p:=1}$.

    Now, we define $\Delta^{add}_{sparse}=L_p-L_{full}$. This quantity is positive if the model is fit well (adding features should not worsen the prediction quality). The value of this metric shows how well feature selection has worked: if the value is high, it means that some of the required for prediction features are currently disabled (or, that the model is not fit enough).

    A multiplicative version $\Delta^{mult}_{sparse}=\frac{L_p-L_{full}}{L_p}$. This value is positive when $\Delta^{add}$ is positive, and this value is less than $1$, because $L_{full}>0$

    \item Number of edges in the graph, or the number of large components in the matrix $P$. We compute a k-means clustering\footnote{A simple thresholding with $p=0.5$ would suffice to obtain the final graph, however, the benefit of K-means is that it works even when the training is not fully complete, and the probabilities are not fully $p_{\min}$ or $1$. This allows the see manually the preliminary results while training and quickly understand if the trial is likely to be successful.} in 1-dimensional space of all probabilities $\{P_{xx'}\}_{x\in X_{in},x'\in X_{out}}$. This gives a threshold $\tau\in[0,1]$. Next, we select the pairs $(x, x')$ such that $P_{xx'}>\tau$. This gives the answer causal graph, and the number of non-zero components.

    \item Total entropy of distributions $Be(P_{xx'})$. This quantity should be low at the end of training, because all features should be either turned on or off.
\end{itemize}

\subsection{Separate networks to predict pixels and models}
Since we would like our features to be disentangled (corresponding to different things), there is (by definition) no benefit of multi-task learning. In our experiments (see \autoref{ch:evaluation}) we show that multi-task learning harms convergence speed in case if the tasks are not related. We explain it by the fact that a shared model has layers that are used by all tasks. If the tasks are not related, it is hard for gradient descent to "split" the neurons between tasks, as all of them "want" to use the neurons at a layer. The same applies to the reconstructor (predicting weakly correlated pixels in a toy grid world), and the decoder (predicting features that are disentangled)\footnote{In case of a convolutional decoder, this part is shared, but the subsequent fully-connected layers are not. Decoding the image is a shared task, but decoding features from that representation is not}. This approach can be found in \cite{Ke2019}

In order to implement the separate networks we use the {\tt einsum} operator in pytorch to implement "vectorized" fully-connected networks.

\subsection{Project code structure}
The code can be found in our \href{Github repository}{https://github.com/sergeivolodin/causality-disentanglement-rl}. The code uses gin config files for modular configuration, ray tune for hyperparameter tuning, Tensorboard for tracking metrics real-time, and Sacred to save all the data from all experiments.

\subsection{Data collection and the true distribution}
We run $k$ parallel data collection processes with ray, and aggregate the data in one ring buffer process. Each minibatch is sampled as random steps from all of the collected episodes and given to the learner class. The ring buffer process maintains the ratio of collected steps to sampled steps to be at least a constant fraction $\zeta=0.5$ to prevent overfitting to only collected steps.

We note that this does not result in a true distribution $\mathbb E_{h\sim (\mu\leftrightarrow\pi)}\mathbb E_{t\sim Uniform([T])}$. The true distribution requires us to sample one episode, select a random step, and then discard the rest of the episode. Effectively, it would require to set $\zeta=\mathbb E_{h}T$: we sample $T$ times more steps than "required". This would slow down the convergence significantly. In a minibatch from the true distribution, all the steps are likely to be from different episodes. This is important for procedurally-generated environments which have a different layout for each new episode.

\subsection{Number of optimizers}
Consider the loss from \autoref{eq:opt_scheme_linear_comb}. It depends on multiple groups of variables: the decoder $D$, the model $M$, the reconstructor $R$, and the matrix of probabilities $P$. We test two options for optimizing $L$:
\begin{itemize}
    \item One optimizer, updating each group of variables at every iteration.
    \item $4$ optimizers, each updating one of $\{D,M,R,P\}$, and doing multiple steps at a time. This corresponds to finding the optimum a bit more precisely for one group of parameters. The rationale is the following. Since for a single group of parameters (such as $M$), the problem looks more like supervised learning (the model receives a stationary distribution of inputs, and has to fit a stationary distribution of outputs), the problem becomes more like what neural networks are well-tested at -- supervised learning, albeit only for a few iterations.
\end{itemize}

\subsection{Time-scale separation for Lagrange multipliers}
Sometimes the order of magnitude of $\lambda_i$ in \autoref{eq:opt_scheme_fit_guess_granular} or \autoref{eq:opt_scheme_fit_guess_lin_comb} needs to be changed significantly. For example, the value of the sparsity has recently decreased significantly, which leads to a much bigger emphasis on the constraints. We need to decrease the Lagrange multiplier, or, otherwise, too little emphasis is put on decreasing sparsity even more. With standard parameterization, $\lambda_i$ would decrease exponentially like in standard Gradient Descent.

If we reparameterize $\lambda_i=\sqrt{\mu_i}$, or $\lambda_i=\mu_i^2$ or $\lambda_i=\exp(\mu_i)$, the parameter $\lambda_i$ would change slower (in the first two cases) or faster (in the last two cases). Such a difference cannot be achieved by simply changing the learning rate for $\lambda_i$, as such modifications would only change the "slope of the line", while reparametrization changes the asymptotic behavior (for example, a line vs. a quadratic function).

A result shows that a sufficient time-scale separation is crucial \cite{Fiez2020}.
 \cite{Rajeswaran2020} considers a similar bi-level problems as a Stackelberg game (where the follower adapts to the leader). The takeaway is to update the follower faster. We achieve this by doing more optimization steps for the decoder and the reconstructor, and less for the model.

Given a single batch of data at epoch $i$, we try the following optimization schemes:
\begin{enumerate}
    \item Updating all parameters at every epoch $i$
    \item Updating the model for $\tau_m$ epochs, then the decoder for $\tau_d$ epochs, then the reconstructor for $\tau_r$ epochs.
\end{enumerate}

Empirically, the last approach works better.


%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}
\label{ch:evaluation}
%%%%%%%%%%%%%%%%%%%%
In this section, we present the results of running \sysname on a number of benchmark environment that we develop, as well as on established baselines.
We denote the resulting graph given by the algorithm as $\hat{G}_{x'x}$ for whether there is an edge from a cause variable (one at the current time-step) $x\in X_{in}=\{f_1,...,f_f,a_1,...,a_a\}$ to an effect variable (one at the next time-step) $x'\in X_{out}=\{f_1,...,f_f,r,d\}$ (see \autoref{ch:design}). The true causal graph for an environment (if exists) is denoted as $G_{x'x}$. The matrices $G_{x'x}$ and $\hat{G}_{x'x}$ have binary values $G_{x'x}\in\{0,1\}$. $1$ means that the edge is present, and $0$ means that the edge is absent. The total number of edges is $K(G)=\sum\limits_{x\in X_{in}}\sum\limits_{x'\in X_{out}}G_{x'x}$. To obtain $\hat{G}$ from $P$, a thresholding algorithm (K-Means) is applied (see \autoref{ch:implementation} and \autoref{ch:design}).

\section{Environments}
We describe the environments that we develop for this project, as well as established benchmarks. Each environment tests a particular aspect of learning causal graphs. The SparseMatrix environment considers a case of correlated features in which each action can affect multiple features. VectorIncrement has a simple dynamics, but the observations are encoded in pixel space. KeyChest environment is a grid-world with variable complexity with both frequently changing and hard-to-change features.

\subsection{SparseMatrix$(n,k)$ environment}
The simplest environment we consider is SparseMatrix(n,k). Given the dimensionality of the space, we randomly sample a matrix $A\in \mathbb R^{n\times n}$ at the initialization of the environment (once per training session, i.e. the matrix is the same after a {\tt reset()}).

The matrix $A$ is sampled to have $k\geq n$ non-zero component and that it is non-degenerate. We first permute the numbers $[n]$, and then set $A_{ij}:=\pm 1$.
After that, elements $k-n$ are added to random "free" (zero) spots $A_{ij}:=\pm 1$.

At each episode, the state of the environment is initialized as $s_1^i\sim N(0,1)$ for $i\in[n]$. At each time-step, the state is multiplied by the transition matrix:
$$
s_{t+1}=As_t
$$

The actions are ignored for this environment, and the reward is always $0$. An encoder is not applied, i.e. the environment's observation is the state.

The true causal graph (without a decoder) for this environment always equals to non-zero components of the transition matrix: $G^{true}_{f_if_j}=\mathds 1_{A_{ij}\neq 0}$.

Additionally, we add a sparse random matrix influencing actions. To make the effect of actions non-linear, we only apply actions to positive state components:
$$
\tilde{s}_{t+1}=As_t,\, s^i_{t+1}=\mathds 1_{\tilde{s}_{t+1}>0}\cdot \sum\limits_j B_{ij}a_t^j
$$

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{diagrams/env_true_sm3}
    \caption{SparseMatrix$(3, 4)$ environment for the given $A$}
    \label{fig:envtruesm3}
\end{figure}

\begin{example}
    If we take $n=3$ and the following matrix:

    $$
    A=\left(
    \begin{array}{ccc}
    0&-1&0\\
    1&0&-1\\
    1&0&0
    \end{array}
    \right)
    $$

    we obtain the causal graph shown in \autoref{fig:envtruesm3}.
\end{example}

\begin{example}
    We test our approach on the following matrices for $n=5$:

    $$
    A=\left(
    \begin{array}{ccccc}
    0 & 0 & 0 & -1 & 0\\
    0 & 0 & -1 & 0 & 0\\
    -1 & 0 & -1 & 0 & 0\\
    0 &1 & 0 & 1 & 0\\
    0 & 0 & 0 & 0 & -1
    \end{array}
    \right),\,
    B=\left(
    \begin{array}{ccccc}
    0 & 0 & 0 & 0 & -1\\
    0 & 0 & 0 & 1 & -1\\
    -1 & -1 & 0 & 0 & 0\\
    1 & 0 & 0 & 0 & 0\\
    0 & 0 & 1 & 0 & 0\\
    \end{array}
    \right)
    $$

\end{example}

\subsection{VectorIncrement$(n)$ environment}
The state has $n$ components, and there are $n$ actions. At the beginning, the state is a zero vector $s_1=0\in\mathbb R^n$. Executing action $i$ results in an increment in the $i$'th component of the state vector:
$$
s^j_{t+1}=\begin{cases}
s^j_t,&a\neq j\\
s^j_t+1,&a=j
\end{cases}
$$

The reward is given when the lowest component is incremented. A somewhat optimal strategy is to stay close to the diagonal of the environment:
$$
a_t=\arg\min \{s_t^i\big| i\in[n]\}
$$

If no encoder is applied, the ground-truth graph (without an encoder or a decoder) is given by $G_{f_if_j}=\mathds 1_{i=j}$, $G_{f_ia_j}=\mathds 1_{i=j}$, $G_{d,f_i}=1$, $G_{d,a_j}=0$, $G_{r,f_i}=1$, $G(r,a_i)=1$ for all but one action (we can always determine the action taken by all but one one-hot variables, since at every step exactly one action is taken). See Figure \ref{fig:ve5nodec}.

The environment additionally supports rendering observations as images using a low-dimensional digits font, as well as optionally permuting the pixels of the observation to make it hard for humans to play.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{diagrams/ve5_nodec}
    \caption{The causal graph for VectorIncrement$(5)$ without a decoder or an encoder. First, each state component determines its own value at the next time-step (black self-loops for $s_i$). Secondly, a corresponding action increments the state component (red line from $a_i$ to $s_i$). Finally, all state components affect reward and done (green lines from $s_i$ to reward and done), and all but one action affect the reward (red lines from $a_i$ to reward). This one of 5 possible causal graphs for this environment (one of the actions can be disconnected from the reward node). The number of edges is equal to $K(G)=24$ (out of $120$), or $K(G)=5n-1$ out of $2n^2+4n$ in the general case of VectorIncrement$(n)$}
    \label{fig:ve5nodec}
\end{figure}


\subsection{KeyChest$(h,w,f,c,k)$ environment}
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{diagrams/env_true_kc}
    \caption{The causal graph for the KeyChest environment. The actions influence the coordinates. At each time-step, the health variable decreases, and if it is $0$, the episode ends. Collected food (player coordinates equal to the food coordinates) increses health, and food disappears. Keys can be collected as well. A chest can only be collected in case if there are keys already collected, and in that case the number of collected keys decreases. A lamp is toggled in case if the player is at the button position.}
    \label{fig:envtruekc}
\end{figure}

The environment is described fully in my previous publication \cite{Volodin2020} at ICLR CLDM 2020. The causal graph is shown in \autoref{fig:envtruekc}

\section{True and learned causal graphs}
\todo{Add graphs}
\subsection{VectorIncrement$(5)$ environment}
\subsection{VectorIncrement$(2)$ environment}
\subsection{KeyChest$(5, 5)$ environment}
\subsection{SparseMatrix$(5, 7)$ environment}

\section{Ablation study for VectorIncrement$(5)$}

\section{Failure mode analysis}

toy experiments

resulting graphs

ablation study

failure modes



%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}
\label{ch:related_work}
%%%%%%%%%%%%%%%%%%%%%%

{\bf Symbolic approaches.}
\cite{Ave, DesJardins1993,Evans2018} considers a symbolic reasoning approach. \cite{Jonsson2006} considers grouping states into "meta states". \cite{Corneil2018} defines "prototype" states.
\cite{Yarats2016} cluster the environment's state space.
\cite{Fortuin2018} clusters time series and fits a Markov model in this space.

The main difference between symbolic approaches and causal graphs is that while the structure of the graph is discrete, the transition functions that determine the nodes' values can be arbitrary neural networks. Our work can be seen as complementary to the case when the transition dynamics is more complex.

{\bf Other approaches to RL or time series interpretability.}
\cite{Thomas2018} consider a model where one action corresponds to a change in one feature. Our work generalize this approach (see \autoref{ch:design}).

\cite{Everitt2019,Martic2020,Madumal2019} use the tools of causal reasoning to interpret agents. Our work can be seen as complementary, as \cite{Everitt2019,Martic2020,Madumal2019} use an existing causal graph of the environment to provide explanation. In contrast, our work focuses on learning the graph.

A number of approaches involve sparse feature vectors for interpretability \cite{Weinstein,Zhu2017,VandenOord2017,Cha2018,Deng,Liu2018,Javed2019,Rafati2019,Pei2019,Lillicrap2019,Petros2020,Fallah2020,Javed2020}. See \autoref{ch:design} for discussion.

\cite{zambaldi2018relational} introduce an attention module inside a Reinforcement Learning agent (inside the policy and the value networks), allowing it to solve tasks not solved by a simple MLP, because the task requires relational reasoning. A similar technique can be seen in \cite{hahne2019attention} to solve visual reasoning tasks (similar to IQ tests).
Using an attention mechanism could be the next step for our project (see \autoref{ch:conclusion}).


{\bf Natural language for explanations.}
\cite{Madumal2019,Ehsan2018} generate natural language explanations given a causal model of the environment. Our work can be seen as complementary to \cite{Madumal2019}: we learn the causal graph that can be used as input for that method.

%Sparse Dictionary Learning \cite{Vincent2002, Kreutz-delgado2003, Mairal2009, Baze2012}

{\bf Causal modeling.}
\cite{Barnett2015} apply a {\em linear} decoder and learn a sparse linear model in the context of Granger causality. In our project, we focus on the non-linear case.

A number of approaches learn the sparse graph, but without a learned representation (without a decoder network) \cite{Kalainathan2018,Fu2013,Ke2019,Chalupka2017,Javed2020,Battaglia2016}. In this thesis, we focus on learning the representation as well.

\cite{Kalainathan2018, Ng2019} propose to learn a distribution over causal graph in a mean-field way (all edges are independent and modelled as Bernoulli distribution). Our project uses this technique (see \ref{ch:background}).

\cite{brouillard2020differentiable} extend this approach to modelling interventions, and this line of work can be used to design better exploration techniques, when combined with learning sparse graphs (see \autoref{ch:conclusion}).

\cite{Javed} learn causal graphs in the context of RL. Our work can be seen as an extension of this project to a learned decoder, and the cases where sparsity is important.

\cite{Nair2019} implicitly code for the edges of a causal graph in terms of a neural network, i.e. the network predicts which edges to add or remove. In contrast, we use a matrix of probabilities which determines the node adjacency matrix. Using the approach of \cite{Nair2019} could yield potential improvements in our framework.

%\cite{genewein2020algorithms} propose to use probability trees instead of graphical models to encode causal relationships. The benefit is that the set of parents of a node can be dynamically dependent on values of other nodes.

\cite{Johnson2016,Tang2019,Kurutach2021,Chalupka2015} learn a causal model jointly with a decoder, but do not evaluate the sparsity. In our project, we focus on the sparsity as well as on jointly learning the model with a decoder.

{\bf Graph neural networks.}
 \cite{Battaglia2016} introduces Graph Neural Networks \cite{Zhou2018} (GNNs) based on the idea of thoughts as graphs of connections between abstract concepts. The idea is to combine the strengths of "handcoded" or "biased" models and the "end-to-end" or "from scratch" models. \cite{Velickovic2020} extend this idea by allowing a linear (in the number of nodes) number the edges to be learned.

 \cite{Cranmer2020} use GNNs to discover unknown laws of dynamics. They apply symbolic regression to the node and edge models of the GNN, to extract the symbolic expression. The model is used for pairwise forces and for discovering the laws for Dark Matter. The symbolic expression (Eureqa) generalizes better than the non-symbolic model it was extracted from. $l_1$ sparsity is used in the GNN to ease the problem for the symbolic regression.

One important difference between GNNs and Structural Causal Models (SCMs) used in this project is that the graph is usually fixed for GNNs. In contrast, to learn the SCM is to learns the graph of dependencies.

% \cite{Johnson2016} introduce a variational approach to model dynamics: probabilistic graphical model in terms of Switching Linear dynamics is applied to a learned embedding using a VAE. The model is applied to 1D and 2D image tasks, and it successfully uncovers the underlying dynamics.

\cite{VanDenOord2018} uses contrastive losses to obtain a representation and a model in the latent space.
\cite{Kipf2020} work on learning abstract representations from pixels in 3D environments and in Atari using Graph neural networks, leading to more interpretable world (environment) models. Using the contrastive loss in our approach could yield faster convergence of the world model, and using sparsity constraints along with the approach of \cite{Kipf2020} could lead to better interpretability.

 % a neural autoregressive model. The paper suggests that fitting many-step future predictions is of utter importance, since only in such predictions the model is tested for "true" generalization of high-level features. The paper also suggests that MSE or other losses commonly used for reconstruction are not optimal, and generative models are hard to train. As an alternative, the paper suggests optimizing for the mutual information between the latent code and the original inputs. The model predicts the log-odds ratio for an offset $k$ and input up to time $t$ as $f_k(x_{t+k},c_t)\sim \frac{p(x_{t+k}|c_t)}{p(x_{t+k})}$ with a model $f_k(x_{t+k}, c_t)=e^{D(x_{t+k})^TW_kc_t}$. This model is trained using the InfoNCE loss (noise-contrastive estimation) $L=-\mathbb E_X\log\frac{f_k(x_{t+k}, c_t)}{\sum_{x_j\in X}f_k(x_j,c_t)}$. The dataset $X$ consists of one sample $x\sim p(x_{t+k}|c_t)$ (ground truth) and negative samples $x\sim p(x_{t+k})$ (the prior).

%When using probabilistic inference with gradient descent-like algorithms, we need to differentiate through a sampled value $X$ with respect to the sampled parameters $X\sim X(\theta)$. For discrete distributions, reparametrization trick would not work directly, as the gradient of argmax is zero a.s. One version is the log-likelihood trick, but it results in high variance. Another approach is to replace the hard discrete distribution with a soft one, one version of which is the Concrete distribution \cite{Maddison2017}.



% \cite{Fallah2020} use dictionary learning with an autoencoder.




%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
\label{ch:conclusion}
%%%%%%%%%%%%%%%%%%%%

We present the first (up to our knowledge) that learns a sparse causal graph of a Reinforcement Learning environment with a learned feature representation. We test our approach on a set of benchmarks and show a success of the proposed approach: it outputs an interpretable causal graph and allows to understand the environment. Using this approach could yield increased interpretability of Reinforcement Learning agents if used together with already existing methods requiring a causal graph as an input.

The main complexity right now is the time to fit a model of the environment, and then adjust the decoder to sparsify the model. Better (more adaptive) training procedures could speed up the convergence and increase the range of applicability of our approach.

\section{Future work}
A natural extension is to consider multiple time-step dependencies, either by stacking several subsequent observations together, or by using recurrent neural networks with structured sparsity, or by using an attention mechanism in the time axis \cite{Nauta2019}. It would be interesting to apply our approach to natural languages and other challenging not-yet-fully-understood time series and environments such as the protein folding problem. This could yield novel insights into the latent nature of these problems.

Combining the abstract causal concept discovery with doing interventions using policies \cite{Volodin2020} using an RL agent could yield more structured exploration via {\em interventions} on the causal graph \cite{He2008,Frisch2014,Pathak2017,Dasgupta2019}, and, thus, faster convergence time.

An even better approach would use contrastive losses \cite{VanDenOord2018,Kipf2020,Mohammadi2020} instead of the reconstructor. However, this did not work as well as the reconstruction loss in the observation space in this project.

\section{Sparse causal models and the problem of consciousness}
The connection to the problem of consciousness stems comes from two directions. First, this work can be seen as an implementation of the \cite{Bengio2017} hypothesis. Indeed, we are learning the most sparse model in the latent space, along with learning the representations. The paper explicitly defines this as one of the components of the consciousness prior \cite{Bengio2017}.


The fact that our approach works (albeit for simple environments) proves the merit of this hypothesis and shows that it could be used to explain consciousness: humans indeed can be learning abstract representations (at least partially by the virtue of) learning a space in which the dynamics becomes simple. For example, it is beneficial to learn a variable "time", because this single variable allows to predict if the sun rises, if the metro is open, or if a predator is likely to be sleeping.

Our learned representations can be used to ground the agent in natural language. Indeed, if we already have learned that there are several abstract concepts in the game such as "number of keys collected" or the "health of the player", it would suffice to only give two sentences associated with the player collecting a key or food. In contrast, a vanilla CNN given the same sentences at the same time would likely associate them with raw pixel state, and a policy network would likely associate them with the current action taken.

In this way, we shed (a bit of) light onto the easy problem of consciousness: the abstract concepts can come {\em before} language, in an unsupervised way, and the first words of language are simply mapped to already existing concepts using Hebbian learning (word and the stimulus occur together). Next, this process could be bootstrapped, since presence of words becomes new variables in the causal model, and the process is repeated, with a more powerful model and a more powerful set of words learned. Asking questions like "what is X?" can be seen as performing interventions on the causal graph: we know that asking "what is X" is likely to result in novel data (a learned way to optimize for an objective), and an $X$ is a novel variable which is still largely unused. Therefore, the agent intervenes setting $\mbox{do}(\mbox{ask 'what is X'})$ and samples data from a novel distribution.

While the grounding technique is known for a long time \cite{Hofstadter1994}, my work provides a proof that abstract concepts can be learnt before language from raw pixels, just by optimizing for the inner model simplicity -- a simple and beautiful enough principle to be innate to the brain \cite{Babadi2014}. Other projects like mine are involving more sophisticated (and less simple) priors, such that the action only increments one component, or that the dynamics in the latent space is linear, or that the environment is a $2D$ world.

Secondly, I express a hypothesis that a learned representation is crucial when dealing with the Integrated Information Theory \cite{Tononi2016}. Currently, the theory does not seem to specify at which level of abstraction it should be applied -- at the level of atoms, or at the level of molecules? Or, maybe, at the level of individual electrical components of a computer that we examine? Specifically, it is unclear what should be the "nodes", "factors" or "variables" in the model, the dependencies between which we are to examine to compute the $\varphi$. This leads to the value of $\varphi$ potentially being arbitrarily high, if a bijective transformation is applied before the analysis \cite{Doerig2019}.

If we declare that we need to first apply a transformation in which the graph becomes most sparse (which intuitively is connected to the value of $\varphi$: a sparse graph will likely have less edges \cite{Seth2011,Johnson}, or even becomes two independent components instead). Next, we compute $\varphi$. This seems to resolve the issue raised in \cite{Doerig2019}, albeit, in a bit of an ad-hoc fashion. More importantly, this resolves the ambiguity of the level of abstraction the theory should be applied: if we apply a decoder first and optimize for sparsity (or, maybe, low $\varphi$), the raw wave-function can be mapped into the state of molecules -- and the two are (somewhat) bijective.

Because of this connection between $\varphi$ and sparsity, our approach can be seen as approximately computing the $\varphi$ for the {\em environment}, in other words, how conscious it is. With this in mind, the VectorIncrement environment, for example, is not conscious: its causal graph is a union of two connected components (the "done" and the "time" is not connected to everything else), but can be seen as two "conscious" components. In contrast, the KeyChest environment's graph has only one connected component, and, thus, potentially has a non-$0$ $\varphi$ as a whole. However, this claim does not seem to be empirically testable, at least now.

\section{The AIXI framework and its connection to our work}
\label{sec:solomonoff_aixi}
In this section, we consider the theoretical Solomonoff induction framework, which gives a convergence bound for Supervised learning for sequence prediction, along with the AIXI framework \cite{Hutter2003,legg2008machine}, which gives the same guarantee for RL. The frameworks described in this section do not provide direct practical results, as they are uncomputable \cite{Hutter2003}. There exist computable approximations\cite{Veness2010}.

\subsection{Solomonoff induction}
\label{subsec:solomonoff}
We consider the class of all computable (having a probabilistic Turing machine that successively prints the string onto the tape) infinite distributions over the sequences of binary strings $\{\mu\}$. We assume that the true distribution is a member of this class. We are given symbols $x_t\in \{0,1\}$ one-by-one, and the task is to predict the next symbol given the previous ones. We apply the Bayesian approach: given a prior $\xi(\mu)\in[0,1]$ over the sequences $\mu$, we would like to find the posterior $\xi|(x_1,...,x_t)$ over the sequences given the observed data $x_1,...,x_t$. We denote as $K(\mu)$ the length of the shortest program that generates $\mu$\footnote{See \cite{Hutter2003} for a fully formal definition}.

It can be shown \cite{Hoang2020,Citation2021} that any possible prior has to decay as $2^{-\alpha K(\mu)+\beta}$ as $K(\mu)\to\infty$. Indeed, while the prior distribution has to sum up to $1$, the number of programs with length $n$ increases exponentially.

Solomonoff has shown in 1964 that with a prior defined as $\xi(\mu)\sim 2^{-K(\mu)}$, Bayesian inference results in a fast convergence to the true distribution. The squared expected distance in the space of all sequences, between the posterior $\xi|(x_1,...,x_{t-1})$ given the data from the first $t-1$ time-steps $x_{< t}=(x_1,...,x_t)$, and the true distribution $\mu$ is bounded via the complexity (See \cite{Hutter2003}, Equation 18):
$$
\sum\limits_{t=1}^{\infty}\sum\limits_{x_{<t}}\mu(x_{<t})\left(\xi(x_t=0,x_{<t})-\mu(x_t=0,x_{<t})\right)^2\leq  \mbox{const}\cdot K(\mu)
$$

Roughly speaking, the algorithm will make approximately $K(\mu)$ mistakes, when learning a sequence of complexity $K(\mu)$.

\subsection{The AIXI framework}
\label{subsec:aixi}
AIXI\cite{Hutter2003} is an extension of Solomonoff induction to the problem of Reinforcement Learning. We set $x_t=(o_t,r_t)$ -- the observation and the reward given by the environment. The action is selected greedily to optimize for the expected reward given the posterior (see Equation 23 in \cite{Hutter2003}):
$$
a_{t+1}=\arg\max\limits_{a}\mathbb E_{x_{t+1},x_{t+2},...\sim \xi|(x_1,...,x_t)}V_{\geq t}
$$

Roughly speaking, this equation means that we first select the simplest environments (because the prior $\xi$ penalizes for sequence's complexity), that fit the observed data well. Having such a model, we simulate the future using tree search, and select an action optimizing for the discounted value.

\subsection{Connection to \sysname}
First, we note that while in theory, penalizing for model's complexity is inevitable (see \autoref{subsec:solomonoff}), for practical cases the total model complexity is limited by the hardware, and, therefore, we are free to favor more complex models over simpler ones.

Having the simplicity prior arguably helps the convergence speed. Indeed, having a similar prior and a good inference algorithm approximating Bayesian inference would could that we obtain similar convergence results. Neural networks arguably do not have such a simplicity prior "built-in", as they are likely to learn spurious correlations\cite{ilyas2019adversarial}. Therefore, such a prior should be explicitly added.

It is left for future work to test if using our approach leads to better sample-efficiency of learning dynamics model of RL environments.

% Appendices are optional
% \appendix
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{How to make a transmogrifier}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% In case you ever need an (optional) appendix.
%
% You need the following items:
% \begin{itemize}
% \item A box
% \item Crayons
% \item A self-aware 5-year old
% \end{itemize}


\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{thesis}
\bibliographystyle{abbrv}
%\printbibliography

\end{document}
