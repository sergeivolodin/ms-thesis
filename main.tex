%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EPFL report package, main thesis file
% Goal: provide formatting for theses and project reports
% Author: Mathias Payer <mathias.payer@epfl.ch>
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,11pt,oneside]{report}
% Options: MScThesis, BScThesis, MScProject, BScProject
\usepackage[MScThesis,lablogo]{EPFLreport}
\usepackage{amsmath,amssymb}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
%\usepackage{natbib}
\usepackage{xspace}

\title{Learning Abstract Representations in General Reinforcement Learning Environments via Model Sparsity}
\author{Sergei Volodin}
\supervisor{Dr. Johanni Brea}
\adviser{Prof. Wulfram Gerstner}
%\coadviser{Second Adviser}
\expert{Carl-Johann Simon-Gabriel}

\address{EPFL IC/SV Laboratory of Computational Neuroscience (LCN)\\
    BÃ¢timent AAB \\
    offices 135-141 \\
    CH-1015 Lausanne}

\begin{document}
    \maketitle
    \makededication
    \makeacks

\begin{abstract}
We would like to learn a sparse causal model of a Reinforcement Learning environment. Our experiments show that for reasonably challenging environments such as Montezuma revenge and MuJoCo, we are able to extract a latent space where the environment's model becomes linear. In addition, the resulting model is sparse, which allows to interpret it as a sparse causal graph. This shows the inherent hidden simplicity in the environment. Our findings simplify modelling of RL environments and allow to extract the game logic in an automated manner, allowing for more efficient planning. The agents trained with this approach are also more robust to distributional shift and require less data to train.
\end{abstract}

\maketoc

%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%

The introduction is a longer writeup that gently eases the reader into your
thesis~\cite{dinesh20oakland}. Use the first paragraph to discuss the setting.
In the second paragraph you can introduce the main challenge that you see.
The third paragraph lists why related work is insufficient.
The fourth and fifth paragraphs discuss your approach and why it is needed.
The sixth paragraph will introduce your thesis statement. Think how you can
distill the essence of your thesis into a single sentence.
The seventh paragraph will highlight some of your results
The eights paragraph discusses your core contribution.

This section is usually 3-5 pages.

%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
%%%%%%%%%%%%%%%%%%%%

The background section introduces the necessary background to understand your
work. This is not necessarily related work but technologies and dependencies
that must be resolved to understand your design and implementation.

This section is usually 3-5 pages.


%%%%%%%%%%%%%%%%
\chapter{Design}
%%%%%%%%%%%%%%%%

Introduce and discuss the design decisions that you made during this project.
Highlight why individual decisions are important and/or necessary. Discuss
how the design fits together.

This section is usually 5-10 pages.


%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
%%%%%%%%%%%%%%%%%%%%%%%%

The implementation covers some of the implementation details of your project.
This is not intended to be a low level description of every line of code that
you wrote but covers the implementation aspects of the projects.

This section is usually 3-5 pages.


%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}
%%%%%%%%%%%%%%%%%%%%

In the evaluation you convince the reader that your design works as intended.
Describe the evaluation setup, the designed experiments, and how the
experiments showcase the individual points you want to prove.

This section is usually 5-10 pages.


%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}
%%%%%%%%%%%%%%%%%%%%%%

The related work section covers closely related work. Here you can highlight
the related work, how it solved the problem, and why it solved a different
problem. Do not play down the importance of related work, all of these
systems have been published and evaluated! Say what is different and how
you overcome some of the weaknesses of related work by discussing the
trade-offs. Stay positive!

This section is usually 3-5 pages.


%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
%%%%%%%%%%%%%%%%%%%%

In the conclusion you repeat the main result and finalize the discussion of
your project. Mention the core results and why as well as how your system
advances the status quo.

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

% Appendices are optional
% \appendix
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{How to make a transmogrifier}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% In case you ever need an (optional) appendix.
%
% You need the following items:
% \begin{itemize}
% \item A box
% \item Crayons
% \item A self-aware 5-year old
% \end{itemize}

\section{Introduction}
Consciousness prior, AIXI lead to simple or sparse models.

Disentangled representations and representation learning

Model-based reinforcement learning

Causal modelling for reinforcement learning. Interventions

AI safety: distributional shift robustness (generalization), interpretability, sample-efficiency

\section{Proposed architecture}
\subsection{Theory}
Like in AIXI, would like to find the simplest model of the environment:
$K(\mu)\to\min$ s.t. $L(\mu, data)\to\min$. In general, this problem is uncomputable.

We decompose the model into a Decoder and a sparse Model. The model in observational space $W$ is then the sparse Model $M$ applied to Decoded features, with the result projected back into the high-dimensional space.
$$
W=D^{-1}MD
$$

\subsubsection{Why sparse models?}
Traditionally, sparsity of models usually means making the number of non-zero parameters low. This can be seen in linear regression when applying the Lasso regularization. Such methods learn to rely only on a handful of features, instead of considering the whole input. When data is multicollinear, this leads to better generalization, as the model only relies on the most salient features, and is less likely to learn spurious correlations between features.

In Supervised Deep Learning, sparsity of representation traditionally means regularizing the activation of the latent hidden layer in an autoencoder-like setup (an encoder transforming a high-dimensional representation into a low-dimensional one, and then a decoder doing the inverse transform back to the high-dimensional space). In such setups, sparsity is enforced in order to enforce constraints on the learned latent space. With such regularization, the model is learning a sparse coding -- one where each input is represented with a minimal number of non-zero components.
This representation is better than a "dense" one, in case if there are certain regularities in the dataset.
For example, if the input images have an underlying latent generative model in which the high-dimensional image is a "child" of only few discrete variables (such as, a cat can only be of certain colors, which can be represented in a one-hot way, and other images, such as images of dogs, can be distinguished from images of cats by another binary variable).
In neuroscience, such representations are biologically plausible due to lower energy usage and higher fault tolerance [??].

However, sparse representation does not necessarily imply interpretability. In supervised tasks, it it known that [Google post on disentanglement] the model still learns spurious correlations, which leads to different controllable aspects of the image being "entangled" in one activation node.

Spurious correlations in supervised learning do not have a general way of being resolved, because observational data is not enough to learn the correct causal model. An example of this is adversarial images, which can be seen as spurious correlations, namely, the model relying on high-frequency features which, while discriminating between images on the training set due to high memorizing capacity of the network, do not generalize to the shifted adversarial distribution [features not bugs post].

While the problem of learning good models from observational data is interesting in its own right, in this project we focus on a reinforcement learning setting where there is a possibility to perform actions. Since switching from one policy to another determines the distribution of histories, switching from one policy to another can be seen as performing interventions on the underlying causal graph. In this way, for example, an agent playing Breakout by precisely targetting each block one-by-one is different from an agent trying to put the ball to the top of the playing field, speeding up the process significantly. If we consider the underlying data generation process in the latent space of positions of various objects (x and y coordinates of the ball, the player and all the targets), and in an interaction between them, and a similar representation for the agent (for example, the first agent's action depends on the leftmost still-standing target, which it hits perfectly, and the second agent's action depending as well on whether or not there is an opportunity to get to the top), switching from one policy to another can be seen as an intervention in this history-generating graphical model, in which the gate switching the dependencies of the policy's action is altered, and set to $1$ instead of $0$, signifying the reliance to the second strategy.

While the total space of policies is represented with exponentially many such switch nodes (namely, one node for each subset of coordinates the policy can possible depend on, and one node for every discretized value of the weights of a neural network, for example), some of them are more complex than others.

We start the theoretical derivation from the AIXI framework, where the environment is a Turing machine, and the agent is a non-computable greedy best-response Bayesian solution considering all possible environments. Theoretically, such agent is known to have certain optimality conditions under the assumption of sufficient exploration. Compared to Solomonoff  induction (a perfect uncomputable Bayesian solution to the online supervised learning problem, or the task that the GPT-3 solves -- self-supervised prediction), in RL, AIXI (which is designed in the same way as the Solomonoff induction) is not always optimal. Indeed, it was discovered that AIXI does not explore enough, because there are bad priors leading to the agent "believing" that it is not beneficial to explore. In the simplest setting, we can always select such a prior where the probability of the true environment, and of any sufficiently close-by environment leading to rewards when taking action $a_1$ is arbitrarily low, leading to the agent never trying $a_1$, and, thus, never updating its posterior [On the optimality...].

General theory of RL suggests solutions to this problem, leading to weaker convergence results [Thompson sampling]. The basic idea is to let go of the agent being perfectly Bayesian, and (knowingly) use an outdated model to take actions. Intuitively, this 'denial' leads to even improbable actions being taken into consideration, and, thus, has more chances of discovering the true posterior.

The Solomonoff Bayesian setting considers weighting the environment models by their complexity.

In this setting, we consider the prior over environments $\mu$, $\xi(\mu)=2^{-K(\mu)}$ where $K$ is the Kolmogorov complexity. Next, we collect history using a policy $\pi$, which gives a Bayesian posterior:

$$
\nu(\mu|h)=\frac{\mu(h)2^{-K(\mu)}}{\sum_{\nu}\nu(h)2^{-K(\nu)}}=\frac{\mu(h)\xi(\mu)}{\xi(h)}
$$

Given the posterior $\nu'=\nu|h\equiv \nu(\mu|h)$, we "simply" run an infinite tree search to find the best possible action:
$$
a_t^{AIXI}=\arg\max_{a_t}\sum\limits_{t=t_0}^{\infty}\gamma^t\sum_{r_t,s_t}\nu'(r_t,s_t|r_{<t}s_{<t}a_{<t})
$$

Here, $\xi(h)$ is the prior probability of the data, or the probability of history in the Universal Environment (a mixture of environments where each environment is weighted with its complexity).

In practice, it is not viable to search over all possible Turing machines (which is required to evaluate the denominator, $\xi(h)$), and we cannot run them for an infinite number of time-steps (like it is done in AIXI). Another problem here is that we cannot compute $K(\mu)$.

To make the problem practical, we additionally assume that there is a structure to the environment.

Namely, we assume that for the whole environment there exist a mapping $f\colon o\to f$ which maps observations to features. For a while, we assume deterministic environments. We also assume 1 time-step dependencies (Markov property). Given our feature assumption, we assume that in the feature space, the dynamics of the environment is linear (inherent simplicity??). We only consider environments $\mu$ which are representable (we consider $r_t$ as part of $s_t$ for simplicity of notation here) as
$$
f_t(\mu)=M_ff_t+M_aa_t+b
$$

{\bf OMG}

Our linear class $f_{t+1}=M_ff_t+M_aa_t+b$ is a really small class of environments, even with a non-linear decoder... Indeed, a Line environment (navigating on a line with 3 states) does not fit into that definition. Consider an MDP with 3 states, $[1, 2, 3]$, and two actions, right and left encoded as one-hot: $a_1=[1, 0]$ and $a_2=[0, 1]$. The bias term in the linear model can be shifted into the actions $M_a$. Suppose that the decoder gives some features for the states $f_1, f_2, f_3$ (does not matter how, it's only important that the features are fixed). Multiplying $M_aa_i=A_i$ by definition, this gives two vectors $A_1$ and $A_2$.

Then, $f_1=M_ff_1+A_2$ (going left from $f_1$ gives $f_1$), and $f_3=M_ff_3+A_1$ (going right from $f_3$ gives $f_3$). Now consider what happens in $f_2$. If we go left, we get $f_1$: $M_ff_2+A_2=f_1$. If we go right, we get $f_3$: $M_ff_2+A_1=f_3$. Subtracting these two gives $f_3-f_1=A_1-A_2$. On the other hand, the "stuck at the wall" equations give us $f_3-f_1=M_ff_3+A_1-M_ff_1-A_2=M_f(f_3-f_1)+A_1-A_2$. So, equating these two, we get
$f_3-f_1=M_f(f_3-f_1)+A_1-A_2=A_1-A_2$. This means that $M_ff_3=M_ff_1$.

Now, consider the "stuck at the right wall" $f_3=M_ff_3+A_1$, and going right from $f_1$: $f_2=M_ff_1+A_1$. But $M_ff_1=M_ff_3$, which means that $f_2=M_ff_3+A_1$. Note the same RHS as for the "stuck at the right wall". Therefore, $f_2=f_3$, {\bf we cannot distinguish between $f_2$ and $f_3$, degenerate representation.}



Here, $f_t=D(s_t)$ and $s_t=R(f_t)$

Imagine if the representation $f$ is sparse at every time-step. Does this guarantee an interpretable environment?

From the point of view of the Free Energy principle, we are looking for a minimal Markov Blanket.

From the point of view of the leading theory of consciousness, Integrated Information Theory. Why do we look for sparsity, when consciousness there is about irreducibility of a big model into smaller models? IIT does not specify which representation of the physical world needs to be mapped to the nodes in the causal graph, while we have a decoder which learns this representation, and selects one where the model is the simplest one. Tononi says that we should pick the representation with maximal $\phi$, but, it seems, In our case, something not affecting the reward (or the reconstruction in general), would be independent of causal variables that are optimized for. Thus, such a variable would be unconscious. The project can be seen as computing a proxy for $\phi$ -- minimizing the complexity. Such an interpretation of IIT (allowing for an arbitrary function computing the representation, and minimizing over complexities/$\phi$) could resolve the issues raised in \cite{doerig2019unfolding}. There, a function is applied to the representation, after which an inverse transform is applied. If we only care about the minimal-$\phi$ representation, all of that would be cancelled, and only the high-level variables would remain.


What if the decoder is too complex? We could re-use components, like humans don't need to re-learn existing conv1 filters to learn a new game.

In the model, we can sample from a categorical gumbel-softmax distribution in order to select a model. We can re-use models for different features. Each of the feature models has a categorical distribution over the bank of models, and we regularize for the total entropy of all models (with an addition that we can permute the features in arbitrary way before applying the model).


The total complexity of the environment $\mu$ is thus $K(\mu)=K(M_a)+K(M_f)+K(D)+K(R)$


$\mu(h)\equiv \mu()$

Finding good features by regularizing the {\em model} to be sparse.

Three losses: model fit, sparsity regularization, reconstruction

Reconstruction losses: inverse decoder norm, reconstructing observations (autoencoder), inverse model norm (we force the model to be an invertible matrix), predicting actions, predicting value function from features (like in muzero)

Causal learners: linear, graph NNs, linear with sampling

For the model fit, we use Mean Squared Error. Alternatives are to use contrastive loss (with fake samples)

For the sparsity, we use L1-regularization. An alternative is to use projection.

Architecture improvements: model at each layer of the decoder for a faster fit, many time-steps, batch normalization, adaptive sparsity loss

\section{Literature review}

Simple mathematical equations can describe our world well \cite{hamming1980unreasonable}. We assume that the environments in RL that we are interested in are also describable by such equations.

\cite{Battaglia2018} introduces Graph Neural Networks (GNNs) based on the idea of thoughts as graphs of connections between abstract concepts. The idea is to combine the strengths of "handcoded" or "biased" models and the "end-to-end" or "from scratch" models. \cite{Velickovic2020} extend this idea by allowing a linear (in the number of nodes) number the edges to be learned.

\cite{Cranmer2020} use GNNs to discover unknown laws of dynamics. They apply symbolic regression to the node and edge models of the GNN, to extract the symbolic expression. The model is used for pairwise forces and for discovering the laws for Dark Matter. The symbolic expression (Eureqa) generalizes better than the non-symbolic model it was extracted from. $l_1$ sparsity is used in the GNN to ease the problem for the symbolic regression.

Libraries: pytorch-geometric, graph-nets from DeepMind

\cite{Genewein} propose to use probability trees instead of graphical models to encode causal relationships. The benefit is that the set of parents of a node can be dynamically dependent on values of other nodes. The paper does not provide a way to construct the probability trees.

\cite{Zambaldi2018} introduce an attention module inside a Reinforcement Learning agent (inside the policy and the value networks), allowing it to solve tasks not solved by a simple MLP, because the task requires relational reasoning. The attention on the charts seem to favor one object attended to, corresponding to the ground truth next item to go to. Similar technique can be seen in \cite{Hahne2019} to solve visual reasoning tasks (similar to IQ tests)

\cite{Xie2020} introduce latent variables into the learned linear causal models using statistical tests.

\cite{Johnson2016} introduce a variational approach to model dynamics: probabilistic graphical model in terms of Switching Linear dynamics is applied to a learned embedding using a VAE. The model is applied to 1D and 2D image tasks, and it successfully uncovers the underlying dynamics.

\cite{VanDenOord2018} fit a neural autoregressive model. The paper suggests that fitting many-step future predictions is of utter importance, since only in such predictions the model is tested for "true" generalization of high-level features. The paper also suggests that MSE or other losses commonly used for reconstruction are not optimal, and generative models are hard to train. As an alternative, the paper suggests optimizing for the mutual information between the latent code and the original inputs. The model predicts the log-odds ratio for an offset $k$ and input up to time $t$ as $f_k(x_{t+k},c_t)\sim \frac{p(x_{t+k}|c_t)}{p(x_{t+k})}$ with a model $f_k(x_{t+k}, c_t)=e^{D(x_{t+k})^TW_kc_t}$. This model is trained using the InfoNCE loss (noise-contrastive estimation) $L=-\mathbb E_X\log\frac{f_k(x_{t+k}, c_t)}{\sum_{x_j\in X}f_k(x_j,c_t)}$. The dataset $X$ consists of one sample $x\sim p(x_{t+k}|c_t)$ (ground truth) and negative samples $x\sim p(x_{t+k})$ (the prior).

When using probabilistic inference with gradient descent-like algorithms, we need to differentiate through a sampled value $X$ with respect to the sampled parameters $X\sim X(\theta)$. For discrete distributions, reparametrization trick would not work directly, as the gradient of argmax is zero a.s. One version is the log-likelihood trick, but it results in high variance. Another approach is to replace the hard discrete distribution with a soft one, one version of which is the Concrete distribution \cite{Maddison2017}.

\cite{kalainathan2018structural, ng2019masked} propose to learn a distribution over causal graph in a mean-field way (all edges are independent and modelled as Bernoulli distribution). \cite{Brouillard2020} extend this approach to modelling interventions.

\cite{Fallah2020} use dictionary learning with an autoencoder.

\cite{kipf2019contrastive} work on learning abstract representations from pixels in 3D environments and in Atari using Graph neural networks, leading to more interpretable world (environment) models.

\section{Model learning for a fixed policy}
The problem of learning a model is formulated as follows. Given a deterministic RL environment $\mu$ and a policy $\pi$, we want to find a function computing the next observation: $o_{t+1}=M_o(o_t, a_t)$. Additionally, we would like to learn the reward and the termination ("done"): $r_t=M_r(o_t, a_t)$, $d_t=M_d(o_t, a_t)$. Since observations are high-dimensional, we require the model to be represented as a composition of a decoder, a model in the feature space, and a reconstructor: $o_{t+1}=R(M_f(D(o_t), a_t))$. If we denote $f_t=D(o_t)$, we can additionally require $D(o_{t+1})\approx M_f(f_t, a_t)$. In this case, the function $M_f$ is a {\em model in feature space}. If we additionally require $R(D(o_t))\approx o_t$, the function $R$ becomes the inverse for the decoder.

Training the reconstructor is usually the most time-consuming part, since it requires to train a generative model for high-dimensional observations. Therefore, several techniques are applicable to avoid this part. In these cases, a full model in observation space is not obtained.
\begin{enumerate}
    \item Margin loss in the space of observations. For observations $o_1\neq o_2$, we require that $\rho(D(o_1), D(o_2))\geq h$ via a margin loss: $\max(h-\rho, 0)\to\min$
    \item Contrastive loss: we train a classifier $C(f, f')$ with a loss $\frac{C(f_t, f_{t+1})}{C(f_t, f_{t+1})+C(f_t, f_{neg})}\to\max$ where $f_{neg}$ is not equal to the true feature $f_{t+1}$
    \item Training a discriminator with labels $C(f_t, f_{t+1})\approx 1$, $C(f_t, M_f(f_t, a_t))\approx 0$, and with a separate optimizer training for $C(f_t, M_f(f_t, a_t))\approx 1$ (GAN)
    \item Instead of predicting high-dimensional observations, only predicting some sufficient (in the sense of playing the game) statistic, such as the value function: $V_f(D(o_t))\approx V(o_t)$, where $V_f$ is a trainable value-to-go predictor, and $V(o_t)$ is the reward-to-go collected at the episode.
\end{enumerate}

{\bf Sampling procedure.} Since we would like the model to approximate the next observation $o_{t+1}\approx M_o(o_t, a_t)$, we must sample triples $(o_{t+1}, o_t, a_t)$ (and other related features such as reward, termination, or reward-to-go) from the uniform distribution: we randomly sample an episode, and then pick a random observation. This would mean that we only keep one step from a whole episode. This is not practical, as we have to "throw away" most of the steps. We approximate this with an experience replay buffer. In case if the environment "looks differently" for different initialization random seeds (for example, the labyrinth has a different layout), such an approximation results in the following issue. If we sample a mini-batch from 1 episode, and the subsequent mini-batch from another (single) episode, the model would tend to overfit to the first mini-batch (specifically, for example, to the locations of objects in the labyrinth). On the next call, the loss will spike, and the model will overfit to the next object locations. Having an experience replay buffer and sampling the triples randomly from it alleviates the issue. This is the same problem as was encountered in the DQN paper.

In practice, we use the mean-squared regression. To account for different scales of observation components, feature components and rewards, we compute the {\em relative} mean absolute error:
$$
L_{rel-mse}(f^{pred}, f^{true})=\frac{1}{T}\sum\limits_{t=1}^T\sum\limits_{f=1}^F\left[\frac{f^{true}_{f,t}-f^{pred}_{f, t}}{\sigma'(f^{true}_{f, \cdot})}\right]^2
$$

Here $\sigma(f^{true}_{f, \cdot})$ is the standard deviation for the true features. In case if this value is less than $\varepsilon=10^{-8}$, we set this value to $1$. This is required for the case of constant pixels, or constant features. Setting this value to $\varepsilon$ instead of $1$ produces extremely high loss spikes in case if in some of the batches, a feature is constant, and in others, it is not.

The relative loss $L_{rel-mse}$ is more interpretable than the standard mean-squared-error loss, because its value signifies how many features have a large error. For example, for $10$ equal features with equal predictions, a value $L_{rel-mse}=10$ means that for all features, the prediction error is equal to the standard deviation. More importantly, a value $L_{rel-mse}=10^{-2}$ means that for each of the features, the relative error is at most $10\%$ in terms of the standard deviation.

\begin{itemize}
    \item Reconstruction loss: $L_{rec}=L_{rel-mse}(R(D(o_{x})), o_{x})$. Here we denote values before (and while) taking an action as $x$, and values at a step immediately after taking an action as $y$. In this notation, for example, $o_{x, t+1}=o_{y,t}$
    \item Prediction loss in observation space: $L_{pred}=L_{rel-mse}(R(M_f(D(o_{x}, a_{x})), o_{y})$
    \item Feature consistency loss in feature space: $L_{cons}=L_{rel-mse}(M_f(D(o_x, a_x)), D(o_y))$
\end{itemize}

Both of the losses $L_{pred}$ and $L_{cons}$ are required to obtain good features. First, training with $L_{pred}$ allows to have an end-to-end model, where the right-hand side does not depend on the decoder. This becomes a supervised learning problem. Secondly, without optimizing for the feature consistency loss, $L_{pred}=0$ does not guarantee that $L_{cons}=0$, since the reconstructor can "recognize" two different sets of features. In this case, output features from the model do not necessarily correspond to the input features.

Since we would like to obtain an interpretable model, we add the feature consistency loss $L_{cons}$.

{\bf Separate models for each feature and each pixel.} Empirically, in our test toy environments (VectorIncrement and KeyChest), the tasks of predicting different pixels with a reconstructor $R$ do not have many information in common, and, therefore, are poor tasks for multi-task learning. Empirically, predicting each pixel $i$ with a separate model $R_i(f)$ converges faster than having a joint neural network $R(f)$.

\section{Sparse model learning}
Suppose that we have reasonably low losses $L_{rec},\,L_{pred},\,L_{cons}<\varepsilon$. This means that for each of the output pixels, we can predict it with relative error at most $\varepsilon$. In addition, the same applies to the relative error in predicting each feature.

In the space of all reconstructors, decoders and models, we would like to find a model which is most sparse in the sense of a causal graph. This means that, for each output feature $f_j$, we would like it to depend on the fewest amount of input features $f_i$. Given that, the graph of dependencies in the feature space is most sparse (in terms of the number of edges), or, the program describing the environment is most simple.

To do so, we use the model from Bengio. We use a tensor of probabilities of shape $p=(F+F', F+A)$, where $F$ is the number of features, $F'$ is the number of additional features to predict (2 -- reward and termination), and $A$ is the number of actions. This tensor has values between $0$ and $1$. The values are interpreted as probabilities of having an edge from feature $f_i$ to feature $f_j$: if the value is $1$, the feature $f_j$ depends on $f_i$ with probability $1$.

Given such a tensor of probabilities, we can sample random variables $\xi\sim p$, where $\xi$ is a binary matrix of dimensions $(F+F', F+A)$ with values in $\{0, 1\}$. Now, if an edge is not present in the graph, the model for a feature should not depend on the corresponding input feature. We achieve this by multiplying the input with this tensor $\xi$. Next, we have $F+F'$ models predicting each feature, each depending on the input multiplied with the binary mask $\xi$.

Having $F+F'$ independent models instead of having one big model with $F+F'$ outputs has the same advantage as stated before for the reconstructor $R$: since the tasks do not have a lot in common, the convergence is faster if the networks are independent.

In addition, the model receives the mask $\xi$ apart from the input $[f_t, a_t]\odot \xi_i$. This is necessary because the model needs to "know" whether it has received $0$ as input because the feature is $0$, or because the feature was "de-selected".

Suppose that we have a dataset of triples $\{(f_{t+1}, f_t, a_t)\}$. We sample masks $\xi_t$ for every time-step $t$. For this example, we set $p_{ji}=0.5$ for all features. The loss $L_{pred}$ and $L_{cons}$ becomes a random variable, because the inputs a pre-multiplied by random masks. If we fit the model on this data (in a supervised way), it will learn to rely on features if they are present, and to replace them with a "mean value" in case if they are not. We can optimize the parameters of $R, M, D$ using gradient descent. Now, how to obtain the gradient with respect to $p$?

The well-known trick is Gumbel-Softmax, which can yield $\frac{\partial L}{\partial p}$ given samples. However, since it internally uses saturating softmax,  it takes a lot of time in practice to obtain a value close to $1$. Even in the mostly-linear regime, it takes a lot of time to optimize for parameters $p$, because the gradient is multiplied by $\sigma(\cdot)(1-\sigma(\cdot))$. Instead, we use the REINFORCE gradient:

$\frac{\partial L}{\partial p}=\frac{\partial}{\partial p}\mathbb{E} L(\xi(p))=\frac{\partial}{\partial p}\mathbb E_{\xi_{11}}...\mathbb E_{\xi_{nm}}L(\xi_{11},...,\xi_{nm})$.

Now, consider one-dimensional case and $\frac{d}{dp}\mathbb E_{\xi\sim F(p)}L(\xi)=\int \frac{d}{dp}f_p(\xi)L(\xi)d\xi$. Now, using the log-likelihood trick, $\frac{df_p(\xi)}{dp}=\frac{d\log f_p(\xi)}{dp}f_p(\xi)$. Therefore, $\frac{d}{dp}\mathbb E_{\xi\sim F(p)}L(\xi)=\int\frac{d\log f_p(\xi)}{dp}f_p(\xi)L(\xi)d\xi=\mathbb E_{\xi\sim F(p)}\frac{d\log f_p(\xi)}{dp}L(\xi)$. In case of discrete distributions, we need to replace the probability density function $f_p(\xi)$ with the probability mass function $F_p(\xi)$

For the Bernoulli distribution, this gives $\frac{d}{dp}\mathbb E_{\xi\sim Be(p)}=\sum_{\xi\in\{0, 1\}}\frac{d\log F_p(\xi)}{dp}L(\xi)$. Since $F_p(1)=p$ and $F_p(0)=1-p$, we obtain $\frac{d}{dp}\mathbb E_{\xi\sim Be(p)}=L(\xi=1)-L(\xi=0)$.

In case if we have many random variables $\xi_{ji}$, we can set $L(\xi_{ji})=\mathbb E_{\xi_{-ji}}L(\xi_{ji}, \xi_{-ji})$, and then obtain $\frac{\partial }{\partial p}L_{rel-mse}=\mathbb E_{\xi_{-ji}}L_{rel-mse}(\xi_{ji}=1, \xi_{-ji})-\mathbb E_{\xi_{-ji}}L_{rel-mse}(\xi_{ji}=0, \xi_{-ji})$.

In practice, given a sample of $\xi$ of shape $[T, F+F', F+A]$, for a pair of features $j, i$, we select the values of the loss $L_{t}$ where $\xi_{t, j, i}=1$, and take the average of such $L_t$ to obtain a sample estimate of $\mathbb E_{\xi_{-ji}}L_{rel-mse}(\xi_{ji}, \xi_{-ji})$. The same procedure is applied for the values of $t$ where $\xi_{t, j, i}=0$. This corresponds to running $F+F'\times F\times A$ "randomized control trials" to establish the effect of $\xi_{ji}$ on $L$, and computing the difference in means. This way, we manually compute the gradient for each $p_{ji}$.

While REINFORCE gradients are known to be noisy, here we only have two values, and sample sizes are considerably large for large batch sizes $T\sim 5000$ and $p\sim 0.5$. By the central limit theorem, the sample means are close to the true expectations. After applying the gradient, the probabilities $p$ are clipped to be between $0$ and $1$.

Note that the problems of estimating the gradient are not independent, in a sense that a gradient with respect to $\xi_{ji}$ depends on values of $p_{-ji}$. This happens because the values of the loss depend on these features as well.

Let's consider different cases, assuming that the model was fitted to convergence on a set of features with a fixed tensor of probabilities. Consider the case of $2$ input and output features, with features being equal and having a deterministic relationship (from input to output).
\begin{enumerate}
    \item $p=0$ for all pairs. In this case, no features are given to the model, and the model will learn to predict the mean of the output features. The causal graph is empty. The loss is equal to $2$ (since standard deviation for each feature is $1$ w.l.o.g. because of the relative error).
    \item $p=1$ for all pairs. In this case, the model uses all features, and the error is minimal. The loss is $0$.
    \item $p=0.5$ for all pairs. In this case, the model will try to extract as much information as possible when the feature is present. Essentially, with probability $0.25$, the model will give the mean output feature, and with probability $0.75$ it will use either one of the input features, or both of them. The loss is equal to $2\times 0.25=0.5$.
    \item In the general case $p\in [0, 1]$, the model has an error of $2$ with probability $(1-p)^2$ and a loss of $0$ with probability $1-(1-p)^2$. The expectation is $2(1-p)^2$.
\end{enumerate}

Now consider a similar case when the mutual information between features is $0$ (they are independent). In this case, for a probability $p$, we have the loss being $2$ with probability $(1-p)^2$, a loss of $1$ with probability $2p(1-p)$ and a loss of $0$ with probability $p^2$. The expectation is $2(1-p)^2+2p(1-p)=2-4p+2p=2-2p=2(1-p)$. This loss is higher than for the case of equal features for all $p\in (0,1)$.

To avoid a case when there are no samples with $\xi=1$ (with low $p$), we limit the range of $p$ to be $p\geq p_{\min}=0.01=1\%$. Setting this value higher results in feature duplicates, as the model tries to predict the outputs even from partially present features. A value of $1\%$ with a batch size of $T=5000$ gives $50$ samples with $\xi=1$ in the worse case (on average), which is empirically sufficient to turn features on. To turn the features off, we introduce a sparsity regularizer. To ease this even more, we add a version of the losses with $p=0.5$ (regardless of the true value of $p$). To train the model faster, we add a version of the loss with $p=1$ as well.

Denote $L_{model}=L_{pred}+L_{cons}$. Given the two other versions with $p=0.5$ and $p=1$ we have also $L_{model}^{p=0.5}$ and $L_{model}^{p=1}$.

\subsection{Annealing.} To learn a sparse model, we additionally regularize the probabilities $p$ for sparsity as $L_{sparse}=\sum_{ij}|p_{ij}|$. The total gradient for $p$ is a sum of gradients from $L_{sparse}$ and the REINFORCE gradient from $L_{pred}$ and $L_{cons}$.

Consider the relative error ("relative sparsity gap") $\gamma(R, D, M, p)=\frac{L_{model}^{p=p}-L_{model}^{p=1}}{L_{model}^{p=p}}$. It shows how the loss given the current selected features compares to enabling all features. For a any fixed $p$ and any $D$, if we fit the $M$ and $R$ on $L_{model}$ and $L_{model}^{p=1}$ (note that there is no conflict between these two losses, because masks uniquely determine whether the features are on or off. Essentially, this is a big supervised dataset), the value of $\gamma\in[0, 1]$: turning off some features can only increase the loss. In case if $p<1$ and $\gamma=0$, the output does not depend on the input. If $\gamma=1$, the model perfectly fits the data ($L=0)$ with $p=1$ (this does not happen in practice). $\gamma=0.5$ means that the error could decrease $50\%$ in case if we turn on all features.

An additive error $\delta(R, D, M, p)=L_{model}^{p=p}-L_{model}^{p=1}$ is in range between $0$ (if model is trained) and $N=F+F'$ (in case if model can perfectly fit the data and $p=0$). Therefore, we consider the ratio $\Delta(R, D, M, p)=\frac{\delta(R, D, M, p)}{F+F'}\in[0, 1]$.

Annealing proceeds in the following stages:
\begin{enumerate}
    \item $\delta>\delta_{\max}$. In this case, the model is trained to decrease $L_{model}^p$, which increases some $p$. Sparsity coefficient is decreased (we care more about fitting any model rather than about sparsity).
    \item $\delta\leq \delta_{\max}$. In this case, we care about sparsity and increase the coefficient. In case if at some point, $\delta$ crosses the threshold, we do not change the coefficient for some time, to allow the model to learn from a dataset where some features are turned off. This "freezing" at a "low temperature" allows the model to "crystallize" and learn a new representation (one where some features are turned off).
\end{enumerate}

Consider the following cases:
\begin{enumerate}
    \item $p=p_{\min}$. In this case, the model cannot predict features reliably: even in the toy example above, the error would be $L\sim 2\times (1-p)\approx 2$ for 2 features -- very bad result for a relative error. In this case, the REINFORCE gradient will turn on features yielding maximal decrease in the loss.
    \item $p=1$. In this case, we would like to turn off some of the features, because we would like to obtain the most sparse model.
\end{enumerate}

Overall we would like to find the sparsest graph such that the sparsity gap does not exceed some $\varepsilon_{\max}$:
$$
\min L_{sparse}\,s.t.\,\delta\leq \varepsilon_{\max},L_{model}^{p=1}+L_{rec}\leq \varepsilon_{loss}
$$

This is a constrained optimization problem. There are several approaches to this problem:
\begin{enumerate}
    \item Static linear combination: $\frac{L_{sparse}}{NM}+\frac{L_{model}^{p=p}+L_{model}^{p=1}}{2N}+\frac{L_{rec}}{O}$
    \item Dynamic linear combination: $\frac{L_{rec}}{O}+\frac{L_{model}^{p=1}+L_{model}^{p=p}}{2N}+\beta L_{sparse}$ where $\beta$ is chosen based on the value of $\delta$ as described above
    \item Lagrange multipliers. We consider $\mathcal L(\lambda, D, M, R)=L_{sparse}+\lambda (L_{rest}-\varepsilon_{rest})$. The point with $\max_{\lambda}\min_{D, M, R}\mathcal L$ corresponds to $L_{sparse}\to\min$ s.t. $L_{rest}\leq\varepsilon_{rest}$. The standard solution is a primal-dual method:
    $$\lambda_{t+1}=\lambda+\eta\frac{\partial \mathcal L}{\partial \lambda}$$
    $$(D, M,R)_{t+1}=(D,M,R)-\eta\frac{\partial\mathcal L}{\partial (D, M, R)}$$
\end{enumerate}

\section{Environments}
\begin{enumerate}
    \item VectorIncrement. Recovering a sparse linear matrix from skewed observations
    \item KeyChest. Learning a sparse causal graph in a grid-world
    \item Physics environments. Learning physics laws
    \item Atari: MsPacman, Montezuma Revenge. Learning a better representation than pixels
    \item PyGame learning environment (flapping bird)
    \item Starcraft. Learning in high-dimensional environments
    \item VIZDoom. Learning abstract features
    \item TextWorld. Extracting a low-dimensional model governing the decisions
    \item OpenSpiel. Finding good representations automatically for known games
\end{enumerate}

\section{Results}

\begin{enumerate}
    \item Can fit a sparse model on VectorIncrement with pinverse sparsity loss, fully-connected nets and value function for reconstruction
    \item Can train agent on KeyChest (DQN and PPO) demonstrating that it is Markov
    \item Can fit a sparse model (without sparsity regularization) ignoring actions that somewhat well predicts the health (but not health increase, only decrease)
    \item Cannot fit a VAE, AE, VAE-GAN on KeyChest data for some reason
\end{enumerate}

\subsection{What does MuZero learn}
\subsection{Learned graphs for our environments}
\subsection{Game complexities}
\subsection{Properties of the new agent}
\subsubsection{Spurious correlations and exploration speed}
\subsubsection{Sample efficiency}
\subsubsection{Online DS robustness (re-training speed)}
\subsubsection{Offline DS robustness (deploying into the environment)}


\end{document}
